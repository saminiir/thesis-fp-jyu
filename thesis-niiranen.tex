%{{{ Settings
% !TeX encoding = latin1
%
% [ Tiedostossa käytetty merkistö on ISO 8859-1 eli Latin 1. Ylläoleva rivi ]
% [ tarvitaan, jos käyttää MiKTeX-paketin mukana tulevaa TeXworks-editoria. ]
%
% TIETOTEKNIIKAN KANDIDAATINTUTKIELMA
%
% Yksinkertainen LaTeX2e-mallipohja kandidaatintutkielmalle.
% Käyttää Antti-Juhani Kaijanahon ja Matthieu Weberin kirjoittamaa
% gradu2-dokumenttiluokkaa.
%
% Laatinut Timo Männikkö
%
% Jos kirjoitat pro gradu -tutkielmaa, tee mallipohjaan seuraavat muutokset:
%  - Poista dokumenttiluokasta optio shortthesis .
%  - Poista makro \tyyppi .
%  - Lisää suuntautumisvaihtoehto makrolla \linja .
%  - Kirjoita ylimmän tason otsikot makrolla \chapter, toisen tason otsikot
%    makrolla \section ja mahdolliset kolmannen tason otsikot makrolla
%    \subsection .
%
% Halutessasi voit tehdä myös kandidaatintutkielman "pro gradu -tyylillä":
%  - Poista shortthesis-optio.
%  - Kirjoita otsikot makroilla \chapter , \section (ja \subsection ).

\documentclass[english]{tjt-latex-gradupohja/gradu3}

\usepackage{graphicx} % tarvitaan vain, jos halutaan mukaan kuvia

\usepackage{booktabs} % hyvä kauniiden taulukoiden tekemiseen

\usepackage{tikz} % Portable graphics, diagrams
\usetikzlibrary{matrix,positioning,fit,arrows,shapes.geometric}

\usepackage{amsmath}

\usepackage{caption}
\usepackage{listings}
\lstset{language=Haskell,
        captionpos=b,
        numbersep=25pt,
        xleftmargin=0.55in,
        aboveskip=.125in,
        belowskip=.125in,
        belowcaptionskip=0in,
        numberstyle=\color{mygray}
       }

\captionsetup[lstlisting]{aboveskip=1em,belowskip=1em}

\definecolor{mygray}{rgb}{0.5,0.5,0.5}

\usepackage{babel}

\usepackage{comment}

\usepackage[bookmarksopen,bookmarksnumbered,linktocpage]{hyperref}

\addbibresource{sources-niiranen.bib}

\tolerance=2000 \emergencystretch=10pt % Better hyphenation without overflowing lines

\begin{document}
%}}}
%{{{Meta
\title{Software quality metrics and functional programming}
\translatedtitle{Ohjelmistomittarit ja funktionaalinen ohjelmointi}

\author{Sami Niiranen}
\contactinformation{sami.i.niiranen@student.jyu.fi}

\supervisor{Ville Tirronen,}
\supervisor{Antti-Juhani Kaijanaho,}
\supervisor{Jari Veijalainen}
\supervisor{Jukka Lindström (Reaktor Innovations)}

\setdate{02}{03}{2014}

\abstract{TODO}
\tiivistelma{TODO}

\avainsanat{tietojärjestelmätiede, pro gradu -tutkielma, TODO} 
\keywords{information technology, Master's thesis, TODO} % avainsanoilla

%\studyline{Järjestelmäkehitys}

\maketitle

\mainmatter
%}}}
%{{{Introduction
\chapter{Introduction}
Software quality metrics is a branch of software engineering, where the quality of code is analyzed and quantified. The purpose of this observation is to e.g. identify problematic modules in the code and predict future maintenance effort~\parencite{fenton1999software}. The definition for software quality metrics is given in IEEE standard 1061~\parencite{ieeesoftwaremetrics1998} as: 
~\begin{quoting}
A function whose inputs are software data and whose output is a single numerical value that can be interpreted as the degree to which software possesses a given attribute that affects its quality.
~\end{quoting}

\noindent~\citet{fenton1998software} regards these as ``structural and complexity metrics'' when dealing with the internal attributes of software.
%{{{
\begin{comment}
Structural and complexity metrics~\parencite{fenton1998software} p 18.
\end{comment}
%}}}
\noindent Examples of such attributes are lines of code (LOC) and cyclomatic complexity~\parencite{mccabe1976complexity}.


\section{Previous work}

Code analysis and comparison of quality in functional programming (FP) paradigms is relatively scarce~\parencite{hudak1994haskell,harrison1996comparing,harrison1995estimating,harrison1996evaluation}.

Explicit formulation of metrics for functional programming has been presented in the works of~\citet{ryder2004software,van1995software}. Furthermore,~\citet{ryder2004software} leaves the measurement of type systems, polymorphism and higher-order functions for future research, which this thesis aims to address.

\section{Limitations}

Commercial use of programming paradigms are still in favour of the object-oriented (Java) and imperative (C) approaches. Despite FP being largely utilized in academia, the research on it is somewhat diverse and not uniform. In addition, real-world applications of FP are not abundant and finding a suitable program for research might be challenging. 

Software metrics, regardless of their quantitative nature, are up for interpretation and thus the results of the metrics are subjectively assessed.

Algorithmic efficiency is left out.

\section{Research problem}

The aim of this thesis is to provide an up-to-date overview of software metrics and discuss future research. Moreover, metrics are specifically pruned in the context of functional programming languages, where Haskell is used as reference.  As an example,~\citet{ryder2004software} encourages future research for measuring polymorphism and higher-order functions in functional programming.
%{{{
\begin{comment}
Functional languages often provide powerful abstraction mechanisms such as polymorphic and higher-order functions, or abstract data types. These languages features suggest that useful metrics could be defined to measure attributes of abstraction in programs.
For example, one might measure how polymorphic a function is by counting the number of different type variables present in the function¿s type signature, or by counting how many different types the function is used as. Likewise one might measure how abstract a given algebraic data type is by counting the number of constructors which are exported for that data type. Similar metrics could also be created to measure attributes of higher-order functions and other abstraction mechanisms.~\parencite{ryder2004software}
\end{comment}
%}}}

The research can be decomposed into four questions:

\begin{itemize}
    \item What are software quality metrics?
    \item What is functional programming?
    \item How do existing software quality metrics apply to the functional programming paradigm? 
    \item What kind of new metrics can be formulated and how do they apply to FP programs?
\end{itemize}

\noindent The research problem can be expressed as follows: \textit{What kind of software metrics exists for the functional programming paradigm, and how do the metrics apply to applications with functional implementation?}

\section{Research methods}
The thesis utilizes literature review as its main research method. The review consists of an overview into the metrics of software quality and functional programming. In preparation of the empirical part, FP metrics~\parencite{ryder2004software} are scrutinized and the possibility for new metrics is investigated.

New metrics formulated will be presented. For example, polymorphism and other type system -specific metrics are currently nonexistant~\parencite{ryder2004software} and could serve as a basis for new metrics research.

In the second part, the research themes are investigated through empirical research. One or more case study applications are chosen and their source code is used for applying the metrics. As an example, the abstract syntax trees (AST) with type information could be built from the case study source codes and the metrics quantified with the help of the AST.

\section{Expected results and their significance}
The results aim at explaining the various semantics of FP and how they relate to software quality metrics. The discovery of new metrics for FP-specific metrics is likely, since research on the subject has not been extensive~\parencite{ryder2004software}.

The research is expected to be of significance to researchers and practitioners alike. The results will help in formulating software metrics and FP into practical settings. Future research will be fueled by providing more support to the functional paradigm as opposed to imperative approaches, and encourage real-world usage of the functional paradigm.
%{{{Additional questions
\begin{comment}
There are two general types of criticisms that can be applied to current software metrics. The first category are those theoretical criticisms that are leveled at conventional software metrics as they are applied to traditional, non-00 software design and development. Kearney, et al. criticized software complexity metrics as being without solid theoretical bases and lacking appropriate properties [21]. Vessey and Weber also commented on the general lack of theoretical rigor in the
structured programming literature [41]. Both Prather and Weyuker proposed that traditional software complexity metrics do not possess appropriate mathematical properties, and consequently fail to display what might be termed normal predictable behavior [34], [47].~/parencite{chidamber1994metrics}.

One of the peculiarities of software engineering rela-
tive to other engineering and scientific disciplines is its
lack of maturity (Abran et al., 2003; Abran, 1998).
Symptoms of this are the small number of empirical
studies and the lack of an empirical basis in the aca-
demic world (exceptions are works like Boehm et al.,
2005; Endres and Rombach, 2003), and the very limited
number of internationally accepted software measure-
ment methods available for practitioners. In mature dis-
ciplines, there is international consensus on measurement,
in addition to established measurement methods and an
etalon for each. There is also a significant number of
measuring instruments to be used in different contexts
and under various constraints, many of them often cali-
brated against internationally recognized, and traceable,
etalons. In the software domain, we have none of the
above, with the exception of functional size measurement
with an ISO meta-standard prescribing key concepts of
the entity and attribute to be measured ISO/IEC 14143
(2003) and four ISO recognized measurement methods
ISO/IEC 19761, COSMIC-FFP (2003), ISO/IEC 20968
(2001),ISO/IEC20968(2001)andISO/IEC24570
(2001), since it has not yet reached the stage of having
a single universal way of measuring software functional
size.~\parencite{habra2008framework}.

\end{comment}
%}}}

%}}}
%{{{Software metrics
\chapter{Software metrics}
\label{chap:software_metrics}

Software is known to be brittle in the sense that its runtime behavior might not correspond to what the programmer had intended. More specifically, human error often introduces faulty logic into the software's code. Once these unintended ``features'', or bugs, slip into production versions and the software is released, fixing a critical error afterwards might require approximately hundred times more effort~\parencite{shull2002we}. To help assessing and predicting the quality of the
software at the time of development,~\textit{software metrics} are engineered.
%{{{
\begin{comment}
General data were presented that supported an effort increase of approximately 100:1. Don O¿Neill described data from IBM Rochester [10] in the pre-meeting feedback that found an increase in effort of about 13:1 for defect slippage from code to test and a further 9:1 increase for slippage from test to field (so, a ratio of about 117:1
from code to field).~\parencite{shull2002we}
\end{comment}
%}}}

Software metrics is a rather generic term traditionally meant to include the assessment of software by its internal and external attributes~\parencite{fenton1999software}. The characteristics of software are quantified through metrics in order to predict (1) effort/cost of development processes and (2) the quality of software~\parencite{fenton2000software}. The central idea of measuring software is that with access to the characteristics
(quality, complexity) of the software, developers and management can make educated guesses about development and resource planning~\parencite{fenton1999software}.
%{{{
\begin{comment}
¿Software metrics¿ is the rather misleading collective term used to describe the wide range of activities concerned with measurement in software engineering. These activities range from producing numbers that characterise properties of software code (these are the classic software ¿metrics¿) through to models that help predict software resource requirements and software quality. The subject also includes the quantitative aspects of quality control and assurance - and this covers
activities like recording and monitoring defects during development and testing.~\parencite{fenton1999software}.

Although there are literally thousands of metrics that have been proposed since the mid 1960's (all of which fit into the framework of Table 1) the rationale for almost all individual metrics has been motivated by one of two activities:
1. The desire to assess or predict effort/cost of development processes;
2. The desire to assess or predict quality of software
key in both cases has been the assumption that product 'size' measures should drive any predictive models.~\parencite{fenton2000software}
\end{comment}
%}}}

First attempts at recording software metrics were witnessed in the 1960s when properties such as program size were measured. The results were then used to assess programmer productivity and the quality of their work. In the 1970s the limitations of simple measures like lines of code was observed, and research departed to produce more elegant measures. These new measures, such as the ones from~\citet{halstead1977elements} and~\citet{mccabe1976complexity} were aimed to provide complexity measures independent of the accompanying programming languages. Ever since, research has produced numerous new metrics and validation frameworks, each with their own advantages and disadvantages.~\parenciteseveral{fenton1999software}.

The definition for software quality metrics is given in IEEE standard 1061~\parencite{ieeesoftwaremetrics1998} as: 
~\begin{quoting}
A function whose inputs are software data and whose output is a single numerical value that can be interpreted as the degree to which software possesses a given attribute that affects its quality.
~\end{quoting}

\noindent Examples of such attributes are the number of lines of code and cyclomatic complexity~\parencite{mccabe1976complexity}. This thesis adopts the general categorization of software measures as proposed by~\citet{fenton1998software}. In specific, measures are divided into size, structure and information flow measures. In essence, the software metrics presented in the thesis deal with the internal attributes of software, mainly the composition of the program code.

In addition, much disparity on the nature of metrics research is evident between researchers.~\citet{briand1996property} argue that many of the metrics presented are misplaced in the sense that they do not obey mathematical axioms of a given category, rendering their validation difficult. On the other hand, many researchers seem to emphasize real-world studies where results count more than their theoretical foundings~\parencite{kitchenham2010s}. As such, this thesis recognizes the interpretative nature of the metrics and does not attempt to prove the validity of each presented measure in mathematical terms. The metrics presented in this thesis were chosen by their apparent popularity and impact in research. 
%{{{
\begin{comment}
Unfortunately, new software measures are very often defined to capture elusive concepts such as complexity, cohesion, coupling, connectivity, etc. (Only size can be thought to be reasonably well understood.) Thus, it is impossible to assess the theoretical soundness of newly proposed measures, and the acceptance of a new measure is mostly a matter of belief.~\parencite{briand1996property}

Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems.~\parencite{briand1996property}
\end{comment}
%}}}

This chapter continues on presenting many, if not all, classic measures of software code. These foundings are important to understand the implications of the empirical research that has been conducted and nature of the trends in the field. After the initial presentation, the focus of the chapter shifts to newer measurement approaches such as the measurement of code version history, which albeit does not directly fall into the category of internal code attribute measurement, is a prevalent current trend in the field~\parencite{zimmermann2005mining}. Lastly, the challenges of metrics are discussed and the main obstacles of future research are provided.

%{{{ TODO: sources
\begin{comment}
In the software engineering literature, the notion of measure validity is used in many ways with different mean- ings [16]. Fenton and Pfleeger [17] call a measure internally valid if it is a proper numerical characterisation of the attribute. The internal validity of a measure must be distinguished from its external validity. The latter perspective on measure validity refers to the usefulness of a measure. According to Briand et al. [18] a software measure is useful if it is related to a measure of some external attribute, e.g.~\parencite{poels2000distance}

A measure is (internally) valid if it is a homomorphism from the empirical relational system into a formal relational system, i.e. if it maps entities into values such that all empirical relations among the entities are preserved as formal relations among the measurement values. The funda- mental problem of software measurement is that for many software attributes, including software complexity, it is not known what the empirical relational system looks like [20].~\parencite{poels2000distance}.

As a consequence, these measure property sets are useful to in- validate software measures by showing that they do not measure the attribute such as it is empirically understood. However, the set of measures that would be considered valid for the attribute of interest is only a subset of the set of measures that satisfy all the necessary axioms.~\parencite{poels2000distance}

A useful way to understand empirical relations on a set of object-elements is to consider the measurement of complexity.  A designer generally has some intuitive ideas about the com- plexity of different object-elements, as to which element is more complex than another or which ones are equally complex.  For example, a designer intuitively understands that a class that has many methods is generally more complex, ceteris paribus, than one that has few methods. This intuitive idea is
defined as a viewpoint. The notion of a viewpoint was originally introduced to describe evaluation measures for information retrieval systems and is applied here to capture designer views [9 1. More recently, Fenton states that viewpoints characterize intuitive understanding and that viewpoints must be the logical starting point for the definition of metrics [14]. An empirical relation is identical to a viewpoint, and the two terms are distinguished here only for the sake of consistency with
the measurement theory literature.~\parencite{chidamber1994metrics}.
\end{comment}
%}}}

\section{Size metrics}

Perhaps the first approach to measuring the internal attributes of software is the ~\textit{size} metric. It is relatively easy to be quantified, since only source code is required for the analysis and the measurement is often made statically, meaning that the program code is not executed.~\citet{fenton1998software} divide the size metric into the following subcategories: 

\begin{itemize}
    \item~\textit{Length} (actual size of the source code)
    \item~\textit{Functionality} (the features the user receives)
    \item~\textit{Complexity} (how difficult the program code is to reason with) 
    \item~\textit{Reuse} (how much of the code is original and what is reused)
\end{itemize}

\noindent Essentially the simplest size metrics are relatively easy to be formed, but also provide only a one-dimensional view into the quality of the software. Thus some of the metrics are most often utilized as an aid for interpreting internal attributes, but not too much information should be expected of their results.~\parenciteseveral{fenton1998software}.
%{{{
\begin{comment}
Rejecting simple size measures on these grounds reflects a misunderstanding of the basic measurement principles.~\parencite{fenton1998software}
\end{comment}
%}}}

%{{{TODO: Sources
\begin{comment}
Several measures introduced in the literature can be classified as size measures, according to our properties Size.1 - Size.3. With reference to code measures, we have: LOC, #Statements, #Modules, #Procedures, Halstead's Length [H77], #Occurrences of Operators, #Occurrences of Operands, #Unique Operators, #Unique Operands. In each of the above cases, the representation of a program as a system is quite straightforward. Each counted entity is an element, and the relationship between elements is just the sequential relationship.~\parencite{briand1996property}
\end{comment}
%}}}

\subsection{Lines of code}

Counting the~\textit{lines of code} (LOC) is perhaps the most commonly used metric, but on the same time it poses challenges regarding the interpretation of the results~\parencite{fenton1998software}. Programmers and projects have distinct styles and guidelines which can affect the results. For example, a function can be separated into multiple lines in order to make it more readable, where in other cases the same results are achieved by a shorter declaration. In turn, the LOC metric varies even though the intent of the function is the same.
%{{{
\begin{comment}
Some lines of code are different from others~\parencite{fenton1998software}.
\end{comment}
%}}}

Furthermore, not all source code lines are equal. Often comments and blank lines are used to increase the readability of the program. These types of lines might not require the same effort of development as the code itself and as a result they can distort the size metric. For these reasons, LOC is traditionally meant to contain only the ``effective'' source code lines without comments and blank lines~\parencite{grady1987software}. To iterate the LOC metric further,~\citet{fenton1998software} suggest calculating the ratio of the different types of source code lines: 
\begin{flalign*}
\text{Total length (LOC) = Non-Comment LOC + Comment LOC}
\end{flalign*}
\noindent Then the following serves as the ratio of commented lines in the source code:

\begin{flalign*}
\dfrac{CLOC}{LOC}
\end{flalign*}

\noindent Using a ratio is helpful since it is comparable between different applications or modules. Executable statements (ES) is one form of quantifying lines of source codes. In ES, comment lines, data declarations and headers are ignored and instead the operation statements are counted. Multiple executable statements in one line are thus considered separate.
%{{{
\begin{comment}
(ES) This measure counts separate statements on the same physical line as distinct. it ignores comment lines, data delcarations and headings.~\parencite{fenton1998software}.
\end{comment}
%}}}

The implications are that the LOC metric is highly situation-dependant. Different software projects can be compared based on their line count, but also modules in the same source code are often inspected. Puzzlingly, this seemingly simple metric can be nontrivial to interpret, since its use cases are manifold. One source code line might be short, while another one is long and complex. As is, measuring the lines of code should be tailored for specific use.~\parenciteseveral{fenton1998software}.~\citet{rosenberg1997some} argues that despite its wide usage, there exists common misconceptions about the usage of the LOC metric. The actual format of the metric should not matter and that the metric is best used as a cofactor of other, more advanced metrics~\parencite{rosenberg1997some}. Indeed, these observations resonate with later research that do utilize the LOC metric as the entry point for measuring software, but refrain from solely basing the results on it.
%{{{
\begin{comment}
Clearly, the definition of code length is influenced by the way in which it is to be used. Some organizations use length to compare one proeject with another, to answer questions such as: What is our largest/smallest/average project? What is our productivity? What are the trends in project length over time?~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Halstead's program volume}
\label{subsection:halstead-metric}

One of the earliest static analysis methods for source code is the Halstead-metric~\parencite{halstead1977elements}. With an effort to unify empirical research on software,~\citet{halstead1977elements} divides program code into tokens of~\textit{operators} and~\textit{operands} that can be used to assess several characteristics of the software regardless of, for example, the implementation language. Operators can be considered as ``actions'' in
the source code, like the the assignment operator {\tt =} in imperative languages. Operands represent data, such as variables~\parencite{ryder2004software}. Moreover, the variables of the Halstead-metric are:
%{{{
\begin{comment}
Halstead classifies symbols in a program into two classes, those that are used to specify actions are classed as operators while symbols that represent data are classed as operands.~\parencite{ryder2004software}.
\end{comment}
%}}}
\begin{flalign*}
\mu_1 = \text{number of unique operators}\\
\mu_2 = \text{number of unique operands}\\
N_1 = \text{total occurrences of operators}\\
N_2 = \text{total occurrences of operands}
\end{flalign*}
\noindent These variables are then used to quantify several properties such as the~\textit{length},~\textit{vocabulary},~\textit{volume},~\textit{program level} and~\textit{difficulty} of the source code. The example C code (listing~\ref{lst:halstead-example}) has thus 6 operands ({\tt a, b, 1, 2, sum, "sum of.."}) and 5 operators ({\tt int, =, +, printf, ()}).

\begin{lstlisting}[language=C,caption=Halstead volume quantity example,label=lst:halstead-example]
int a = 1;
int b = 2;
int sum = a + b;
printf("sum of %d and %d is %d\n", a, b, sum);
\end{lstlisting}

The Halstead program volume measures are calculated for listing~\ref{lst:halstead-example} as follows. The length $N$ is 20 ($N_1 + N_2$). The vocabulary $\mu$ is calculated with $\mu_1 + \mu_2$, which amounts to 10. The volume is calculated by $V = N * log_2 \mu$, resulting to 66.44. Moreover, the program level is calculated with $L = V_1/V$ where $V_1$ is the volume of the minimal potential size of the implementation. The inverse of level is difficulty, calculated by $D = 1/L$.~\textit{Effort} is calculated by $E = D * V$.

The meaning of Halstead's mathematical models on real-world usage are unclear. Halstead is critized for not indicating whether the metrics are measures or predictive variables of systems.~\parenciteseveral{fenton1998software}. This might add to observations made by other researchers, such as~\citet{jones1994software}, about the convoluted nature of the Halstead metric and its inherent flawness in attempting to deliver accurate estimations of development and maintenance effort.

\section{Structural metrics}

Another interesting property of software is their structure, which indicates additional information of the development and maintenance effort of the source code. One approach to inspecting structural metrics is the perspective of~\textit{control-flow}, or the logical paths in the execution of the program. On the other hand,~\textit{data-flow} deals with information flow between modules in the software.~\parenciteseveral{fenton1998software}. In this section, the more evident metrics of structural metrics are presented and their challenges are analyzed.
%{{{
\begin{comment}
The structure of the product plays a part, not only in requiring development effort but also in how the product is maintained.~\parencite{fenton1998software}.

It is the inter-module dependencies that interest us, and measures of these attributes are called inter-modular measures.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Flow graphs}

As the research on software metrics traditionally centers on imperative languages,~\textit{flowa graphs}~\parencite{allen1970control} are an intuitive way of mapping the ``flow'' of program statements. Flow graphs are directed graphs that indicate program statements as nodes, and flow of control as arcs (or edges) in the graph. Together these elements visualize all of the paths the program's execution can take~\parencite{fenton1998software}.
%{{{
\begin{comment}
A great deal of software metrics work has been devoted to measuring the control-flow structure of imperative language programs or algorithms. The control flow measures are usually modeled with directed graphs, where each node (or point) corresponds to a program statement, and each arc (or directed edge) indicates the flow of control from one statement to another.
\end{comment}
%}}}

For example, the following pseudo-code {\tt if A then X else Y} can be expressed as a flow graph:

\begin{figure}[h]\centering
    \begin{tikzpicture}[>=latex']
        \tikzset{block/.style= {draw, rectangle, align=center,minimum width=2cm,minimum height=1cm},
        rblock/.style={draw, shape=rectangle,rounded corners=1.5em,align=center,minimum width=2cm,minimum height=1cm},
        input/.style={ % requires library shapes.geometric
        draw,
        trapezium,
        trapezium left angle=60,
        trapezium right angle=120,
        minimum width=2cm,
        align=center,
        minimum height=1cm
    },
        }
        \node [rblock]  (start) {A};
        \node [rblock, below right =2cm of start] (acquire) {X};
        \node [rblock, below right =2cm and -5.5cm of start] (gchannel) {Y};

        \draw[->,black] (start) to node[right = 0.5cm] {true} (acquire);
        \draw[->,black] (start) to node[left = 0.5cm] {false} (gchannel);
    \end{tikzpicture}
  \caption[Example of flow graph~\parencite{allen1970control}
  ]{Example of flow graph~\parencite{allen1970control}}
    \label{fig:flow graph-example}
\end{figure}
%{{{
\begin{comment}
\begin{figure}[h]\centering
  \begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=1cm,
    node/.style={%
      ellipse,
      minimum width=5em,
      minimum height=3em,
      draw
    }
  ]
    \node[node] (1)                                             {1};
    \node[node] (2) [below=of 1]                                {2};
    \node[node] (4) [node distance=1cm and 3mm,below left=of 2] {4};
    \node[node] (3) [left=of 4]                                 {3};
    \node[node] (5) [below=of 4]                                {5};
    \node[node] (6) [node distance=2cm,right=of 5]              {6};

    \path (1) edge                   node {} (2)
          (2) edge                   node {} (3)
          (2) edge                   node {} (4)
          (2) edge                   node {} (6)
          (3) edge                   node {} (5)
          (4) edge                   node {} (5)
          (5) edge [bend right=20pt] node {} (2);
  \end{tikzpicture}
  \caption[Example of flow graph~\parencite{allen1970control}
  ]{Example of flow graph~\parencite{allen1970control}}
    \label{fig:flow graph-example}
\end{figure}
\end{comment}
%}}}

\noindent As can be seen from figure~\ref{fig:flow graph-example}, directed graphs consist of a set of nodes that are connected with arcs. Furthermore, the arrowhead indicates the direction of the execution, flowing from one node to another.~\parenciteseveral{allen1970control}. Logical paths arriving at a node are called ~\textit{in-degrees} of the node, while a node's~\textit{outdegrees} indicate the arcs ``leaving'' the node~\parencite{fenton1998software}.
%{{{
\begin{comment}
Thus, directed graphs are depicted with a set of nodes, and each arc connects a pair of nodes. We write an arc as an ordered pair, <x,y>, where x and y are the nodes forming the endpoints of the arc, and the arrow indicates that the arc direction is from x to y. The arrowhead indicates that something flows from one node to another node.~\parencite{fenton1998software}.

The in-degree of a node is the number of arcs arriving at the node, and the outdegree is the number of arcs that leave the node. We can move from one node to another along the arcs, as long as we move in the direction of the arrows. A path is a sequence of consecutive (directed) edges, some of which may be traversed more than once during the sequence. A simple path is one in which there are no repeated edges.~\parencite{fenton1998software}.
\end{comment}
%}}}

The flow graph is convenient for visualizing the structural properties of the program. Nodes are the program statements themselves, and the directed arcs make the flow of control explicit. Numerous measures can also be derived, such as the number of nodes, number of edges (arcs), the~\textit{largest prime} measure and so on. The call graph itself is, however, usually used in conjuct with other measuring approaches such as the~\textit{cyclomatic complexity}.~\parenciteseveral{fenton1998software}.
%{{{
\begin{comment}
The flow graph is a good model for defining measures of control-flow structure, because it makes explicit many of the structural properties of the program. The nodes enumerate the program statements, and the arcs make visible the control patterns.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Cyclomatic complexity}

The~\textit{Cyclomatic complexity} metric~\parencite{mccabe1976complexity} utilizes flow graphs to evaluate the complexity of a program's decision structure. A program is represented as a flow graph $F$ with single entry and exit points of execution. The~\textit{cyclomatic number} is then calculated as:
\begin{flalign*}
v(F) = e - n + 2
\end{flalign*}

\noindent where the flow graph $F$ has $e$ amount of arcs and $n$ amount of nodes. The cyclomatic number points out the number of independent execution paths through $F$ and is seen to indicate the complexity of the program's structure. More specifically, comprehending the source code in general increases in difficulty when the cyclomatic number is higher~\parencite{ryder2004software}. Indeed,~\citet{mccabe1976complexity} originally advocated the number 10 as the cyclomatic complexity limit when refactoring of the module should be considered.
%{{{
\begin{comment}
Programmers have been required to calculate complexity as they create software modules. When the complexity exceeded 10 they had to either recognize and modularize subfunctions or redo the software. The intention was to keep the "size" of the modules manageable and allow for testing all the independent paths~\parencite{mccabe1976complexity}.

\end{comment}
%}}}

~\citet{fenton1998software} agree that the cyclomatic number is objective, but argue concurrently that it is a poor indicator of ``general complexity'' of a program's structure. Mainly because this ``complexity'' of a flow graph $F$ does not necessary hold in conjuction with general measurement theory and its ``intuitive relations about complexity''.~\citet{fenton1998software} do suggest, however, that cyclomatic number is a well-suited indicator for how difficult the program is to test and maintain. Other proponents~\parencite{shepperd1988critique} argue that simple lines of code metrics can be more indicative than cyclomatic complexity.
%{{{
\begin{comment}
The number of independent paths is a good indicator of the complexity of a program because generally a program becomes harder to understand as the number of paths increases.~\parencite{ryder2004software}.

From a measurement theory perspective, it is extremely doubtful that any of these assumptions corresponds to intuitive relations about complexity. Thus, v cannot be used as a generla complexity measure.~\parencite{fenton1998software}
\end{comment}
%}}}

Cyclomatic complexity continues to be one of the widely used static code measures~\parencite{zimmermann2005mining}. It is relatively easy to extract out of static code and thus serves at least as an entry-point to more advanced measuring. Much debate over its efficacy has emerged over the decades, but no solid evidence one way or the another has been produced.

\subsection{Test coverage measures}

One approach to software measurement is to measure how difficult the modules of a program are to test. In specific, if the structure of a program is misconstructed, also testability of the modules should be low and difficult to be carried out. Moreover, motivation for software measurement and testing is that complex structures should often be concentrated on over ``simpler'' functionality. This has real-world implications where quality assurance budgets are limited and using measurement tools to pinpoint problematic program modules translate to direct cost savings~\parencite{menzies2010defect}.
%{{{
\begin{comment}
In particular, the structure of a module is related to the difficulty we find in testing it.
\end{comment}
%}}}

The~\textit{minimum number of test cases} metric aims to quantify the lowest possible amount of test cases to satisfy a certain testing strategy. This approach also provides useful information about how time consuming the testing will be and further aid in planning the testing strategy. The flow graphs of program execution are used for deriving several properties of the program, such as the amount of simple paths, branches and statements, to be used in quantifying the possible minimum number of test cases.~\parenciteseveral{fenton1998software}.  

~\textit{Test effectiveness ratio}, or test coverage, is a common metric to indicate to which extent the actual execution logic is covered by the testing strategy used~\parencite{fenton1998software}. Test effectiveness ratio can be calculated as
%{{{
\begin{comment}
For a given program and a set of cases to test it, we would like to know the extent to which the test cases satisfy a particular testing strategy.~\parencite{fenton1998software}.
\end{comment}
%}}}

\begin{flalign*}
TER_T = \dfrac{\text{number of T objects exercised at least once}}{\text{total number of T objects}}
\end{flalign*}

\noindent where $T$ objects are paths in the execution logic, such as loops and branches. Few important observations of $TER$ are that it often requires run-time execution, as which paths a test case covers is difficult to deduce statically~\parencite{ryder2004software}. Second, the resulting ratio can be deceiving if the $T$ objects are constrained loosely. Namely, a $TER$ of 100\% is easy to achieve by only ``simple paths'' as $T$ object values, but this hardly indicates whether some particular functionality is actually tested~\parencite{fenton1998software}.
%{{{
\begin{comment}
This may be difficult to achieve statically, so it is often necessary to perform some form of runtime execu- tion tracing to be able to perform this measurement.~\parencite{ryder2004software}.

However, the actual TER is typically no better than 40%. The typical TER for branch coverage is lower still, providing objective evidence that most software is not tested as rigorously as we like to think.~\parencite{fenton1998software}.
\end{comment}
%}}}

\section{Data-flow attributes}
\label{section:data-flow-attributes}

The metrics so far have dealt with~\textit{intra-modular measures}, i.e. the measurement of the properties of single modules. Another interesting application of software measurement is how different program modules relate to each other in terms of structure and modularity~\parencite{fenton1998software}.~\citet{fenton1998software} propose that a module is any program construct that can be ``at least theoretically separately compilable''. 

Data-flow is often depicted with~\textit{call graphs}, which are directed graphs, but do not contain the start and end nodes found in flow graphs~\parencite{fenton1998software}. Furthermore, a call graph does not contain individual attributes of modules, but rather describes which modules a specific module calls.
%{{{
\begin{comment}
We use a more abstract model of the design, a directed graph known as the module call-graph. A call-graph is not a flow graph, as it has no circled start or stop node.~\parencite{fenton1998software}

For example, instead of examining variables, we may need to know only whether or not one module calls (or depends on) another module.~\parencite{fenton1998software}
\end{comment}
%}}}

\begin{figure}[h]\centering
    \begin{tikzpicture}[>=latex']
        \tikzset{block/.style= {draw, rectangle, align=center,minimum width=2cm,minimum height=1cm},
        input/.style={ % requires library shapes.geometric
        draw,
        trapezium,
        trapezium left angle=60,
        trapezium right angle=120,
        minimum width=2cm,
        align=center,
        minimum height=1cm
    },
    arrow/.style={->,ultra thick},
        }
        \node [block]  (start) {Main};
        \node [block, below right =2cm of start] (average) {Average};
        \node [block, below left =2cm of start] (read_scores) {Read\_Scores};
        \node [block, below left =2cm of average] (calc_av) {Calc\_Av};
        \node [block, below right =2cm of average] (print_av) {Print\_Av};

        \draw[arrow] (start)   to (average);
        \draw[arrow] (-28:1.8) -- (-30:2.7) node[above right]{scores};
        \draw[arrow] (208:2.7)  -- (206:1.8) node[above left]{scores};
        \draw[arrow] (218:2.4)  -- (219:1.6) node[pos=0.5, below right]{EOF};
        \draw[arrow] (start)   to (read_scores);
        \draw[arrow] (average) to (calc_av);
        \draw[arrow] (-60:3.7)  -- (-73:3.9) node[above left]{scores};
        \draw[arrow] (-70:4.4)  -- (-55:4.2) node[pos=0.5, below right]{average};
        \draw[arrow] (average) to (print_av);
        \draw[arrow] (-33:6.1)  -- (-33:7.2) node[above right]{average};
    \end{tikzpicture}
  \caption[Example of call graph~\parencite{fenton1998software}
  ]{Example of call graph~\parencite{fenton1998software}}
    \label{fig:call-graph-example}
\end{figure}

\noindent Several properties for assessing software design quality from call graphs have been formed~\parencite{yourdon1979structured}. The most obvious one is~\textit{size}, where the number of nodes and edges are counted from the call graph. A large call graph should then indicate increased complexity in the structure of the program~\parencite{ryder2004software}.~\textit{Depth}, on the other hand, is the longest path from the root node to a leaf node. In figure~\ref{fig:call-graph-example}, depth would be measured as the distance from {\tt Main} to either {\tt Calc\_Av} or {\tt Print\_Av}.

~\textit{Width} measures the maximum number of nodes at any one level. In figure~\ref{fig:call-graph-example}, the horizontal layer containing the most nodes are considered as the call graph's width, e.g. {\tt Read\_Scores + Average = 2}.~\textit{Arc-to-node ratio} indicates the amount of ``connections'' between nodes. In other words, the more function calls are in- or outbound to a node, the more the arc-to-node ratio increases~\parencite{ryder2004software}. Several other more specific attributes of call graphs have been derived. The advantage of such graph measures are that they are, in fact, often applicable to most graph-type models~\parencite{fenton1998software}.
%{{{
\begin{comment}
Yourdon and Constantine have analyzed what contributes to good design, and they suggest several ways to view design components and structure. They use the notion of morphology to refer to the ``shape'' of the overall system structure when expressed pictorially.~\parencite{fenton1998software}.

The greater the size of the call graph, the more dependencies between different parts of the program there is likely to be, and so the greater the complexity.~\parencite{ryder2004software}.

This is a connectivity density measure that increases as more connections, or function calls, are made and is similar to cyclomatic complexity. A high arc-to-node ratio indicates complex interactions between functions, and thus may highlight areas of program code that may benefit from re-engineering. Section 2.7 examines the use of metrics in the re- engineering process.~\parencite{ryder2004software}.

These measures are not restricted to dependency graphs. They are generally applicable to most graph-type models, and we shall apply them in Chapter 12 to evaluate project management structures.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Graph impurity}

One measure for system complexity is~\textit{graph impurity}~\parencite{ince1988approach}, which can be derived from the call graph of a program. The measure holds a simple hypothesis that the more the program's call graph deviates from being a tree-structure, the more complex the structure is~\parencite{ince1988approach}. 
%{{{
\begin{comment}
One measure of system complexity that has been sug- gested is graph impurity: the more a system deviates from being a pure tree structure towards being a graph structure, the worse the design is~\parencite{ince1988approach}.

\end{comment}
%}}}

\begin{figure}[h]\centering
  \begin{tikzpicture}[%
    shorten >=2pt,
    >=stealth,
    node distance=0.7cm,
    node/.style={%
      rectangle,
      minimum width=3em,
      minimum height=1.5em,
      draw
    }
  ]
    \node[node] (1)                                 {A};
    \node[node] (2) [below left=of 1]               {B};
    \node[node] (3) [below right=of 1]              {C};
    \node[node] (4) [below left=of 2]               {D};
    \node[node] (5) [below right=of 2]              {E};
    \node[node] (6) [below right=of 3]                   {F};

    \path (1) edge                   node {} (2)
          (1) edge                   node {} (3)
          (1) edge                   node {} (5)
          (2) edge                   node {} (4)
          (2) edge                   node {} (5)
          (3) edge                   node {} (6);
  \end{tikzpicture}
  \caption[Example of graph impurity~\parencite{ince1988approach}
  ]{Example of graph impurity~\parencite{ince1988approach}}
    \label{fig:graph-impurity-example}
\end{figure}

~\citet{ince1988approach} argue, that graph impurity should not be achieved at the cost of generally accepted design patterns. For example, a module that is often referenced by other modules would be considered as contributing to graph impurity, but the structure of the program can actually be well-designed because of module reuse. As is, graph impurity should not be interpreted fully objectively, but rather ``where choice exists a design should be produced which minimises graph impurity''~\parencite{ince1988approach}.
%{{{
\begin{comment}
This does not mean that a designer should aim at producing a pure tree-structured design. The very nature of the subroutine, an object called from many places in a program which saves space, means that the designer will make use of common calls. What it does mean is that where choice exists a design should be produced which minimises graph impurity.~\parencite{ince1988approach}.
\end{comment}
%}}}

Furthermore,~\citet{fenton1998software} provide a formula for calculating the impurity:
\begin{flalign*}
m(G) = \dfrac{2(e - n + 1)}{(n - 1)(n - 2)}
\end{flalign*}

\noindent where $e$ is the edge (or arc) count and $n$ is the node count in the call graph.~\citet{fenton1998software} conclude that ``system designs should strive for a value of $m$ near zero, but not at the expense of unnecessary duplication of modules''. This is in conjuction with the earlier presented observation that while graph impurity can indicate problematic software design, it might contradict beneficial design choices.
%{{{
\begin{comment}
Results suggest a relationship between tree impurity and poor design. Thus m may be useful for quality assurance of designs. System designs should strive for a value of m near zero, but not at the expense of unnecessary duplication of modules.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Internal reuse}

{\itshape Internal reuse} is the measure of to which extent a program reuses its own modules~\parencite{fenton1998software}.~\citet{yin1978establishment} have defined a measure for internal reuse ({\itshape system design measure}~\parencite{yin1978establishment}) which can be calculated from the call graph of a program:
%{{{
\begin{comment}
We call internal reuse the extent to which modules are reused within the same product.~\parencite{fenton1998software}.
\end{comment}
%}}}
\begin{flalign*}
r(G) = e - n + 1
\end{flalign*}

\noindent where graph $G$ has $e$ edges and $n$ nodes.~\citet{fenton1998software} criticize the measure by stating that (1) it does not consider different calls from the same module and (2) it does not consider the size of the reused components. As such, the measure is perhaps best used as a general measure of a program's internal reuse and used in conjuction with other measures such as tree impurity.
%{{{
\begin{comment}
This measure is crude as a reuse measure; not only does it take no account of possible different calls from the same module, it also takes no account of the size of the reused components.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Coupling}

{\itshape Coupling} addresses the degree of interdependence between modules~\parencite{allen2001measuring}. Namely, coupling is most often inspected between two modules ($x$, $y$).~\textit{Global coupling} can be derived from the combination of coupling between modules~\parencite{fenton1998software}.
%{{{
\begin{comment}
Coupling of a subsystem characterizes its interde- pendence with other subsystems.~\parencite{allen2001measuring}

Usually, coupling is an attribute of pairs of modules, rather than of the design as a whole. The entire set of modules in the design exhibits global coupling, which can be derived from the coupling among the possible pairs.
\end{comment}
%}}}

~\citet{fenton1998software} define several relations for inspecting the coupling between two modules:
\begin{itemize}
    \item{\itshape No coupling relation} $R_0$ when $x$ and $y$ have no interdependency.
    \item{\itshape Data coupling relation} $R_1$ when $x$ and $y$ communicate by data parameters but no control element is incorporated. An example would be a mathematical function which performs the same computation regardless of the parameter values~\parencite{ryder2004software}.
    \item{\itshape Stamp coupling relation} $R_2$ when $x$ and $y$ accept the same types as parameters. The modules can be otherwise unrelated but interdependency can emerge from shared data types.
    \item{\itshape Control coupling relation} $R_3$ when $x$ passes parameters to $y$ which changes its behavior. For example, a rendering function $y$ might receive coordinate points from module $x$.
    \item{\itshape Common coupling relation} $R_4$ when $x$ and $y$ refer to the same global data. The modules are then coupled, because of the common interdependency on the same data. 
    \item{\itshape Content coupling relation} $R_5$ when $x$ refers and changes the contents (e.g. values, state) of $y$.
\end{itemize}
%{{{
\begin{comment}
n example might be passing parameters to a mathematical function, the func- tion performs the same actions regardless of the parameters passed. This is in contrast to control coupling in which different sets of actions may be performed depending on the parameters that are passed.~\parencite{ryder2004software}.
\end{comment}
%}}}

In essence, the higher the $R_i$ value for two given modules, the more~\textit{tightly coupled} they are. In contrast, a $R_i$ value of 1-2 between two module would be characterized as~\textit{loosely-coupled}. However, the coupling classification can be challenging to determine, at least programmatically. A manual inspection of the program's structure is thus often warranted~\parencite{ryder2004software}.
%{{{
\begin{comment}
We say that x and y are loosely coupled if i is 1 or 2, and they are tightly coupled if i is 4 or 5.~\parencite{fenton1998software}.

It is also worth noting that in practice it may be difficult to exactly classify coupling by Fenton¿s classifications. For instance, determining between R3 and R1 coupling may be difficult or impossible to do mechanically and may therefore require manual inspection in order to decide the exact classification.~\parencite{ryder2004software}.
\end{comment}
%}}}

\begin{figure}[h]\centering
  \begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=0.5cm,
    node/.style={%
      rectangle,
      minimum width=3em,
      minimum height=1.5em,
      draw
    }
  ]
    \node[node] (A) at (0,0)    {A};
    \node[node] (B) at (4,0)    {B};
    \node[node] (C) at (0,-3)   {C};
    \node[node] (D) at (4,-3)   {D};

    \draw[->] ([yshift=4pt] A.east) -- ([yshift=4pt] B.west) node[pos=0.5, above]{(2,2)};
    \draw[<-] ([yshift=-4pt] A.east) -- ([yshift=-4pt] B.west) node[pos=0.5, below]{(2,2)};
    \draw[<-] ([xshift=-4pt] B.south) -- ([xshift=-4pt] D.north) node[pos=0.5, left]{(5,1)};
    \draw[->] ([xshift=4pt] B.south) -- ([xshift=4pt] D.north) node[pos=0.5, right]{(5,1)};
    \draw[->] (A.south) -- (C.north) node[pos=0.5, left]{(3,1)};

  \end{tikzpicture}
  \caption[Example of coupling model graph~\parencite{fenton1998software}
  ]{Example of coupling model graph~\parencite{fenton1998software}}
    \label{fig:coupling-model-graph-example}
\end{figure}

For visualizing the coupling of a system, a~\textit{coupling model graph} can be utilized~\parencite{fenton1998software}.  \noindent In the coupling model graph, nodes correspond to modules and the directed edges (arcs) are dependencies between the modules. A value $(i,j)$ defines the level of coupling ($i = R_i$) and the count of how many times a given coupling occurs ($j$) between two modules.~\parenciteseveral{fenton1998software}. Additionally, ~\citet{ryder2004software} argues that ``the visualisation of the module coupling may be of as much, or even more, use than any measures on coupling because it can allow developers to gain a high level overview of the interactions in the software system very quickly''.
%{{{
\begin{comment}
To model module coupling, we use a directed graph with more detail than a call graph but less than the full module structure chart. The nodes correspond to the modules, and there may be more than one arc between two nodes. Each arc from a node x to node y represents coupling between modules x and y.~\parencite{fenton1998software}
\end{comment}
%}}}


\subsection{Cohesion}

The metric~\textit{cohesion} determines the relative proximity of similar program features~\parencite{briand1996property}. Another interpretation is that cohesion determines the level of dependencies inside a module, in contrast to inter-module inspection of coupling~\parencite{allen2001measuring}. The higher the cohesion of a module is, the better its features are encapsulated and the responsibilities defined. As a result, the reliability and maintainability of the system increases~\parencite{fenton1998software}. 
%{{{
\begin{comment}
The concept of cohesion has been used with reference to modules or modular systems. It assesses the tightness with which "related" program features are "grouped together" in systems or modules.~\parencite{briand1996property}.

 subsystem¿s cohe- sion, on the other hand, characterizes its internal in- terdependencies.~\parencite{allen2001measuring}.
\end{comment}
%}}}

The classical definition of~\parencite{stevens1974structured} consists of seven degrees of cohesion, listed from the most desirable to least desirable:

\begin{itemize}
    \item {\itshape Functional}: the module performs a single well-defined function
    \item {\itshape Sequential}: the module performs several functions, but according to specification
    \item {\itshape Communicational}: the module performs several functions, but on the same data
    \item {\itshape Procedural}: the module performs several functions that relate only by the same control flow
    \item {\itshape Temporal}: the module performs several functions that relate only by their shared timespan
    \item {\itshape Logical}: the module performs several functions that relate only logically
    \item {\itshape Coincidental}: the module performs several unrelated functions
\end{itemize}

A module can contain several of the cohesion degrees~\parencite{fenton1998software}. Curiously, the cohesion metric has no formal measure by which it can be quantified. Instead,~\citet{fenton1998software} state that the level of a module's cohesion can be deduced by simply asking questions about the module's purpose. For example, if the purpose of a module cannot be described in a single sentence, it might have too many responsibilities, and consequently its cohesion is low. Paradoxically, a good designer probably has asked the question already and developed a module with high cohesion.~\parenciteseveral{fenton1998software}.
%{{{
\begin{comment}
There is no obvious measurement procedure for determining the level of cohesion in a given module. However, we can get a rough idea by writing down a sentence to describe the module's purpose. A good designer should have already done so, but a module with low cohesion is unlikely to have been produced by a good designer! If it is impossible to describe its purpose in a single sentence, then the module is likely to have coincidental cohesion.~\parencite{fenton1998software}.
\end{comment}
%}}}

Several other interpretations of cohesion have been presented, such as the ones from~\citet{chidamber1994metrics},~\citet{hitz1996chidamber} and~\citet{briand1998unified}. However, most of the interpretations are specially tailored for the object-oriented programming paradigm and as such are not generally applicable.

\subsection{Information flow}

As the flow of data between modules is an important part of measuring the attributes of software, also the overall level of~\textit{information flow} in a system is of interest.~\citet{henry1981software} describe three different types of information flow. First,~\textit{Local indirect flow} occurs when a module passes information to another module or conversely, returns a result to a module invoking it. Second,~\textit{Local indirect flow} exists if ``the invoked module returns information
that is subsequently passed to a second invoked module''. Third,~\textit{global flow} occurs when data is passed between modules through a global data structure.~\parenciteseveral{fenton1998software}.
\begin{comment}
Generally a modular program will consist of modules with data flowing through them in a manner that is termed information flow. Measuring properties of in- formation flow may be of interest to programmers because it can help to indicate where particularly complex interaction is occurring between functions, and thus where there may be complex algorithmic behaviour that is likely to make modifi- cations to the program difficult to perform.~\parencite{ryder2004software}.

We say a local direct flow exists if either 1. a module invokes a second module and passes information to it; or 2. the invoked module returns a result to the caller. Similarly, we say that a local indirect flow exists if the invoked module returns information that is subsequently passed to a second invoked module. A global flow exists if information flows from one module to another via a global data structure.~\parencite{fenton1998software}.
\end{comment}
%}}}

The information flow is relatively easy to be determined, as it can be done by simply analyzing the procedures and calls of the system's module~\parencite{henry1981software}. The~\textit{Henry-Kafura} measure~\parencite{henry1981software} formulates additional variables that aid in assessing the complexity of a module: The~\textit{fan-in} is the number of~\textit{local flows} into the module plus the number of data structures the module utilizes for information retrieval. On the other
hand,~\textit{fan-out} is the amount of local flows out from a module plus the number of data structures the module updates. ~\parenciteseveral{henry1981software}.
%{{{
\begin{comment}
An important property of the information flows defined above is that the knowledge necessary toconstructthecom- pleteflowsstructurecanbeobtainedfromasimpleprocedure- by-procedureanalysis.~\parencite{henry1981software}.

Thetermsfan-in,fan-out,complexity,andmodulearespecif- icallydefinedforinformationflowinordertopresentthe measurementsinthissection.Fan-inandfan-outaredescribed withrespecttoindividualprocedures.  Definition5:Thefan-inofprocedureAisthenumberof localflowsintoprocedureAplusthenumberofdatastruc- turesfromwhichprocedureAretrievesinformation.  Definition6:Thefan-outofprocedureAisthenumberof localflowsfromprocedureAplusthenumberofdatastruc-
tureswhichprocedureAupdates.~\parencite{henry1981software}.
\end{comment}
%}}}

The~\textit{Henry-Kafura} measure is formulated as follows:
\begin{flalign*}
    \text{Complexity of module $=$ length $*$ (fan-in $*$ fan-out)$^2$}
\end{flalign*}

\noindent where~\textit{fan-in $*$ fan-out} indicates the number of possible combinations of an input source to an output destination. Furthermore, the number is squared because the complexity is believed to be more than linear. Length is used for a simple measure of the complexity of the module's code. The result defines the complexity of a module based on its external dependencies and internal code quality.~\parenciteseveral{henry1981software}.
%{{{
\begin{comment}
Thetermfan-in*fan-outrepresentsthetotalpossiblenumber ofcombinationsofaninputsourcetoanoutputdestination.~\parencite{henry1981software}

ofcombinationsofaninputsourcetoanoutputdestination.  Theweightingofthefan-inandfan-outcomponentisbasedon thebeliefthatthecomplexityismorethanlinearintermsof theconnectionswhichaprocedurehastoitsenvironment.  ThepoweroftwousedinthisweightingisthesameasBrook's ~\parencite{henry1981software}.
\end{comment}
%}}}

The Henry-Kafura measure is improved upon by the research of~\citet{ince1989empirical}. Namely,~\citet{ince1988approach} address ``several theoretical problems with the measure''~\parencite{fenton1998software}, such as the informal notion of indirect flow and its applicability as a measurement model. In conclusion,~\citet{ince1988approach} propose an improved measure where the length of the module is not taken into consideration.

%{{{
\section{Metrics for object-oriented languages}

The popularity of object-oriented programming (OOP) has produced numerous new or adjusted software measures specific to the paradigm~\parencite{chidamber1994metrics,lorenz1994object,bansiya2002hierarchical}. In specific, the main components of OOP are real-world simulated classes, from which~\textit{objects} are instantiated. This is distinct to the imperative and functional approaches, where functions and data are the main drivers of design~\parencite{chidamber1994metrics}. 
%{{{
\begin{comment}
The 00 approach centers around modeling the real world in terms of its objects, which is in contrast to older, more traditional approaches that emphasize a function-oriented view that separates data and procedures.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

An object possesses~\textit{methods} and~\textit{attributes} that characterize its functionality. For example, a {\tt Dog} class might have a method {\tt wagTail()}. A {\tt Dog} object can then be instantiated and its methods used:
\begin{lstlisting}
var dog = new Dog();
dog.wagTail();
\end{lstlisting}

The purpose of the OOP paradigm is to model real-world entities as closely as possible~\parencite{booch1986object}. This thesis chooses to present OOP metrics specific to the work of~\citet{chidamber1994metrics}, where several measures for OOP are defined based on previous research. Namely, a close inspection of an object's properties and its relation to other objects is mapped as the foundation for OOP software metrics. Their measures have been validated as good predictors defects in real-world software by numerous empirical studies~\parencite{basili1996validation,subramanyam2003empirical,gyimothy2005empirical,zhou2006empirical,olague2007empirical} and is the most widely used OOP metric suite~\parencite{catal2009systematic}. On the other hand, some of the metrics of~\citet{chidamber1994metrics} have been invalidated by researchers~\parencite{hitz1996chidamber,kitchenham2010s}. Still, researchers continue to use the CK-metric suite as the reference measurement tool for OOP.
%{{{
\begin{comment}
We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes.~\parencite{olague2007empirical}

discussed. Several of Chidamber&Kemerer¿s OO metrics appear to be useful to predict class
fault-proneness during the early phases of the life-cycle. We also showed that they are, on our
data set, better predictors than ¿traditional¿ code metrics, which can only be collected at a later
phase of the software development processes.~\parencite{basili1996validation}

However, CK metrics suite is much more popular than other suites and they are mostly used if class-level metrics are applied.~\parencite{catal2009systematic}.
\end{comment}
%}}}

\subsection{Weighted methods per class}

~\citet{chidamber1994metrics} argue that the the number of methods and their complexity in a class is a predictor of the effort required to develop and maintain it. Furthermore, the more methods in a class, the greater the effect on subclasses. Thus the first metric proposed by~\citet{chidamber1994metrics} is the~\textit{weighted methods per class} (WMC) metric. WMC is defined as

\begin{flalign*}
WMC = \sum_{i=1}^{n} c_i
\end{flalign*}

\noindent where $n$ is the amount of methods in a given class and $c_i$ is the ``complexity'' of the method. The complexity metric is deliberately left undefined, as it can be interpreted in multiple ways and thus interchangeable in the metric. In other words,~\citet{chidamber1994metrics} leave the actual complexity measure as an implementation decision.

\subsection{Depth of inheritance}

As inheritance is often utilized in object-oriented design~\parencite{booch1986object}, also its quantity and quality should offer some information for software measurement.~\textit{Depth of inheritance} (DIT) simply inspects the ``position'' of a class in its inheritance hierarchy. For example, the deeper the class is in regards to inheritance, the more likely it is to obtain unneeded methods from parent classes and thus resulting in increased complexity. On the other hand, inheritance's main
advantage is the reusability of methods, since general functionality can simply be acquired higher from the inheritance hierarchy. As is, the DIT metric is easy to be calculated, but does not directly point problems in the design. The inheritance tree can be ``top-heavy'', where the tree is depicted as flat and wide, or ``bottom-heavy'', where the tree is thin but long. These properties do not, however, necessarily point out design flaws directly, as every program's design choices are unique.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    Metric 2: Depth of Inheritance Tree (DIT) 
    Definition: Depth of inheritance of the class is the DIT metric 
    for the class. In cases involving multiple inheritance, the DIT will 
    be the maximum length from the node to the root of the tree. 
    Theoretical Basis: DIT relates to Bunge¿s notion of the 
    scope of properties. DIT is a measure of how many ancestor 
    classes can potentially affect this class. 
    Viewpoints: 
    1) The deeper a class is in the hierarchy, the greater the 
    number of methods it is likely to inherit, making it more 
    complex to predict its behavior.16 
    ¿61nterestingly, this has been independently observed by other researchers 
    2) Deeper trees constitute greater design complexity, since 
    3) The deeper a particular class is in the hierarchy, the 
    more methods and classes are involved. 
    greater the potential reuse of inherited methods. 
    )~\parencite{chidamber1994metrics}
\end{comment}
%}}}

\subsection{Number of children}

The~\textit{number of children} metric is likewise easy to be measured. It is simply the amount of immediate subclasses (children) of a class. The number of children metric is based on the notions that the greater the amount of subclasses, also the greater the reusage of the parent class's methods is. With increased subclass count, however, also the likelyhood of misuse of inheritance by improper abstractions is increased. Lastly, the number of subclasses predict the impact of the parent
class on the design. For example, a high subclass count may mandate more testing of that class.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
nated to a class in the class hierarchy. 
Theoretical Basis: NOC relates to the notion of scope of 
properties. It is a measure of how many subclasses are going 
to inherit the methods of the parent class. 
Viewpoints: 
1) Greater the number of children, greater the reuse, since 
inheritance is a form of reuse. 
2) Greater the number of children, the greater the likelihood 
of improper abstraction of the parent class. If a class has 
a large number of children, it may be a case of misuse 
of subclassing. 
3) The number of children gives an idea of the potential 
influence a class has on the design. If a class has a large 
number of children, it may require more testing of the 
methods in that class. ~\parencite{chidamber1994metrics}

\end{comment}
%}}}

\subsection{Coupling between object classes}

The metric~\textit{coupling between object classes} (CBO) is measured by the amount of objects an object is coupled to.~\citet{chidamber1994metrics} define coupling as ``an object is coupled to another object if one of them acts on the other, i.e., methods of one use methods or instance variables of another''.  
%{{{
\begin{comment}
    Definition: CBO for a class is a count of the number of other 
    classes to which it is coupled. 
    Theoretical Basis: CBO relates to the notion that an object is 
    coupled to another object if one of them acts on the other, i.e., 
    methods of one use methods or instance variables of another. 
    As stated earlier, since objects of the same class have the same 
    properties, two classes are coupled when methods declared in 
    one class use methods or instance variables defined by the 
    other class.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

CBO is argued to predict the quality of the program's design. Tight coupling between objects hinders modularity and reuse of the components and subsequently increases the CBO count. In other words, coupling and the CBO count is encouraged to be kept at a minimum. Additionally, the CBO measure can be a indicator of the effort needed to test the program's code. A high CBO count is probable to warrant more testing.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    Viewpoints: 
    1) Excessive coupling between object classes is detrimental 
    to modular design and prevents reuse. The more inde- 
    pendent a class is, the easier it is to reuse it in another 
    application. 
    2) In order to improve modularity and promote encapsu- 
    lation, inter-object class couples should be kept to a 
    minimum. The larger the number of couples, the higher 
    the sensitivity to changes in other parts of the design, 
    and therefore maintenance is more difficult. 
    3) A measure of coupling is useful to determine how 
    complex the testing of various parts of a design are 
    likely to be. The higher the inter-object class coupling, 
    the more rigorous the testing needs to be.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

\subsection{Response for class}

The~\textit{Response for class} (RFC) metric deals with the amount of methods that are called when the object receives a message. If a high amount of method calls can potentially be executed when the object receives a message, also the testing and debugging of the object's class becomes more difficult. Additionally, when the RFC number is high, also the complexity of that class increases. With the help of the RFC metric, developers can draw conclusions on, for example, the effort needed to test the
class.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    The response set of a class is a set of methods that can 
    potentially be executed in response to a message received 
    by an object of that class.26 The cardinality of this set is 
    a measure of the attributes of objects in the class. Since it 
    specifically includes methods called from outside the class, it 
    is also a measure of the potential communication between the 
    class and other classes. 
    Viewpoints: 
    1) If a large number of methods can be invoked in response 
    to a message, the testing and debugging of the class 
    becomes more complicated since it requires a greater 
    level of understanding required on the part of the tester. 
    2) The larger the number of methods that can be invoked 
    from a class, the greater the complexity of the class. 
    3) A worst case value for possible responses will assist in 
    appropriate allocation of testing time. 
    ~\parencite{chidamber1994metrics}.
\end{comment}
%}}}

\subsection{Lack of cohesion in methods}

Cohesiveness roughly translates to the level of uniformity of a class. The greater the cohesiveness of a class's methods is, the greater the encapsulation is and subsequently the quality of design is heightened. A lack of cohesion increases complexity, which in turn increases the effort required to maintain and test the class. Low cohesion implies that the class has too many responsibilities and would benefit from division to smaller
classes.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    therefore is a measure of the attributes of an object class. 
    Viewpoints: 
    1) Cohesiveness of methods within a class is desirable, 
    since it promotes encapsulation. 
    2) Lack of cohesion implies classes should probably be 
    split into two or more subclasses. 
    3) Any measure of disparateness of methods helps identify 
    flaws in the design of classes. 
    4) Low cohesion increases complexity, thereby increasing 
    the likelihood of errors during the development process. 
    ~\parencite{chidamber1994metrics}.
\end{comment}
%}}}

The~\textit{lack of cohesion in methods} (LCOM) metric measures the level of cohesion in an object class. Namely, it calculates the amount of method pairs in a class that have no relation to each other. For example, if two methods do not share the use of an instance attribute of the object, their LCOM measure is zero. In essence, the LCOM metric ``provides a measure of the relative disparate nature of methods in the class''~\parencite{chidamber1994metrics}.
%{{{
\begin{comment}
    The LCOM is a count of the number of method pairs whose 
    similarity is 0 (i.e., U ( )  is a null set) minus the count of method 
    pairs whose similarity is not zero. The larger the number 
    of similar methods, the more cohesive the class, which is 
    consistent with traditional notions of cohesion that measure 
    the inter-relatedness between portions of a program. If none 
    of the methods of a class display any instance behavior, i.e., 
    do not use any instance variables, they have no similarity and 
    the LCOM value for the class will be zero. The LCOM value 
    provides a measure of the relative disparate nature of methods 
    in the class. A smaller number of disjoint pairs (elements of set 
    P) implies greater similarity of methods. LCOM is intimately 
    tied to the instance variables and methods of a class, and 
    therefore is a measure of the attributes of an object class.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

\section{Modularity metrics}

The complexity of an application is managed by structuring and modularizing the source code into logical parts that lessen the effort of maintenance and further development. For instance, namespaces are used to group similar program code entities together, classes encapsulate shared functionality and components provide reusable features. Although modularization of complex entities is not unique to software engineering, several principles for it have been
presented.~\parenciteseveral{beck2011congruence}.
%{{{
\begin{comment}

\end{comment}
%}}}

The principles of~\textit{low coupling} and~\textit{high cohesion} were first proposed by~\citet{stevens1974structured}, who argue that better program code can be achieved by encapsulating similar functionality with similar dependencies to one entity (high cohesion), and in turn, separating functionality that have few dependencies between each other (low coupling) to separate entities~\parencite{beck2011congruence}. 

Furthermore,~\parencite{parnas1971information} introduced the notion of ``information hiding'', where restricting information about components and libraries to their interfaces are seen beneficial for the cycle of development. This encourages well-behaved use of other modules without compromising the control of the authors on the internal implementation of those modules~\parencite{parnas1971information}. 
%{{{
\begin{comment}
    Our concerns about the inconsistent decision orderings were based 
    on the assumption that information would be used shortly after the cor-
    responding decision. The restrictions placed by the three considerations 
    are considerably relaxed if we have the possibility of hiding some decisions 
    from each group. For example, we have noted a conflict between the desire 
    to produce an external specification early and the desire to produce a 
    system for which the external interface is easily changed. We can avoid 
    that conflict by designing the external interface, using it as a check 
    on the remaining work, but hiding the details that we think likely to 
    change from those who should not use them.~\parencite{parnas1971information}.
\end{comment}
%}}}

An often cited principle of system design is~\textit{Conway's law}, which holds that ``organizations which design systems (used in the broad sense here) are constrained to produce designs which are copies of the communication structures of these organizations''~\parencite{conway1968committees}. Although not a direct principle of software engineering, the notion of Conway's law has implications for the modularization degree and quality in software.

~\parencite{beck2011congruence} provide several metrics for assessing the level of coupling and quality of a software's modularity. The metric~\textit{structural dependencies} is formed in three ways: two classes are coupled if they are (1) related by inheritance, (2) related by aggregate class variables used by another class, (3) related by direct usage, e.g. as a local variable or as a function parameter.

~\textit{Fan-Out Similarity (FO)}~\parencite{beck2011congruence} addresses the fact that the coupling of two entities can be indirect. For example, two classes can be related by extending the same class or implementing similar interfaces. Furthermore, the two classes can aggregate similar classes and components, thus increasing their coupling. Also, simply utilizing the same entities increases the coupling of two classes.

~\textit{Evolutionary coupling}~\parencite{beck2011congruence} occurs when two seemingly unrelated entities are often changed simultaneously during development. The metric~\textit{support} quantifies the amount of common changes between the two entities.~\textit{Confidence} normalizes the support-metric by the total number of changes of one of the artifacts.

The~\textit{code clone} metric~\parencite{beck2011congruence} determines the duplication of code in the application by comparing two classes and extracting their similarity in relevance to code. This is derived from the observation that changing such a ``code clone'' also requires the modification of other similar application parts, thus resulting in an increased coupling. This is derived from the observation that modifying such a ``code clone'' also requires the modification of other similar application parts, thus resulting in an increased coupling. However, one can argue that sometimes duplication is desirable, e.g. for performance reasons.

The last metric provided by~\parencite{beck2011congruence} is~\textit{semantic similarity}, where the source code is interpreted as plain text documents and similar vocabulary and domain concepts between documents indicate a higher coupling. Parts that are reoccurring but otherwise irrelevant to the logic of the program are ignored. This includes license texts, namespace imports and some language-specific keywords. The problem of semantic similarity is that circumstances affecting the vocabulary are numerous: Author information, references on other code entities and design patterns introducing certain naming conventions are examples of ways that can reduce the accuracy of metrics such as semantic similarity.

~\citet{beck2011congruence} test the coupling metrics by applying them into 16 Java software projects and inspecting their effect on modularity. Structural dependencies (SD) received high correlation values and the authors estimate that the result stems from the fact that low coupling and high cohesion modularization principle~\parencite{stevens1974structured} is widely used in software design. Information hiding~\parencite{parnas1971information} also received correlation through the metrics fan-out similarity and evolutionary coupling. Code ownership resulted in low correlation, thus decreasing the plausibility of Conway's law~\parencite{conway1968committees} in software design. The rest of the modularization metrics showed medium-to-slight correlation values.

~\citet{sarkar2007api} provide several modularity and coupling metrics that specifically inspect the effect of Application Programming Interfaces (APIs) on the quality of the system as a whole. The authors argue that software generally considered as of high quality also exhibit above-average interfaces, which in turn, leads to high cohesion, low coupling and information hiding. The metrics were tested on several highly regarded open-source software projects, such as MySQL, GCC and the Linux Kernel. The metrics were controlled for results by generating module sets that contained randomly assigned functions and files.

Several of the metrics presented by~\citet{sarkar2007api} show degraded values when the human-formed modular structure of the software is disassembled. However, as often is with software metrics, no single value should be used for drawing conclusions about the software. Additionally, the validation of the metrics is still insufficient and further research is warranted.
%{{{
\begin{comment}
    These observations regarding the various metrics as we
    go from human modularization to random modularization
    speak to the fact that any single metric all by itself does not
    tell us much about the quality of modularization. For a
    composite picture of the quality of modularization, one has
    no choice but to examine all the metrics.~\parencite{sarkar2007api}.
\end{comment}
%}}}

\section{Fault prediction models}

Fault prediction has been a central research topic in software metrics for detecting problematic software components~\parencite{catal2009systematic,hall2012systematic,radjenovic2013software}. Fault prediction is seen important because non-trivial systems are larger than the finite quality assurance resources allocated for the project. Thus the ability of reliably pinpointing targets for inspection is crucial and can prevent often occurring ``blind spots'' in quality assurance~\parencite{menzies2010defect}. Fault prediction utilizes, in addition to metrics, modeling techniques that are specifically tuned for locating possible faults or problematic modules in the software.~\citet{radjenovic2013software} show that statistical analysis is the most prevalent modeling technique, followed by machine learning techniques and correlation analysis. However, concentrating on the models themselves may degrade the quality of research since they still need to be bundled with high-quality defect predictors (metrics)~\parencite{menzies2010defect}.
%{{{
\begin{comment}
    Recent results show that better data mining technology is not leading to better
    defect predictors. We hypothesize that we have reached the limits of the standard
    learning goal of maximizing area under the curve (AUC) of the probability of false
    alarms and probability of detection ¿AUC(pd, pf)¿; i.e. the area under the curve of a
    probability of false alarm versus probability of detection.
    Accordingly, we explore changing the standard goal. Learners that maximize~\parencite{menzies2010defect}.
\end{comment}
%}}}

A persistent problem in defect predicting is the ``ceiling effect'' of the efficacy of the models. For example,~\citet{menzies2007data} utilize~\textit{naive Bayes classifiers} for modelling the results of the research's static code metrics. The simple model is still more effective than complex approaches~\parencite{menzies2010defect}.

One of the currently more active approaches to measuring and predicting the quality of software is the inclusion of version change history as a measurement dimension~\parencite{zimmermann2005mining,hassan2009predicting}. These so-called ``change metrics'' presume that problematic components can be identified by the focus they receive during development.~\citet{zimmermann2005mining} apply~\textit{evolutionary coupling} as an indicator of components that would benefit from maintenance. Evolutionary coupling is simply deduced by the amount of changes to any two modules in the same revision of the version history. For example, when two source code files change, their evolutionary coupling number is increased. After inspecting the whole revision history, the evolutionary coupling count between modules is argued to be a good indicator for maintenance effort~\parencite{zimmermann2005mining}. The metric can also be tuned more precise, by e.g. inspecting the evolutionary coupling of methods and functions.
%{{{
\begin{comment}
    Both files have been changed together 20 times, indicat-
    ing some evolutionary coupling. This is not a very strong
    coupling, though, since ComparePreferencePage.java
    has been changed 40 times overall¿that is, it has been
    changed 20 times without plugin.properties being
    changed at the same time.
    To obtain more details, we can increase the granularity
    from files to entities and determine the evolutionary
    coupling between the individual attributes and functions
    defined in ComparePreferencePage.java. This reveals
    new couplings¿for instance, a coupling between the
    fKeys[] attribute and the initDefaults() method as
    well as a coupling between the fKeys[] attribute and the
    plugin.properties file. Both couplings are strong: In
    10 out of 11 times that fKeys[] has been changed,
    plugin.properties has been changed, too.~\parencite{zimmermann2005mining}.
\end{comment}
%}}}

\section{Challenges of metrics}

None of the metric suites presented in research literature are without their criticism. For example,~\citet{hitz1996chidamber} show that some of the metrics of~\citet{chidamber1994metrics} can be easily misused by not establishing a solid measurement environment. In essence, measuring software's internal attributes is highly context-specific. Concentrating on irrelevant attributes is bound to cause skewed measurement results and thus active engagement in resolving the truly important aspects of the software is required~\parencite{hitz1996chidamber}. This seems to be the core problem of software metrics: Determining what to measure, what data is relevant and how to interpret the results. The problem is further escalated by the fact that many metrics serve only as ``proxies'' to relevant measures, making the recognition of what to measure even harder. 
%{{{
\begin{comment}
    Before any measurement activity, we must identify the attribute to 
    be measured. Such an attribute must bear a certain significance for 
    a person involved in the development process, such as a designer, 
    programmer, manager, user, etc. The attribute might not necessar- 
    ily be interesting per se, but might serve as an independent vari- 
    able for indirect measurement of another (interesting!) attribute or 
    in a given prediction model (a case we do not consider further 
    here), however, one should avoid collecting data about meaning- 
    less aspects of the software document under investigation (just 
    because they happen to be easily collectible). 
    In the next step, a "sufficient" empirical relation system must ~\parencite{hitz1996chidamber}
\end{comment}
%}}}

Even after decades, no uniform guidelines for measuring software exist~\parencite{kitchenham2010s}. Since metrics are scrutinized closely by the research community, also their theoretical foundation can be often invalidated. For example,~\citet{kitchenham2010s} notes that the metric suite of~\citet{chidamber1994metrics} is long been theoretically invalidated, and as a consequence empirical results of those metrics are also invalid. This results in a large body of studies and results that are potentially invalid in the scientific sense. On the other hand, many researchers acknowledge the situational nature of metrics and seem to concentrate more on real-world scenarios~\parencite{kitchenham2010s}. As is, the best of both worlds (solid ontologies for creating metrics and guidelines for empirical studies) is still yet to emerge.
%{{{
\begin{comment}
    It seems that empirical validation papers are of major impor-
    tance to the software metrics research community. However, this
    study suggests that the limitations of empirical studies are not al-
    ways understood, in particular, the impact of the context in which
    data arises, the impact of constructed metrics (such as defect den-
    sity, change density), the implication of linear relationships, and
    the notion of measures needing to be identically, independently,
    distributed. Furthermore, we appear to have no idea when to stop
    empirical validation, even in the case of well-investigated metrics
    such as the Chidamber¿Kemerer metrics.
    I believe the software engineering research community must
    take up the challenge to aggregate its results into an empirically-
    based body of knowledge. The evidence from this mapping study~\parencite{kitchenham2010s}

    Currently many studies use multiple data sets. However, most
    researchers still construct post-hoc explanations for different out-
    comes rather than refining their hypotheses to predict differences.
    Overall, we need to reflect more on what we mean by ¿¿empirical
    validation¿ and identify appropriate methods for performing such
    studies. The complexity of the interaction between context and
    outcomes suggest support for Fenton and Neil¿s proposals to model~\parencite{kitchenham2010s}

\end{comment}
%}}}

The sheer number of metrics formed also contribute to the difficulty of comparing and validating the results. For example, object-oriented metrics suite studies from~\citet{briand2000exploring} and~\citet{briand2001replicated} deal with 67 and 54 different metrics~\parencite{kitchenham2010s}. Needless to say, the field would most likely benefit from a higher level of cooperation, where researchers would share measurement frameworks and increase the chance of comparable results. In addition,~\citet{kitchenham2010s} criticizes the field of advancing too fast to empirical research to validate the presented metrics, when in actuality, the background theory is not sufficiently established. Indeed, producing research data out of case studies is pointless when the theory used cannot be reliably applied to other settings as well.~\citet{kitchenham2010s} concludes that more attention is to be put on the unique contexts in which research is conducted and to consider the overall use of the metrics and their validation theory. 

\section{Summary}

The field of software measurement has existed as a research discipline since the first computer systems have been developed. In this chapter, all of the classic static code quality metrics were presented and the current trends were analyzed. Metrics specific to the object-oriented programming paradigm dominate in popularity, but researchers are also turning to other aids such as mining data out of version control histories~\parencite{menzies2007data}. This proves that the field is still healthy and many research topics are undoubtedly yet to emerge. 

With the increase of programming languages and complex systems, the need for researching software metrics is evident. For example, even in safety-critical systems, the requirements of the software are easily overlooked, since every corner case is hard to be taken into consideration upfront~\parencite{lutz2003operational}. This advocates the use of automated tools to aid in discovering defects in the software. Thus the quantitative nature of software metrics is an important asset when dealing with nontrivial systems, where finite quality assurance resources limit the extent to which modules can be manually tested.

%{{{Additional sources
\begin{comment}
    This type of overriding can be particularly troublesome if the conflicting defi- nitions have the same type, which prevents the compiler from indicating possibly erroneous behaviour when modifying the program. For instance, consider the (contrived) area function in Example 10. If the a in the where clause is renamed, the function will compile with no errors, but will give incorrect results unless the a after the = is also changed.
    It therefore appears that measuring the number of pattern variables involved in overriding might predict potential points of error. However, the implementation of the metric used in this work does not yet contain a type system, and so for this work it is not possible to check if the variables involved in the overriding have the same type, which would be interesting to factor into the measurement, and so this implementation of the metric treats all overriding in the same manner, regardless of the types involved. However it would be possible to re-implement the metric using a framework such as Programatica [41] to make use of type information in~\parencite{ryder2004software}


    In the world of imperative and object-oriented languages, software measure- ment, also known as software metrics, has been used for many years to provide developers with additional information about their programs. Such information can give programmers important indications about where bugs are likely to be in- troduced, or about how easy parts of a program are to test, for instance. This can be extremely valuable in easing the testing process by focusing programmers¿ at- tention on parts
    of the program where their effort may provide the greatest overall benefit, which in turn can help ease the whole process of validating software.~\parencite{ryder2004software}.

    The example of the imperative and object-oriented communities suggests that software metrics could provide a useful complement to the existing debugging tools available to functional programmers today. Some of the measurement techniques from imperative and object-oriented languages may transfer quite cleanly to func- tional languages, for instance the pathcount metric which counts the number of execution paths through a piece of program code, but some of the more advanced features of
    functional programming languages may contribute to the complexity of a program in ways that are not considered by traditional imperative or object- oriented metrics. It may therefore be necessary to develop new metrics for certain aspects of functional programs.~\parencite{ryder2004software}

    Software measurement is a technique in which quantitative measures, often called metrics, are taken from the source code of a program. Typically metrics attempt to quantify how complex a piece of source code is to understand, modify or test. This notion of complexity should not be confused with the computational complexity of an algorithm, which is typically denoted using O( notation. Com- putational complexity is concerned with runtime behaviour, rather than how easy or hard it is for a
    programmer to maintain.~\parencite{ryder2004software}

    One of the claims of software measurement is that it can help identify the parts of a system which are most likely to benefit from inspection. This allows resources to be focused where they will help most and allows them to be used in their most effective way, e.g. peer reviews of small sections of code, rather than large sections, or indicating parts of a program that may benefit from refactoring code changes.~\parencite{ryder2004software}

    Abstract¿The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ¿McCabes versus Halstead versus lines of code counts¿ for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected.~\parencite{menzies2007data}. <-- Hyvaa pointtia siita kuinka metriikka itsessaan ei tarkoita mitaan, vaan kuinka sita kaytetaan!  

    Given recent research in artificial intelligence, it is now
    practical to use data miners to automatically learn
    predictors for software quality. When budget does not
    allow for complete testing of an entire system, software
    managers can use such predictors to focus the testing on
    parts of the system that seem defect-prone. These potential
    defect-prone trouble spots can then be examined in more
    detail by, say, model checking, intensive testing, etc.~\parencite{menzies2007data} <-- Hyvaa pointtia machine learningin kaytosta

    This brittleness result offers a new insight on prior work.
    Prior results about defect predictors were so contradictory
    since they were drawn from a large space of competing
    conclusions with similar but distinct properties. Different
    studies could conclude that, say, lines of code are a better/
    worse predictor for defects than the McCabes complexity
    attribute, just because of small variations to the data.
    Bayesian methods smooth over the brittleness problem by
    polling numerous Gaussian approximations to the nu-
    merics distributions. Hence, Bayesian methods do not get
    confused by minor details about candidate predictors.~\parencite{menzies2007data}

\end{comment}
%}}}

%}}}

%{{{Functional programming
\chapter{Functional programming}
%{{{Chapter questions
\begin{comment}
\end{comment}
%}}}

A recurring challenge of software engineering is to develop applications that are not complex to comprehend and reason about. Actions towards this complexity is actively being taken by e.g. refactoring~\parencite{fowler1999refactoring}, and by analyzing the quality of the software with metrics~\parencite{boehm1976quantitative}. In turn,~\textit{modularization} is in the heart of software engineering, where the aim is to have loosely-coupled relationships between
components and the possibility of changing and upgrading these components with least effort possible~\parencite{hughes1989functional}. Effectively, software that has a well-thought structure is easier to develop, debug and maintain.
%{{{
\begin{comment}
As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write, easy to debug, and provides a collection of modules that can be re-used to reduce future programming costs. Conventional languages place conceptual limits on the way problems can be modularised.~\parencite{hughes1989functional}.

\end{comment}
%}}}

Several programming paradigms exist for developing software. The most popular paradigm is the~\textit{imperative} approach, where programs are written in step-by-step instructions and commands~\parencite{sebesta2002concepts}. This approach is similar to the design of the von Neumann computer architecture and its basic implementation of sending messages back and forth between the central processing unit and the memory~\parencite{backus1978can}. In turn, many programming languages have
adapted the same approach as in the von Neumann architecture to their execution flow: variables, control statements and assignment statements~\parencite{backus1978can}.
%{{{
\begin{comment}
In its simplest form a von Neumann computer has three parts, a central processing unit (or CPU) a store, and a connecting tube that can transmit a single word between the CPU and the store (and send an address to the store).~\parencite{backus1978can}.

All have been designed to make efficient use of von Neumann architecture computers. Although the impera- tive style of programming has been found acceptable by most programmers, its heavy reliance on the underlying architecture is thought by some to be an unnecessary restriction on the alternative approaches to software development.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

The critics of imperative languages argue that as programs grow large, the imperative approach inherently struggles with expressing non-trivial programs consisting of numerous modules in a succinct manner~\parencite{backus1978can,hughes1989functional}. For example, the inclusion of explicit variables forces the developers to understand all of them concurrently, and in the worst case, keep a mental model of the relationships of these variables~\parencite{moseley2006out}.
%{{{
\begin{comment}
One of the fundamental characteristics of programs written in impera- tive languages is that they have state, which changes throughout the execution process. This state is represented by the program¿s variables. The author and all readers of the program must understand the uses of its variables and how the program¿s state changes through execution. For a large program, this is a daunting task. This is one problem with programs written in an imperative language that is not present in
a program written in a pure functional language, for such programs have neither variables nor state.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

Functional programming is a different approach to software engineering compared to imperative languages. It utilizes mathematical functions for program flow, such as recursion and conditional expressions, in contrast to the characteristic of sequential and iterative repetition found in imperative languages~\parencite{sebesta2002concepts}. Moreover, variables that model values in memory locations do not exist in mathematics, and as such, are also omitted from pure functional
programming. 
%{{{
\begin{comment}
One of the fundamental characteristics of mathematical functions is that the evaluation order of their mapping expressions is controlled by recursion and conditional expressions, rather than the sequencing and iterative repetition that are common to the imperative programming languages.~\parencite{sebesta2002concepts}

However, a subprogram in an imperative language may depend on the current values of several nonlocal or global variables. This makes it difficult to determine statically what values the subprogram will produce and what side effects it will have on a particular execution.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

This thesis uses the Haskell programming language~\parencite{marlow2010haskell} to demonstrate the concepts of functional programming. Haskell, named after the logician Haskell B. Curry, is a multi-purpose functional programming language. Used both in education, research and by practitioners, Haskell has enjoyed steady popularity as one of the modern functional languages. Albeit several other alternatives (and predecessors) exist, Haskell is one of the few languages following the functional paradigm in a ``pure'' manner. Additionally, as its roots are in academia, also education is one of the language's design motives.~\parenciteseveral{marlow2010haskell}. However, this thesis does not serve as an introduction to Haskell itself, but rather to the main concepts of functional programming. The syntax of Haskell is not explained, although expressiveness, one of the goals of the functional paradigm, should aid in understanding the examples provided in the chapter.
%{{{
\begin{comment}
The committee's primary goal was to deisgn a language that satisfied these constraints:
1. It should be suitable for teaching, research, and applications, including building large systems.
2. It should be completely described via the publication of a formal syntax and semantics.
3. It should be freely available. Anyone should be permitted to implement the language and distribute it to whomever they please.
4. It should be based on ideas that enjoy a wide consensus.
5. It should reduce unnecessary diversity in functional programming languages.
~\parencite{marlow2010haskell}
\end{comment}
%}}}

%{{{Additional sources
\begin{comment}
    Haskell is a high level language. A really high level language. We can spend our days programming entirely in abstractions, in monoids, functors and hylomorphisms, far removed from any particular hardware model of computation. The language specification goes to great lengths to avoid prescribing any particular evaluation model. These layers of abstraction let us treat Haskell as a notation for computation itself, letting the programmer concentrate on the essence of their problem without getting bogged down in low level implementation decisions. We get to program in pure thought.~\parencite{o2008real}

    The earliest programming languages were developed with one simple goal in mind: to provide a vehicle through which one could control the behavior of computers. Not sur- prisingly, the early languages reflected the structure of the underlying machines fairly well. First, it became obvious that what was easy for a machine to reason about was not necessarily easy for a human being to rea- son about.~\parencite{hudak1989conception} In Conception_evolution_and_application.....pdf

    The class of functional, or applicative, programming languages, in which computation is carried out entirely through the evaluation of expressions, is one such family of languages, and debates over its merits have been quite lively in recent years.~\parencite{hudak1989conception}

    Among the claims made by functional language advocates are that programs can be written quicker, are more concise, are higher level (resembling more closely traditional mathematical notation), are more amenable to formal reasoning and analysis, and can be executed more easily on parallel architectures. Of course, many of these features touch on rather subjective issues, which is one reason why the debates can be so lively.~\parencite{hudak1989conception}.

    TODO: Maininta myös Lispistä!! Esim. hudak1989conception :ssa hyvää juttua
    Despite its impurities, Lisp had a great influence on functional language development, and it is encouraging to note that modern Lisps (especially Scheme) have returned more to the purity of the lambda calculus rather than the ad hocery that plagued the Maclisp era. This return to the purity includes the first-class treatment of functions and the lexical scoping of identifiers.~\parencite{hudak1989conception}.

    A purely functional programming language does not use variables or assignment statements, thus freeing the programmer from concerns related to the memory cells, or state, of the program. Without variables, iterative con- structs are not possible, for they are controlled by variables.~\parencite{TODO: Better source than sebesta}.

    Haskell functions are in general pure functions: when given the same arguments, they return the same results. The reason for this paradigm is that pure functions are much easier to debug and to prove correct. Test cases can also be set up much more easily, since we can be sure that nothing other than the arguments will influence a function's result. We also require pure functions not to have side effects other than returning a value: a pure function must be self-contained, and cannot open a network connection, write a file or do anything other than producing its result. This allows the Haskell compiler to optimise the code very aggressively.
    However, there are very useful functions that cannot be pure: an input function, say getLine, will return different results every time it is called; indeed, that's the point of an input function, since an input function returning always the same result would be pointless. Output operations have side effects, such as creating files or printing strings on the terminal: this is also a violation of purity, because the function is no longer self-contained.~\parencite{wikibooks2014haskell}

    The basic difference between functional and imperative programming lan-
    guages lies in the hiding of the computational model (Petre & Winder, 1990). 
    The imperative model incorporates the Von Neuman machine characteristics 
    in the notions of assignment, state and effect. A characteristic of this language 
    class is the explicit flow of control, e.g. sequencing, selection and repetition. In 
    assignments, the value of memory places denoted with variables is changed 
    during program execution. This model of operation by change of state and by 
    alteration of variable values is also named 'computation by effect'. The func-
    tional model is characterised by 'computation by value'. Functions return val-
    ues and have no side-effects, and expressions represent values. There is no no-
    tion of updatable memory accessible by instruction. The program consists of a 
    script with a number of mathematical-like definitions and an expression that 
    must be evaluated. Functions can be passed as arguments to other functions 
    and can be the result of a function application (higher-order functions). These 
    programs possess the property of referential transparency, which means that 
    in a fixed context the replacement of a subexpression by its value is completely 
    independent of the surrounding expression. Therefore functional programming 
    is more closely related to mathematical activities (Bird & Wadler, 1988). ~\parencite{van1995software} <- Hyvaa johdantoa FP:hen!
\end{comment}
%}}}

\section{Concepts}

The different concepts of functional programming are numerous, but shared between languages. This section aims to provide an overview into the features of functional languages, as well as their strengths and weaknesses. 

\subsection{Functions}

The main motivation behind the functional programming paradigm is the use of mathematical functions in expressing programs concisely. Unlike in imperative languages such as C++, functions in functional languages resemble the mathematical notion of functions. That is, rather than specifying values over sequential operations, function parameters are meant to be mapped over values.~\parenciteseveral{hughes1989functional}. A function that returns an integer doubled by itself (effectively calculating the square of a number) would be defined in Haskell as:

\begin{lstlisting}
square x = x * x 
\end{lstlisting}

\noindent The assignment operator, {\tt =} is meant to define a function (in this case, the computation of a number's square) and not to actually assign any values to a variable. Additionally, the parameters of the function definition are fixed (constant), meaning they can not change in the domain of the function. This is the opposite of the behaviour in imperative languages, where variables received as parameters are often manipulated and their state is changed. In pure functional languages, such as Haskell, variables are omitted and state does not exist in an operational or denotational sense. This can be observed through~\textit{referential transparency}, which holds that a function should return the same result every time with the same given parameters, since no ``external'' variables, or~\textit{side-effects}, can affect the computation.~\parenciteseveral{hudak1989conception}.
%{{{
\begin{comment}
In this definition, the domain and range sets are the real numbers. The symbol K is used to mean ¿is defined as.¿ The parameter x can represent any member of the domain set, but it is fixed to represent one specific element during evalu- ation of the function expression. This is one way the parameters of mathemati- cal functions differ from the variables in imperative languages.~\parencite{sebesta2002concepts}.

Without variables, the execution of a purely functional program has no state in the sense of operational and denotational semantics. The execution of a function always produces the same result when given the same parameters. This feature is called referential transparency.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

A particular inspiration for functional languages is the~\textit{lamdba calculus}~\parencite{church1932set}. Its main purpose is to define the behavior of functions in an intuitive manner, where computations are the central aspect. In particular, the functions in lamdba calculus have one key abstraction: They can be applied to themselves. In addition to functions being anonymous in lamdba calculus, recursion does not have to be explicitly stated because of the general applicability of the functions.~\parenciteseveral{hudak1989conception}. For an example of lamdba calculus, the following expression results in 8 as the value of the computation:

\begin{lstlisting}[mathescape]
$(\lambda (x)x * x * x)(2)$
\end{lstlisting}

\noindent Although the semantics of lamdba calculus are essentially small and simple~\parencite{hudak1989conception}, its application is more involved and thus is not described in this thesis. In conclusion, lambda calculus serves as the basis for functions in functional programming languages.
%{{{
\begin{comment}
Church's work was motivated by the desire to create a calculus (informally, a syntax for terms and set of rewrite rules for transforming terms) that captured one's intuition about the behavior of cuntions. This approach is counter to the consideration of functions, as for example, sets (more precisely, sets of argument/value pairs), since the intent was to capture the computational aspects of functions.~\parencite{hudak1989conception}.

Its [lambda calculus] type-free nature yielded a particularly small and simple calculus, and it had one very interesting property, capturing functions in their fullest generality: Functions could be applied to themselves.~\parencite{hudak1989conception}

This ability of self-application is what gives the lambda calculus its power. It allows us to gain the effect of recursion without explicitly writing a recursive definition.~\parencite{hudak1989conception}.
\end{comment}
%}}}

%{{{ Additional sources
\begin{comment}
The functional programming paradigm, which is based on mathematical functions, is the design basis of the most important nonimperative styles of languages. This style of programming is supported by functional programming languages.~\parencite{sebesta2002concepts}.

\end{comment}
%}}}

\subsection{Higher-order functions}

Higher-order functions are the landmark feature of functional languages. Derived from the previous observation of generally applicable functions in lamdba calculus, higher-order functions can take functions as parameters and also return a function~\parencite{hudak1989conception}. Thus functions are treated like any other value. This has convenient practical implications, where functions can be composed of other functions. For example, composing a function that executes a given function twice is possible:
\begin{lstlisting}
twice f = f . f
\end{lstlisting}

\noindent Now {\tt twice sum 4}, where {\tt sum} is {\tt sum x = x * x}, would return 256. The function twice is now~\textit{curried} (i.e.\ composes multiple functions) and returns a new function. As {\tt twice} is a function it can be used again for function composition, et cetera. This kind of ``glueing'' together functions increases modularity of software written in a functional manner. Treating functions as first-class values also provides a way to abstract over functional behavior.~\parenciteseveral{hudak1989conception}.
%{{{
\begin{comment}
A higher-order function, or functional form, is one that either takes one or more functions as parameters or yields a function as its result, or both.~\parencite{sebesta2002concepts}.

If functions are treated as first-class values in a language--allowing them to be stored in data structures, passed as arguments, and returned as results--they are referred to as higher-order functions.~\parencite{hudak1989conception}.

That glueing property comes not just from the ability to compose functions but also from the ability to abstract over functional behavior as described above.
\end{comment}
%}}}

One of the major benefits of functions as first-class values is their property of abstracting the manipulation of data into succinct higher-order functions. For example, a function that ``folds'' over a data structure and returns a result value can be expressed as: 
\begin{lstlisting}
fold f z [] = z
fold f z (x:xs) = f x (fold f z xs)
\end{lstlisting}

The {\tt fold} function can then be used to accumulate values from a data structure, and the actual logic of the accumulator is passed on to {\tt fold}:
\begin{lstlisting}
fold (+) [1,2,3,4,5] -- Returns 15
fold (-) [1,2,3,4,5] -- Returns -15
fold (*) [1,2,3,4,5] -- Returns 120
\end{lstlisting}

As can be seen, the actual data handling logic is separated from the data structure traversal. This leads into modular programs where components are likely to be reusable, thus resulting in fewer bugs and increasing development time.

\subsection{TODO: Non-strict semantics and lazy evaluation}

Derived from lambda calculus, a~\textit{non-strict} programming language such as Haskell uses avoids evaluating values until they are actually needed. In contrast,~\textit{strict} languages like Java and C++ utilize mechanisms such as~\textit{call-by-value} where function parameters are eagerly computed. Deferring evaluation of parameter values offers the obvious advantage of saving computation time resulting in more efficient programs. On the other hand,~\textit{lazy evaluation} also grants the possibility of defining unbounded data structures such as infinite lists.~\parenciteseveral{hudak1989conception}. 
%{{{
\begin{comment}
A programming language is strict if it requires all actual parameters to be fully evaluated, which ensures that the value of a function does not depend on the order in which the parameters are evaluated. A language is nonstrict if it does not have the strict requirement.~\parencite{hudak1989conception}.

First, nonstrict languages are gener- ally more efficient, because some evaluation is avoided.13 Second, some inter- esting capabilities are possible with nonstrict languages that are not possible with strict languages. Among these are infinite lists. Nonstrict languages can use an evaluation form called lazy evaluation, which means that expressions are evaluated only if and when their values are needed.~\parencite{hudak1989conception}.

The idea is that a programmer should be able to describe a specific data structure without worrying about how it gets evaluated.~\parencite{hudak1989conception}.
\end{comment}
%}}}

\begin{lstlisting}
-- Examples of infinite lists
positives = [1,2..]
evens = [2,4..]
squares = [x * x | x <- [0..]] 

\end{lstlisting}

\noindent The values of the lists are not computed until they are needed. For example, {\tt take 5 squares} returns {\tt [0, 1, 2, 4, 9, 16, 25, 36, 49, 64, 81]}. Effectively, lazy evaluation provides three important mechanisms for evaluating function parameters~\parencite{TODO: Better source than sebesta}:

\begin{itemize}
    \item Only the needed parameters are evaluated
    \item Only the needed part of a single parameter is evaluated
    \item An evaluated parameter is reused if it appears again in function calls
\end{itemize}
%{{{
\begin{comment}
zy evaluation means that an actual parameter is evaluated only when its value is necessary to evaluate the function. So, if a function has two parameters, but on a particular execution of the function the first parameter is not used, the actual parameter passed for that execution will not be evaluated. Furthermore, if only a part of an actual parameter must be evaluated for an execution of the function, the rest is left unevaluated. Finally, actual parameters are evaluated only once, if at all, even if the same actual parameter appears more than once in a function call.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

\noindent As previously presented, functions have the convenient property of being freely composable from other functions. In conjuction with lazy evaluation, this function composition becomes especially useful when ``glueing'' several functions together. A popular usage in functional programs is to include ``generators'' (functions) that produce arbitrarily large data sets. However, combining these generators with functions that include predicates for selecting the correct values provides a modular and concise way to structure applications.~\parencite{hughes1989functional}. This separation of data structures and control is also one of the central notions of functional programming~\parencite{hudak1989conception}.
%{{{
\begin{comment}
Since this method of evaluation runs f as little as possible, it is called 'lazy evaluation'. It makes it practical to modularise a program as a generator which constructs a large number of possible answers, and a sleector which chooses the appropriate one.~\parencite{hughes1989functional}.
\end{comment}
%}}}

\subsection{Polymorphic types}

Although a somewhat technical concept,~\textit{data types} are an essential part of modern functional languages~\parencite{hudak2007history}. In this section, algebraic and abstract data types are discussed and one of the main mechanism for their use, pattern matching, is presented.
%{{{
\begin{comment}
Data types and pattern matching are fundamental to most modern functional languages (with the notable exception of Scheme).~\parencite{hudak2007history}.
\end{comment}
%}}}

~\textit{Algebraic data types} increase the flexibility in which a computation can depend on type alternatives. In other words, the data types of functional languages are often~\textit{polymorphic}, i.e.\ they can represent multiple types.~\parenciteseveral{hudak2007history}. A popular example regarding algebraic data types is to represent the types of a binary search tree:
%{{{
\begin{comment}
In general, an algebraic type specifies a sum of one or more alter- natives, where each alternative is a product of zero or more fields. It might have been useful to permit a sum of zero alternatives, which would be a completely empty type, but at the time the value of such a type was not appreciated.~\parencite{hudak2007history}
\end{comment}
%}}}

\begin{lstlisting}
data Tree a = Leaf a | Branch (Tree a) (Tree a)
\end{lstlisting}

Here {\tt Tree} is a~\textit{data type}. {\tt Leaf} and {\tt Branch} are~\textit{data constructors}. What follows is that~\textit{pattern matching} can be used to conveniently address either of these data constructors in a function~\parencite{hudak2007history}. For example, a function that returns the size (the number of elements) of the tree recursively can be expressed as follows:

\begin{lstlisting}
size (Leaf x)     = 1
size (Branch l r) = size l + size r + 1
\end{lstlisting}

Effectively, {\tt size} receives a data type of {\tt Tree} as the only parameter. In fact, the function signature is {\tt size :: Tree a -> Int}. The pattern matching mechanism is then utilized for evaluating either expression, depending on the actual data constructor. 

Due to the nature of lazy evaluation, the actual implementation of pattern matching has proved to be challenging in functional languages. In particular, the order of parameters rise subtle problems depending on their evaluation mechanism~\parencite{hudak1989conception}. In the end, matching patterns from top to bottom and evaluating parameters from left to right serves as a sufficient compromise and is used in most modern functional languages~\parencite{hudak2007history}.
%{{{
\begin{comment}
In SASL, KRC, Hope, SML, and Miranda, matching against equa- tions is in order from top to bottom, with the first matching equation being used. Moreover in SASL, KRC, and Miranda, matching is from left to right within each left-hand-side¿which is important in a lazy language, since as soon as a non-matching pattern is found, matching proceeds to the next equation, potentially avoiding non- termination or an error in a match further to the right. Eventually, these choices were made for
Haskell as well ~\parencite{hudak2007history}.
\end{comment}
%}}}

~\textit{Abstract data types} address the need for increased modularity and security. Specifically, the actual data type is not exposed outside its module, but rather, can be accessed through utility functions. Similar to~\textit{interfaces} in Java, abstract data types allow changing the representation type without affecting external modules that depend on this ``interface''.~\parenciteseveral{hudak1989conception}.
%{{{
\begin{comment}
Another idea in data abstraction originating in imperative languages is the notion of an abstract datatype (ADT) in which the details of the implementation of a datatype are hidden from the users of that type, thus enhancing modularity and security.~\parencite{hudak1989conception}.

The advantage of this, of course, is that one is free to change the representation type without fear of breaking some other code that uses the ADT.~\parencite{hudak1989conception}.
\end{comment}
%}}}

\subsection{Functors}

Functors add flexibility when encountering different data types, such as the previously presented {\tt Tree} construct or an ordinary list data structure. As looping over the values of a construct would be convenient, functors aim at providing an abstraction for this kind of behavior. Namely, functors enable ``mapping'' functions over data types and is one of the central routines of functional programming.~\parenciteseveral{lipovavca2011learn}. Functors can be thought as
containers for values, or more precisely, as computational contexts~\parencite{typeclassopedia2014haskellwiki}. For example, {\tt Tree} can be made an instance of {\tt Functor}:
%{{{
\begin{comment}
And now, we're going to take a look at the Functor typeclass, which is basically for things that can be mapped over.~\parencite{lipovavca2011learn}

Another intuition is that a Functor represents some sort of ¿computational context¿. This intuition is generally more useful, but is more difficult to explain, precisely because it is so general~\parencite{typeclassopedia2014haskellwiki}
\end{comment}
%}}}
\begin{lstlisting}
instance Functor Tree where
  fmap f (Leaf x) = Leaf (f x)
  fmap f (Branch a b) = Branch (fmap f a) (fmap f b)
\end{lstlisting}

\noindent Now a given function can be applied (~\textit{mapped}) over the values in a {\tt Tree}:

\begin{lstlisting}
fmap (*2) (Branch (Leaf 2) (Leaf 3)) 
-- Outputs: Branch (Leaf 4) (Leaf 6)
\end{lstlisting}

Thus the values inside the {\tt Tree} ``context'' are changed, without affecting the computational context itself. Perhaps a more general example of a functor instance is the list data type, since it provides a map function similarly to the example {\tt Tree} structure~\parencite{typeclassopedia2014haskellwiki}. Derived from category theory, functors provide certain ``homomorphism'' or abstractions over functional data structures. As is the case with mathematics, certain laws must be satisfied for functors, expressed in Haskell code as: 
%{{{
\begin{comment}
From the context point of view, the intention is that fmap applies a function to a value without altering its context~\parencite{typeclassopedia2014haskellwiki}

As noted before, the list constructor [ is a functor ¿; we can use the standard list function map to apply a function to each element of a list ¿.~\parencite{typeclassopedia2014haskellwiki}.
\end{comment}
%}}}

\begin{lstlisting}
fmap id = id
fmap (g . h) = (fmap g) . (fmap h))
\end{lstlisting}

\noindent The first law dictates simply that mapping an ``identity'' function (i.e. {\tt x = x}) over a value should always return the value itself, unchanged. The second law asserts that mapping over functions that are then composited should be the same act as mapping over a result of function composition. If a functor instance satisfies the first law, also the second law is automatically passed.~\parenciteseveral{typeclassopedia2014haskellwiki}. As trivial as these laws seem, they are a powerful aid in reasoning about code written in a functional language.
%{{{
\begin{comment}
As far as the Haskell language itself is concerned, the only requirement to be a Functor is an implementation of fmap with the proper type. Any sensible Functor instance, however, will also satisfy the functor laws, which are part of the definition of a mathematical functor. There are two:

fmap id = id
fmap (g . h) = (fmap g) . (fmap h))

Together, these laws ensure that fmap g does not change the structure of a container, only the elements. Equivalently, and more simply, they ensure that fmap g changes a value without altering its context ¿.

The first law says that mapping the identity function over every item in a container has no effect. The second says that mapping a composition of two functions over every item in a container is the same as first mapping one function, and then mapping the other.

The second law says that composing two functions and then mapping the resulting function over a functor should be the same as first mapping one function over the functor and then mapping the other one.~\parencite{typeclassopedia2014haskellwiki}.
\end{comment}
%}}}

\subsection{Applicative functors}

Applicative functors as an explicit abstraction are relatively new in functional programming~\parencite{mcbride2008functional}. As their name implies, applicative functors are an extension for functors. As functions essentially take only one parameter (and are thus~\textit{curried} functions for subsequent parameters), a data type can naturally contain functions. For example, the leaves of a {\tt Tree} data type could contain functions as values: 
\begin{lstlisting}
fmap (*) (Branch (Leaf 2) (Leaf 3)))
\end{lstlisting}

Now the leaves contain multiplicating functions of 2 and 3 respectively. However, a plain functor can not extract a function out of a functor value ({\tt Leaf} in this case). As a result this recurring pattern of ``picking'' the function value out of the functor was abstracted as applicative functors~\parencite{mcbride2008functional}. The {\tt Tree} data type can be made as an instance of {\tt Applicative}:
\begin{lstlisting}[breakatwhitespace=false, breaklines=true]
instance Applicative Tree where
  pure = Leaf
  (Leaf f) <*> (Leaf x) = Leaf (f x)
  (Branch a b) <*> (Branch c d) = Branch (a <*> c) (a <*> d)
\end{lstlisting}

\noindent Now {\tt <*>} can be used for applying functor values:

\begin{lstlisting}[breakatwhitespace=false, breaklines=true]
(Branch (Leaf (*2)) (Leaf (*3))) <*> (Branch (Leaf 2) (Leaf 3))
-- Outputs Branch (Leaf 4) (Leaf 6) 
\end{lstlisting}

In essence, {\tt <*>} is just function application with a context~\parencite{typeclassopedia2014haskellwiki}. The {\tt pure} function ``embeds pure computations into the pure fragment of an effectful world''~\parencite{mcbride2008functional}. In other words, a pure value is lifted into the computational context of the {\tt Applicative} instance. Applicative functors must also satisfy certain laws: 
%{{{
\begin{comment}
The idea is that pure embeds pure computations into the pure fragment of an effectful world¿the resulting computations may thus be shunted around freely, as long as the order of the genuinely effectful computations is preserved.~\parencite{mcbride2008functional}.
\end{comment}
%}}}

\begin{itemize}
    \item The identity law:
          {\tt pure id <*> v = v}
    \item Homomorphism:
          {\tt pure f <*> pure x = pure (f x)}
    \item Interchange
          {\tt v <*> pure x = pure (\textbackslash f -> f x) <*> v}
    \item Composition
          {\tt pure (.) <*> u <*> v <*> w = u <*> (v <*> w)}
\end{itemize}

The identity law has the same principle as the one with functors. Homomorphism declares that applying a non-effectful function to a non-effectful value is the same as applying the function to a value and ``lifting'' the result in to the applicative context. Moreover, interchange shows that the order of applying an effectful function to a pure value is interchangeable. Composition deals with the associativity of the applicative property~\parencite{typeclassopedia2014haskellwiki}.

One real-world use case for applicative functors have been found in parsing applications. Parsers are a traditional software engineering problem that functional languages solve well by using combinatory parsing~\parencite{swierstra2009combinator}. In contrast to using monadic parsers~\parencite{hutton1996monadic}, an approach where the principles of applicative functors are used have proven to be likewise efficient, mainly because ``the structure of these parsers is independent of the results of parsing'', leading to lazy computation and thus to efficiency~\parencite{baars2004functional}. 

In conclusion, applicative functors are functors with effects. This has convenient implications for implementations where the full power of monads is not needed~\parencite{baars2004functional}.

\subsection{Monads}

Functional programming languages can roughly be divided into two categories; pure and impure approaches.~\textit{Pure} functional languages, such as Haskell, utilize lambda calculus in its simplest form, while~\textit{impure} languages, such as Scheme, introduce additional features, such as assignments, into computations. The main motivation behind ``pure'' languages is that they make data flow explicit and expressions only depend on their free variables. This enables a powerful aid in reasoning about the code's logic, since substitution is always valid(TODO: ??). On the other hand,
impure features in a language can offer more conveniency for the developer as the concept of purity is not strictly enforced.~\parenciteseveral{wadler1995monads}.
%{{{
\begin{comment}
The functional programming community divides into two camps. Pure languages,
such as Miranda2 and Haskell, are lambda calculus pure and simple. Impure lan- guages, such as Scheme and Standard ML, augment lambda calculus with a number of possible effects, such as assignment, exceptions, or continuations. Pure languages are easier to reason about and may benefit from lazy evaluation, while impure lan- guages offer efficiency benefits and sometimes make possible a more compact mode of expression.~\parencite{wadler1995monads}.

A program in a pure functional language is written as a set of equations. Ex- plicit data flow ensures that the value of an expression depends only on its free variables. Hence substitution of equals for equals is always valid, making such pro- grams especially easy to reason about. Explicit data flow also ensures that the order of computation is irrelevant, making such programs susceptible to lazy evaluation.~\parencite{wadler1995monads}
\end{comment}
%}}}

As an example, adding error handling to program logic can turn out to be tedious, since every recursive function call would need to be checked for errors and subsequently handled. In contrast, exceptions in an impure language would not introduce such structuring of code. More to the point, counting the number of operations performed on a function would need similar structuring of the code where the count is explicitly passed on for further operations. Again, in an impure language a global variable for the count would suffice.~\parenciteseveral{wadler1995monads}. Largely for this division of pure and impure functional languages,~\textit{monads} were introduced as an utility to combine the benefits of both approaches. Namely, monads are used ``to integrate impure effects into pure functional languages''~\parencite[25.]{wadler1995monads}. As is customary for the functional programming paradigm, also the concept of monads is derived from mathematics, and in this case, category theory.~\parenciteseveral{wadler1995monads}.
%{{{
\begin{comment}

Recent advances in theoretical computing science, notably in the areas of type theory and category theory, have suggested new approaches that may integrate the benefits of the pure and impure schools. These notes describe one, the use of monads to integrate impure effects into pure functional languages.~\parencite{wadler1995monads}

The concept of a monad, which arises from category theory, has been applied by Moggi to structure the denotational semantics of programming languages [13, 14]~\parencite{wadler1995monads}.
\end{comment}
%}}}

Monads can be seen as ``wrappers'' or contexts for values, similar to functors and applicative functors. For example, the standard Haskell data type {\tt Maybe} is an instance of the monad type class. 



Monads have three laws that they must obey.~\textit{Left identity}

~\textit{Right identity}

~\textit{Associativity}

~\textit{Monads} address the issue of ``purity'' in functional languages. For example, a user interacting with the application results in input and output, or in other words, communication between the application and external domains. However, as functions are pure, they can not As the meaning of purity was presented earlier in the chapter,  A

%{{{Additional sources
\begin{comment}
There are three laws that these definitions should satisfy in order to be a true monad in the sense defined by category theory. These laws guarantee that composition of functions with side effects is associative and has an identity (Wadler, 1992b).~\parencite{hudak2007history}

Although Wadler¿s development of Moggi¿s ideas was not directed towards the question of input/output, he and others at Glasgow soon realised that monads provided an ideal framework for I/O. The key idea is to treat a value of type as a ¿computation¿ that, when performed, might perform input and output before delivering a value of type.~\parencite{hudak2007history}

\end{comment}
%}}}

\section{Metrics for functional programming}

As previously presented in chapter~\ref{chap:software_metrics}, software metrics are an essential approach to quantifying attributes about the quality of software. Even though it is a profoundly different paradigm compared to object-oriented and imperative approaches, also functional programming can benefit from software metrics.

Prior research on metrics exclusively applicable to functional programming is scarce. Foundings of such research are primarily laid by~\citet{van1995software} and~\citet{ryder2004software}, whose metrics are primarily discussed in this section.The initial work of~\citet{van1995software} was spun by the need to effectively measure the readability of code programmed by students taking part in functional programming courses. Somewhat fortunately, many of the existing metrics, such as
lines of code and Halstead's program volume, are also applicable to functional programs with slight modifications~\parencite{van1995software}. Other such metrics are the measures of testing effort (pathcount, cyclomatic complexity), call graph measures (size, depth, arc-to-node ratio) and coupling measures~\parencite{ryder2004software}.
%{{{
\begin{comment}
    A problem encountered was that these metrics have been defined mainly for imperative languages: there-
    fore, a first task was the definition of the metrics for functional programs.~\parencite{van1995software}

    ¿ Program size.The various measures of program size, such as LOC and
    Halstead¿s program volume can be transfered to functional programs with
    little or no modification. These measures are used in Section 4.5.2 of this
    chapter.
    ¿ Testing effort. Metrics that measure the effort involved in testing a program,
    such as pathcount and cyclomatic complexity, can be applied to functional
    programming languages, although as is described later in Section 4.5.1 im-
    plementing such metrics requires care to correctly consider all execution
    paths. The pathcount measure is analysed for Haskell in Section 4.5.1 of
    this chapter.
    ¿ Call graph measures.Measurements taken from call graphs, such as size,
    depth, width and arc-to-node ratio, can all be applied to functional pro-
    grams without modification.These measures are examined for use with
    Haskell in Section 4.4 of this chapter.
    ¿ Coupling measures. Although it is possible to apply OO coupling measures,
    such as object level and class level coupling and coupling strength, from
    functional languages such as Haskell, some modifications are needed to map
    different language elements into the metric. For instance, OO coupling met-
    rics normally measure coupling between classes, but Haskell classes are not
    classes in the OO sense, and instead one is most likely to measure coupling
    between modules or functions.Coupling between functions is examined
    Briefly in Sections 4.4.2 and 4.4.3 of this chapter.~\parencite{ryder2004software}.
\end{comment}
%}}}

~\citet{ryder2004software} presents many code-specific metrics that are specific to the functional paradigm. For example, measures for the size and complexity of patterns are provided, as well as some intermodule metrics, such as the distance of declarations and their use, are created. In addition,~\citet{ryder2004software} integrates existing measures such as attributes of call graphs. 

The validation methodology used by~\citet{ryder2004software} is based on~\textit{subjective complexity}, which is referred as ``a notion of the complexity that might be perceived by a programmer attempting to understand or modify the given function''. The metrics are applied to source code version histories of two relatively small programs and statistical analysis is utilized for determining the correlation of the measurements.(TODO: Enemman tasta?)

\subsection{Complexity of patterns}

One of the most used features in functional programming is pattern matching. In Haskell, pattern matching is used in various ways. In addition to normal function arguments, data types are often used as arguments to pattern matching. The {\tt \_} wildcard can be used to match (or ignore) arguments, and patterns can also be used in function bodies. Constructing metrics to deduce the complexity of patterns is then an obvious first interesting step towards measuring FP programs.(TODO:
Lahde? Esimerkki PatMatchista?)

~\citet{ryder2004software} found a relatively weak correlation between the~\textit{number of pattern variables} and subjective complexity in two case-study programs. He argues that an increased number of variables in pattern matching also increases the effort required to comprehend that specific code. The case-study programs utilize pattern matching heavily, with 70-85\% of the functions containing patterns.

The measure~\textit{number of overridden or overriding pattern variables} deals with the fact that pattern and expression variables can be easily overridden in functional programming. For example, {\tt where} or {\tt let} bindings are often used for denoting intermediary variables(TODO: Esimerkki?). The hypothesis is that an increased amount of overridden variables in patterns increase program complexity.~\citet{ryder2004software} finds a statistical correlation between the hypothesis
and the metric, but it can be argued that such a programming style is detrimental in its own. Thus the measure should be used as a sign of the need to refactor any patterns containing redundant naming~\parencite{ryder2004software}.
%{{{
\begin{comment}
    Patterns that introduce variables may override existing identifiers, and identifiers
    in patterns may be overridden by those in a where or let clause. Overriding
    identifiers in this manner can be confusing, for instance in Example 10 it is not
    immediately clear which a is being used at any point in the function.~\parencite{ryder2004software}.

    The correlation results for the Peg Solitaire program, shown in Table 9 in
    Appendix B, do not exhibit any statistically significant correlation with the num-
    ber of bug fixing changes, probably because of the low number of instances of
    overriding occurring in the program. However, the results from the Refactoring
    program show a statistically significant correlation of 0.3731. Thus the following
    observation can be made.
    Observation 4.1.2 Increasing the number of overridden pattern variables in-
    creases the subjective complexity of the function.
    One might claim that overriding pattern variables is an inherently undesirable
    programming paradigm and as such this metric may be a useful tool to highlight
    such occurrences, so that they may be corrected by the programmer.Such a
    tool might combine this metric with a visualisation technique such as the pixel
    representation to quickly highlight such code (See for instance Sections 6.1 and
    6.5 of Chapter 6), or form part of an automated refactoring tool such as HaRe
    [60].~\parencite{ryder2004software}.
\end{comment}
%}}}

The~\textit{number of constructors} in a pattern can be an indicator of program complexity. Two hypotheses are provided by~\citet{ryder2004software}, where the first deals with the notion that an increased number of constructors also increases complexity. The second hypothesis argues, counteractively, that if the constructors are named properly, it can reduce subjective complexity of a function. Once again,~\citet{ryder2004software} found a slight statistical correlation
between an elevated usage of constructors in patterns and subjective complexity.
%{{{
\begin{comment}
    Patterns are commonly used in Haskell programs for manipulating algebraic data
    types by matching against constructor names. A possible metric is to count how
    many constructors are used in a pattern. There are two interesting and opposing
    hypotheses about the effect of this attribute on the subjective complexity of a
    function.
    One hypothesis is that using more constructors in a pattern requires more
    objects to be understood in order to comprehend the pattern, therefore increasing
    the complexity. The alternative hypothesis is that if constructor names are chosen
    well they are descriptive, and therefore add an element of documentation to the
    pattern, possibly reducing the subjective complexity of the pattern.~\parencite{ryder2004software}.
\end{comment}
%}}}

Also often used entities in patterns are wildcards, which signal unneeded pattern arguments. However, given even slightly numerous function or constructor arguments, wildcards can be easy to misplace.
\begin{lstlisting}[caption={Example of wildcards in patterns},label={lst:wildcard-pattern}]
data Shape = Triangle Int Int Int -- Lengths of sides
                      Int Int Int -- RGB values

redValue :: Shape -> Int
redValue (Triangle _ _ _ r _ _) = r
\end{lstlisting}

Using the two case-study programs,~\citet{ryder2004software} found correlation between complexity and increased number of wildcards in pattern matching. However, he further argues that the complexity is not necessary due to wildcards themselves, but because of the more complex data structures involved.
%{{{
\begin{comment}
    Table 9 in Appendix B shows that once again the results for the Peg Soli-
    taire program show no statistically significant correlation, while the Refactoring
    program shows a correlation value of 0.3572. This seems to suggest that increas-
    ing the number of wildcards can increase the complexity, and so the following
    observation can be made.
    Observation 4.1.3 Large numbers of wildcards may indicate areas of increased
    subjective complexity.
    This seems to contradict the generally perceived wisdom that wildcards reduce
    complexity.However, an explanation that might account for this is that the
    functions that use patterns with wildcards may be manipulating complex data
    structures, and so changes may be due to the complexity of the data structures
    rather than the appearance of wildcards.~\parencite{ryder2004software}.
\end{comment}
%}}}

The~\textit{depth of nesting} in patterns can be measured. For example, {\tt [(a,b),(c,d)]} shows a list containing tuples. The maximum depth of the pattern is 2, since the nested elements amount to two (list and tuple). The sum of the depths, however, may be more accurate in depicting the level of nesting taking place.
\begin{lstlisting}[caption={Example of depth of nesting~\parencite{ryder2004software}},label={lst:depth-of-nesting}]

type Tuple = (Int, Int)
type TupleList = [Tuple]

pattern1 :: TupleList -> Int
pattern1 [(a,b)] = a + b

pattern2 :: TupleList -> Int
pattern2 [(a,b), (c,d),(e,f)] = a + b + c + d + e + f
\end{lstlisting}

For both patterns in listing~\ref{lst:depth-of-nesting}, {\tt pattern1} and {\tt pattern2}, the maximum depth is 2. However, sums of the depths are not the same between the patterns. Finally,~\citet{ryder2004software} found a somewhat strong correlation for increased depth of nesting and complexity.
%{{{
\begin{comment}
    When measuring the depth of nesting one must consider how to measure the
    depth of nesting of patterns that contain more than one nested pattern.For
    instance, [(a,b),(c,d)] contains two nested patterns. One method is to take
    the maximum depth of all the nested patterns. Another method is to take the
    sum of the depths of all the nested patterns.
    Taking the maximum of the depths measures only how deeply nested the pat-
    tern is, while taking the sum of the depths effectively measures how much nesting
    is taking place, which may be more accurate. However it may be that taking the
    sum of the depths will actually be measuring the size of the pattern, in which case
    one would see a strong correlation between those two measures.~\parencite{ryder2004software}
\end{comment}
%}}}

The last pattern metric presented by~\citet{ryder2004software} is~\textit{pattern size}, which can be measured in two ways. The number of components in a pattern can be calculated or the depth of the resulting~\textit{abstract syntax tree}(TODO: reference?) can be measured.~\citet{ryder2004software} utilizes the amount of nodes in the syntax tree as a measure, and found a correlation for increased size in patterns and subjective complexity.
%{{{
\begin{comment}
The size of a pattern could be measured in a number of ways. For instance, one
could count the number of components in the abstract syntax tree, or one might
take the depth of abstract syntax tree as a measure of its size. For the purpose
of exploring the effect of pattern size on the number of changes, the number of
nodes in the abstract syntax tree of the pattern was chosen as the size measure.
The metric results in Table 9 in Appendix B show that the Peg Solitaire
program had no significant correlation, while the Refactoring program had a cor-
relation of 0.5423, which seems to suggest the following observation.~\parencite{ryder2004software}
\end{comment}
%}}}

The pattern metrics are essentially simple and mostly deal with the ``size'' of a pattern. These kind of metrics are usually bundled together to provide stronger observations of the measurements. For this reason,~\citet{ryder2004software} also analyzes the interaction of the pattern metrics by presenting a correlation matrix.~\citet{ryder2004software} concludes that the size-specific metrics, such as size, number of variables and depth of patterns are strongly correlating on the
subjective complexity of the code. However, the rest of the metrics are found to have almost no interaction between them.

As it seems, the size of a pattern is perhaps the only indicator of its complexity. However, further validation of the metrics is warranted and their application to larger projects should be conducted. 

\subsection{Distance between declarations and usage}

The distance between two interacting functions is an interesting property to measure. It is likely that if the distance is large, also the source code is harder for the developer to comprehend and the probability of errors in code increase. However, distance can be interpreted in multiple ways. The actual distance of source code lines can be referred as ``spatial'' distance measures, as where more semantic distance measures can be termed as ``conceptual'' distance measures.~\parenciteseveral{ryder2004software}. First, the~\textit{number of scopes} that are introduced for an executed function call can be quantified.

\begin{lstlisting}[numbers=left,caption={Example of number of scopes~\parencite{ryder2004software}},label={lst:number-of-scopes}]
foo :: Int -> Int
foo a = a * a

bar :: [Int] -> [Int] -> [Int]
bar a b = map fn c
    where 
        c = zip a b
        fn = \(x,y) -> foo (x+y)
\end{lstlisting}

\noindent Listing~\ref{lst:number-of-scopes} contains the scopes {\tt foo} and {\tt bar}. In addition, {\tt (a,b)}, {\tt (fn c)}, and {\tt (x,y)} are considered as scopes for the function {\tt foo} when it is called. Thus measuring the number of scopes signals ``how complex the name-space of the program is at a particular point in the code'', but also indicates how nested the usage of functions is~\parencite{ryder2004software}.(TODO: Tahan lisaselvitysta)

A second approach provided by~\citet{ryder2004software} is to count the~\textit{number of declarations} in the scope of a function call. This is performed by counting the declarations introduced between the function call and its declaration. The scope that contains the declaration is ignored. For example in figure~\ref{lst:number-of-scopes}, {\tt foo} is called at line 8. The scopes needed for this operation are then {\tt (x,y)}, {\tt (c,fn)}, {\tt (a, b)}. The {\tt foo} scope itself is ignored. The purpose of this measurement is to indicate the ``traffic'' of the nested scopes. Finally,~\citet{ryder2004software} concludes that~\textit{nesting} of the local definitions is more important than the number of those definitions for indicating complexity.

An example of spatial distance is to measure the actual source code lines from a function call to its declaration. In essence, the call to function {\tt foo} at line 8 in figure~\ref{lst:number-of-scopes} is seven source code lines away from its declaration, if empty lines are counted. The empirical results of~\citet{ryder2004software} leaves room for further research, since mixed results from the measure were found. 

Counting the distance based on source lines can be argued as being easily misinterpreted, since source lines can contain multiple statements. For this reason, the~\textit{number of parse tree nodes} can be a more accurate spatial distance measure. As program code is often internally represented in parse trees when compiling, it makes sense to count the amount of nodes leading from a declaration to its usage. This approach has the advantage that the actual logic is only dealt with and the
representation details of the source code are dismissed. However,~\citet{ryder2004software} found no distinguishable difference between this approach and counting the source code lines. Further research is thus warranted.

\subsection{Attributes of recursive functions}

Recursion is central to functional programming since often no constructs for explicit iteration of data structures, such as loops, are provided. This is the case in Haskell, where recursion is used extensively either by invoking the function itself or by using higher-order functions such as {\tt foldr}.~\citet{ryder2004software} concentrates on the former case and investigates how explicit recursion can add to subjective complexity.

First recursion measure provided by~\citet{ryder2004software} is the~\textit{binary indication of recursion}. The metric simply determines if a function is recursive in nature, indicated by a return value of zero or one. This seemingly simple metric can correlate with the amount of changes in the code base, and thus may indicate increased subjective complexity. Non-trivial recursion is when a function invokes a function, which in turn invokes the original function back. This kind of
recursion produces a cyclic call graph and is harder to detect in the source code. In addition, a function can contain both trivial recursion (the function invokes itself) and non-trivial recursion at the same time. Thus the binary indication of recursion simply points out whether a function is recursive without pointing out the implementation.

Often recursion is invoked based on multiple paths in a function. The metric~\textit{number of recursive paths} determines the amount of paths leading to recursion in a function, which is argued to correlate with subjective complexity. 

\begin{lstlisting}[caption={Example of multiple recursive paths~\parencite{ryder2004software}},label={lst:number-of-recursive-paths}]
toggleCase :: String -> String
toggleCase [] = []
toggleCase (x:xs)
        |  isUpper x = toLower x : toggleCase xs
        |  otherwise = toUpper x : toggleCase xs
\end{lstlisting}

In listing~\ref{lst:number-of-recursive-paths} the number of recursive paths is two. The resulting hypothesis is that the greater the number of recursive paths, the more complex the function is to comprehend. However,~\citet{ryder2004software} found weak correlation with the number of recursive paths and complexity in the case study applications. This is most likely due to a minimal amount of recursive paths in actual functions, since introducing such complexity would be against good coding practices.

Counting the~\textit{number of non-trivial and trivial recursion} can indicate complexity in the code base. As presented previously, trivial recursion occurs when a function invokes itself, as where non-trivial recursion deals with two functions that invoke each other. If the amount of non-trivial recursion exceeds trivial recursion, it can be hypothesised that the complexity of the code also increases. Again,~\citet{ryder2004software} found no support for the hypothesis,
most likely since the studied applications did not utilize non-trivial recursion to an extent.

One property of recursion is the actual~\textit{length of the recursive path}. Since the path leading to recursion can be somewhat long, also the comprehensibility of the function is likely to lower. The path can be measured in multiple ways. For instance, the number of calls or the number of functions in the path can be quantified. Additionally, if the function contains multiple recursive paths, the paths are summed and their product can be calculated. The results from the research
of~\citet{ryder2004software} do not show any statistically significant correlation for either the sum or product of recursive paths as a metric.

Lastly, all of the recursion-specific metrics are likely to interact with each other, and as such, observation of their interaction from a measurement perspective is worthwhile to conduct.

-Critique: Don't use explicit recursion, use fold!

\subsection{Attributes of call graphs}

As presented earlier in chapter~\ref{chap:software_metrics}, call graphs are a convenient way of rendering and analyzing the data flow of a program. As functions are the main tool of functional languages, it should be possible to utilize much of the same metrics of call graphs to the functional paradigm.

Call graphs can indicate~\textit{strongly connected components} in the program. When nodes of the call graph are coupled so that they call each other, they are strongly connected. This connection can also be indirect by having a more complex graph that produces cyclic paths. The hypothesis is that the size of the strongly connected components also correlate with increased maintenance effort. This is because every change in these kind of coupled functions have the potential to also force
changes in the strongly connected components.~\parenciteseveral{ryder2004software}. The empirical results of~\citet{ryder2004software} do not point out a strong correlation with the hypothesis, however. Further validation for the metric is warranted.

The~\textit{indegree of a function} indicates how many times a function is called. This can point out ``important'' functions and in the same time, problematic areas of the program code. However,~\citet{ryder2004software} found no correlation of the property with subjective complexity. In similar spirit, the~\textit{outdegree} of a function counts the number of outbound function calls from a given function. Since this indicates how coupled the function is to other functions, it can also
indicate how likely the function is to change because of other changes. As is,~\citet{ryder2004software} found correlation with the hypothesis in both case-study applications. The results are encouraging, but they warrant further empirical application of the metric.

A useful property of call graphs is that~\textit{subgraphs} of specific function calls can be generated. Common attributes of call graphs, such as depth, width and arc-to-node ratio, can then be measured to indicate the complexity of given functions. The arc-to-node ratio is calculated by dividing the amount of function calls (arcs) with functions (nodes) (TODO: ?). Depth and width is calculated simply by how wide or deep the call graph's tree is (see section~\ref{section:data-flow-attributes}).~\parenciteseveral{ryder2004software}.

\begin{figure}[h]\centering
    \begin{tikzpicture}[>=latex']
        \tikzset{block/.style= {draw, rectangle, align=center,minimum width=2cm,minimum height=1cm},
        rblock/.style={
                        draw, 
                        shape=rectangle,
                        rounded corners=1.5em,
                        align=center,
                        minimum width=2cm,
                        minimum height=1cm
                      },
        input/.style={ % requires library shapes.geometric
                        draw,
                        trapezium,
                        trapezium left angle=60,
                        trapezium right angle=120,
                        minimum width=2cm,
                        align=center,
                        minimum height=1cm
                     },
        }
        \node [rblock]  (start) {readFile};
        \node [rblock, below right =1.5cm of start] (acquire) {openFile};
        \node [rblock, below right =1.5cm and -5cm of start] (readContents) {readContents};
        \node [rblock, below =1.5cm of readContents] (readChar) {readChar};

    %% paths
        \path[draw,->] (start) edge (acquire)
                       (start) edge (readContents)
                       (readContents) edge (readChar)
                       ;
    \end{tikzpicture}
  \caption[Example of subgraph~\parencite{ryder2004software}
  ]{Example of subgraph~\parencite{ryder2004software}}
    \label{fig:subgraph-example}
\end{figure}

The subgraph accurately depicts the dependencies of a given function (see figure~\ref{fig:subgraph-example}).~\citet{ryder2004software} argues that if the subgraph ``grows'' too deep or too wide, the more problematic the function is to maintain and comprehend.~\citet{ryder2004software} found correlation for the hypothesis in his larger case-study application, where the function call-graphs are more complex.

The interaction of the call graph measures are limited to the width and depth metrics. These two metrics are likely to grow uniformly as the complexity of a function increases. Likewise, the outdegree metric was found to have high correlation values towards complexity. If a program utilizes non-trivial recursion to an extent, the strongly connected component size metric is indicative of the actual complexity~\parenciteseveral{ryder2004software}.

\subsection{Miscellaneous attributes of functions}

As many of the imperative metrics presented in chapter~\ref{chap:software_metrics} are also applicable to programs written in functional languages, it is interesting to explore the correlation of these metrics on complexity. For example, the~\textit{pathcount} and~\textit{operators and operands} are straightforward metrics often used in imperative languages, but may also provide additional insight into software measurement in functional programming.

The pathcount metric is calculated by summing all of the logical paths a program can take. In specific, every branching logic in the function add up to the metric. For example in Haskell, the {\tt if ... then ... else ...} and {\tt case} expressions are branching by definition. However, this branching can also be somewhat subtle. In listing~\ref{lst:pathcount-of-function} the function has three obvious branches in the form of pattern matches.~\parenciteseveral{ryder2004software}.

\begin{lstlisting}[caption={Example of function's pathcount~\parencite{ryder2004software}},label={lst:pathcount-of-function}]
func :: [Int] -> Int
func [] = 0
func (x:xs)
        | x > 0 = func xs
func (x:y:xs) = func xs
\end{lstlisting}

An additional branch is introduced by the clause {\tt x > 0}. If the conditional fails, the execution continues on to the last pattern match. Thus the pathcount of this function is four.~\citet{ryder2004software} argues that this kind of subtle increase in pathcount is easy to miss when studying the source code, and that they manifest often by e.g. leaving the {\tt otherwise} block from a guard expression in Haskell. On the other hand,~\citet{ryder2004software} found no statistically significant correlation between the metric and subjective complexity. 

The~\textit{size} of a function is a simple, yet heavily utilized measurement of complexity.~\citet{ryder2004software} modified the initial size metric specific to functional programming provided by~\citet{van1995software}. The size is calculated much in the same fashion as in the metrics of~\citet{halstead1977elements} (see subsection~\ref{subsection:halstead-metric}), where the program code is divided into~\textit{operands} and~\textit{operators}. Namely, operators include the standard operators and language keywords, such as {\tt :}, {\tt ++} and {\tt where} in Haskell. After applying the metric to his two case-study applications,~\citet{ryder2004software} found some correlation for increased size of functions and subjective complexity. This might especially apply to functional languages, where functions are intended to be kept as small as possible and numerous.

\subsection{Referential transparency as a measure}

~\citet{muddu2013metrics} provide metrics for the functional programming property of referential transparency. As discussed earlier in the chapter, a function is referentially transparent when the same input always returns the same output. The function thus does not have side-effects such as global variables as dependencies that could affect the computation. Referential transparency also makes the program easier to reason about and allows the compiler to provide heavy optimizations of such ``pure'' functions.~\parenciteseveral{hughes1989functional}.

~\textit{Referential Transparency Index} (RTI)~\parencite{muddu2013metrics} quantifies ``the proportion of all the defined methods which can be considered pure functions''. The metric is calculated by the ratio of pure (referentially transparent) functions that are in-use and the amount of overall used functions in the system. The~\textit{Intermodule Referential Transparency Index} (IRTI) calculates the proportion of referentially transparent function calls from a certain module to the rest of the system by the ratio of a module's pure function calls and the overall amount of functions in the system. Furthermore, by taking the average of IRTI, the metric can be calculated for the entire system.~\parenciteseveral{muddu2013metrics}.
%{{{
\begin{comment}
In the Intermodule Referential Transparency Index (IRTI) we measure the proportion of the calls in a certain module to methods in other modules which are pure over all the intermodule calls: ~\parencite{muddu2013metrics}
\end{comment}
%}}}

~\citet{muddu2013metrics} validate the provided metrics by assessing their impact on the modularity of a program. A well-modularized system contains separate modules that are loosely-coupled and thoroughly tested. The program is then easier to maintain, contains fewer bugs and is generally more extendable. The metrics were tested against three popular Scala open-source projects to indicate their level of modularity. The RTI metric was not concentrated on, partly because it
yielded lowest scores of the metrics. The inter-module metric, IRTI, however showed increases between versions, suggesting that updates to software brought about refactorings that included switching the use of impure functions to pure functions, and specifically those that are used between modules~\parencite{muddu2013metrics}. As it stands, function pureness is something of a desirable quality in functional programming and the RTI and IRTI metrics are useful if one wants to
quantify that property.

\subsection{Metrics for higher-order functions}

The use of higher-order functions is prevalent in functional programming. Higher-order functions are functions that receive functions as parameters and can output a function as their return value. The primary benefit of higher-order functions is in modularization, since e.g. data structures can be traversed by these functions without explicitly declaring how the data is utilized~\parencite{hughes1989functional}.
%{{{
\begin{comment}
In functional programming the use of first-order functions
(that is, functions passed as arguments or returned by other
functions) is a standard practice. It allows the reuse of parts
of code as building blocks by separating the way to treat a
data structure from the actual task to be performed on it. This
is clearly an advantage w.r.t modularization..~\parencite{muddu2013metrics}.
\end{comment}
%}}}

Slightly confusingly,~\citet{muddu2013metrics} define several metrics for ``first order'' functions, even though the more prevalent term seems to be higher-orderism~\parencite{hughes1989functional}. This thesis treats the terms higher-order and first-order as interchangeable.~\citet{muddu2013metrics} argue that if related first-order functions are separated in different modules, additional dependencies are created and thus the level of program modularization might suffer. This~\textit{Intermodule First Order Functions Index} (IFOI) can be resolved by taking the minimum of three calculations: 

\begin{itemize}
        \item Ratio of the amount of functions in a module that have at least one external function as their parameter and the amount of functions in the module with a function as a parameter.  
        \item Ratio of the amount of external functions that are called in a first-order way from the current module and the amount of functions outside the module
        \item ``The proportion of other modules whose methods are called upon in a first order way by current module's methods''~\parencite{muddu2013metrics}
\end{itemize}

The~\textit{Pure First Order Function Index} (PFOI) metric addresses the fact that a function being passed in a higher-order manner benefits of the property of pureness, since it increases program modularity. PFOI is defined at a system level by measuring ``the proportion of all the first order calls which are pure methods''~\parencite{muddu2013metrics}. Furthermore, the~\textit{Intermodule Pure First Order Function Index} (IPFOI) metric is similar, but calculates the ratio of first order calls to pure functions in external modules and the amount of first order function calls executed~\parencite{muddu2013metrics}.

The metrics proposed by~\citet{muddu2013metrics} have only been tested with few open-source projects, and thus should be further validated to assess their feasibility. Furthermore, the results of the metrics are somewhat ambiguous and most benefit would be drawn if the developers and architects of the modules themselves would address this issue~\parencite{muddu2013metrics}. 
%{{{
\begin{comment}
The proposed metrics are validated on open source software
systems and are yet to be tested on large scale enterprise
software, but the evidence so far obtained clearly validates the
assumptions on which the metrics were based. The modules
created during the experiments are not based on developer or
architect inputs, who would have a better understanding of
the system and the testing against such a system will further
strengthen the validity of the metrics.~\parencite{muddu2013metrics}.
\end{comment}
%}}}

\subsection{Degree of polymorphism}


\subsection{Summary}

Critique:
~\begin{itemize}
    \item Ryder's work uses only two small case-study programs
~\end{itemize}

\section{Summary}

%{{{ Additional sources
\begin{comment}
Some in the functional programming community have claimed that the use of functional programming results in an order-of-magnitude increase in productivity, largely due to functional programs being claimed to be only 10 percent as large as their imperative counterparts. While such numbers have been actually shown for certain problem areas, for other problem areas, func- tional programs are more like 25 percent as large as imperative solutions to the same problems (Wadler,
1998).~\parencite{sebesta2002concepts}

In fact, because of the necessity of dealing with variables, imperative programs have many trivially simple lines for initializing and making small changes to variables.~\parencite{sebesta2002concepts}

However, there are now compilers for most functional languages, so that execution speed disparities between functional languages and compiled imperative languages are no longer so great. One might be tempted to say that because functional programs are significantly smaller than equivalent imperative programs, they should execute much faster than the imperative programs. However, this often is not the case, because of a collection of language characteristics of the functional
lan- guages, such as lazy evaluation, that have a negative impact on execution efficiency.~\parencite{sebesta2002concepts}.

Another source of the difference in execution efficiency between functional and imperative programs is the fact that imperative languages were designed to run efficiently on von Neumann architecture computers, while the design of functional languages is based on mathematical functions. This gives the imperative languages a large advantage.~\parencite{sebesta2002concepts}.

Functional languages have a potential advantage in readability. In many imperative programs, the details of dealing with variables obscure the logic of the program.~\parencite{sebesta2002concepts}.

In an imperative language, the pro- grammer must make a static division of the program into its concurrent parts, which are then written as tasks, whose execution often must be synchronized. This can be a complicated process. Programs in functional languages are natu- rally divided into functions. In a pure functional language, these functions are independent in the sense that they do not create side effects and their operations do not depend on any nonlocal or global variables.
Therefore, it is much easier to determine which of them can be concurrently executed.~\parencite{sebesta2002concepts}.

\end{comment}
%}}}

%}}}
%{{{Empirical study
%{{{Additional Sources
\begin{comment}
\chapter{Empirical study}

\section{Validation methodology}

%{{{Discussion
\chapter{Discussion}

%}}}


Preliminary guidelines for empirical research in software engineering~\parencite{kitchenham2002preliminary}
    Before developing and using metrics, one must have a clear idea about the 
    purpose of the metric. A Goal-Question-Metric method (Basili & Rombach, 
    1988) gives a framework for this issue. The goal and some questions have been 
    expressed above. In the research in this thesis, one major issue has been the 
    objective assessment of the comprehensibility of programs. Understanding ex-
    isting code is one of the more time-consuming tasks in the maintenance of 
    software products. A recent survey of models for code understanding is given 
    by von Mayrhauser (1994). The models are classified as either top-down mod-
    els or bottom-up models. Top-down models emphasise the nature and structure 
    of domain knowledge and how it is represented in and mapped onto code and 
    documentation information. Bottom-up models build understanding from de-
    tail code using control flow and data flow.~\parencite{van1995software}

    The above-mentioned early developments in software
    metrics naturally led several researchers to question their
    theoretical validity. Theoretical validation implies confor-
    mance to a set of agreed-upon principles and these
    principles are usually stated in the form of a theoretical
    framework. In 1988, Weyuker [6] proposed a set of
    properties to be satisfied by software complexity metrics.
    Many subsequent contributions discussed these properties
    from the standpoint of sufficiency/necessity and whether
    or not they could be supported by more formal under-
    pinnings. See for example [7], [8], and [9], and the citations
    contained therein. Other notable software metrics valida-
    tion frameworks include those by Kitchenham et al. [10]
    (see also Fenton and Pfleeger [11]), who have borrowed the
    needed principles from the classical measurement theory.
    However, this framework was found wanting by Morsaca
    et al. [12] with regard to how the scale types used for
    attribute measurements were constrained.~\parencite{sarkar2007api}.
\end{comment}
%}}}

%}}}
%{{{Summary and conclusion
\chapter{Summary and conclusion}

%}}}

\printbibliography

\end{document}
