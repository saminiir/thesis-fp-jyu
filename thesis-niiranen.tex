%{{{ Settings
% !TeX encoding = latin1
%
% [ Tiedostossa käytetty merkistö on ISO 8859-1 eli Latin 1. Ylläoleva rivi ]
% [ tarvitaan, jos käyttää MiKTeX-paketin mukana tulevaa TeXworks-editoria. ]
%
% TIETOTEKNIIKAN KANDIDAATINTUTKIELMA
%
% Yksinkertainen LaTeX2e-mallipohja kandidaatintutkielmalle.
% Käyttää Antti-Juhani Kaijanahon ja Matthieu Weberin kirjoittamaa
% gradu2-dokumenttiluokkaa.
%
% Laatinut Timo Männikkö
%
% Jos kirjoitat pro gradu -tutkielmaa, tee mallipohjaan seuraavat muutokset:
%  - Poista dokumenttiluokasta optio shortthesis .
%  - Poista makro \tyyppi .
%  - Lisää suuntautumisvaihtoehto makrolla \linja .
%  - Kirjoita ylimmän tason otsikot makrolla \chapter, toisen tason otsikot
%    makrolla \section ja mahdolliset kolmannen tason otsikot makrolla
%    \subsection .
%
% Halutessasi voit tehdä myös kandidaatintutkielman "pro gradu -tyylillä":
%  - Poista shortthesis-optio.
%  - Kirjoita otsikot makroilla \chapter , \section (ja \subsection ).

\documentclass[english]{tjt-latex-gradupohja/gradu3}

\usepackage{graphicx} % tarvitaan vain, jos halutaan mukaan kuvia

\usepackage{booktabs} % hyvä kauniiden taulukoiden tekemiseen

\usepackage{tikz} % Portable graphics, diagrams
\usetikzlibrary{matrix,positioning,fit,arrows,shapes.geometric}

\usepackage{amsmath}

\usepackage{listings}
\lstset{language=Haskell}

\usepackage{babel}

\usepackage{comment}

\usepackage[bookmarksopen,bookmarksnumbered,linktocpage]{hyperref}

\addbibresource{sources-niiranen.bib}

\tolerance=2000 \emergencystretch=10pt % Better hyphenation without overflowing lines

\begin{document}
%}}}
%{{{Meta
\title{Software quality metrics and functional programming}
\translatedtitle{Ohjelmistomittarit ja funktionaalinen ohjelmointi}

\author{Sami Niiranen}
\contactinformation{sami.i.niiranen@student.jyu.fi}

\supervisor{Ville Tirronen,}
\supervisor{Antti-Juhani Kaijanaho,}
\supervisor{Jari Veijalainen}
\supervisor{Jukka Lindström (Reaktor Innovations)}

\setdate{02}{03}{2014}

\abstract{TODO}
\tiivistelma{TODO}

\avainsanat{tietojärjestelmätiede, pro gradu -tutkielma, TODO} 
\keywords{information technology, Master's thesis, TODO} % avainsanoilla

%\studyline{Järjestelmäkehitys}

\maketitle

\mainmatter
%}}}
%{{{Introduction
\chapter{Introduction}
Software quality metrics is a branch of software engineering, where the quality of code is analyzed and quantified. The purpose of this observation is to e.g. identify problematic modules in the code and predict future maintenance effort~\parencite{fenton1999software}. The definition for software quality metrics is given in IEEE standard 1061~\parencite{ieeesoftwaremetrics1998} as: 
~\begin{quoting}
A function whose inputs are software data and whose output is a single numerical value that can be interpreted as the degree to which software possesses a given attribute that affects its quality.
~\end{quoting}

\noindent~\citet{fenton1998software} regards these as ``structural and complexity metrics'' when dealing with the internal attributes of software.
%{{{
\begin{comment}
Structural and complexity metrics~\parencite{fenton1998software} p 18.
\end{comment}
%}}}
\noindent Examples of such attributes are lines of code (LOC) and cyclomatic complexity~\parencite{mccabe1976complexity}.


\section{Previous work}

Code analysis and comparison of quality in functional programming (FP) paradigms is relatively scarce~\parencite{hudak1994haskell,harrison1996comparing,harrison1995estimating,harrison1996evaluation}.

Explicit formulation of metrics for functional programming has been presented in the works of~\citet{ryder2004software,van1995software}. Furthermore,~\citet{ryder2004software} leaves the measurement of type systems, polymorphism and higher-order functions for future research, which this thesis aims to address.

\section{Limitations}

Commercial use of programming paradigms are still in favour of the object-oriented (Java) and imperative (C) approaches. Despite FP being largely utilized in academia, the research on it is somewhat diverse and not uniform. In addition, real-world applications of FP are not abundant and finding a suitable program for research might be challenging. 

Software metrics, regardless of their quantitative nature, are up for interpretation and thus the results of the metrics are subjectively assessed.

Algorithmic efficiency is left out.

\section{Research problem}

The aim of this thesis is to provide an up-to-date overview of software metrics and discuss future research. Moreover, metrics are specifically pruned in the context of functional programming languages, where Haskell is used as reference.  As an example,~\citet{ryder2004software} encourages future research for measuring polymorphism and higher-order functions in functional programming.
%{{{
\begin{comment}
Functional languages often provide powerful abstraction mechanisms such as polymorphic and higher-order functions, or abstract data types. These languages features suggest that useful metrics could be defined to measure attributes of abstraction in programs.
For example, one might measure how polymorphic a function is by counting the number of different type variables present in the function¿s type signature, or by counting how many different types the function is used as. Likewise one might measure how abstract a given algebraic data type is by counting the number of constructors which are exported for that data type. Similar metrics could also be created to measure attributes of higher-order functions and other abstraction mechanisms.~\parencite{ryder2004software}
\end{comment}
%}}}

The research can be decomposed into four questions:

\begin{itemize}
    \item What are software quality metrics?
    \item What is functional programming?
    \item How do existing software quality metrics apply to the functional programming paradigm? 
    \item What kind of new metrics can be formulated and how do they apply to FP programs?
\end{itemize}

\noindent The research problem can be expressed as follows: \textit{What kind of software metrics exists for the functional programming paradigm, and how do the metrics apply to applications with functional implementation?}

\section{Research methods}
The thesis utilizes literature review as its main research method. The review consists of an overview into the metrics of software quality and functional programming. In preparation of the empirical part, FP metrics~\parencite{ryder2004software} are scrutinized and the possibility for new metrics is investigated.

New metrics formulated will be presented. For example, polymorphism and other type system -specific metrics are currently nonexistant~\parencite{ryder2004software} and could serve as a basis for new metrics research.

In the second part, the research themes are investigated through empirical research. One or more case study applications are chosen and their source code is used for applying the metrics. As an example, the abstract syntax trees (AST) with type information could be built from the case study source codes and the metrics quantified with the help of the AST.

\section{Expected results and their significance}
The results aim at explaining the various semantics of FP and how they relate to software quality metrics. The discovery of new metrics for FP-specific metrics is likely, since research on the subject has not been extensive~\parencite{ryder2004software}.

The research is expected to be of significance to researchers and practitioners alike. The results will help in formulating software metrics and FP into practical settings. Future research will be fueled by providing more support to the functional paradigm as opposed to imperative approaches, and encourage real-world usage of the functional paradigm.
%{{{Additional questions
\begin{comment}
There are two general types of criticisms that can be applied to current software metrics. The first category are those theoretical criticisms that are leveled at conventional software metrics as they are applied to traditional, non-00 software design and development. Kearney, et al. criticized software complexity metrics as being without solid theoretical bases and lacking appropriate properties [21]. Vessey and Weber also commented on the general lack of theoretical rigor in the
structured programming literature [41]. Both Prather and Weyuker proposed that traditional software complexity metrics do not possess appropriate mathematical properties, and consequently fail to display what might be termed normal predictable behavior [34], [47].~/parencite{chidamber1994metrics}.
\end{comment}
%}}}

%}}}
%{{{Software metrics
\chapter{Software metrics}
\label{chap:software_metrics}

Software is known to be brittle in the sense that its runtime behavior might not correspond to what the programmer had intended. More specifically, human error often introduces faulty logic into the software's code. Once these unintended ``features'', or bugs, slip into production versions and the software is released, fixing a critical error afterwards might require approximately hundred times more effort~\parencite{shull2002we}. To help assessing and predicting the quality of the
software at the time of development, software metrics are engineered.
%{{{
\begin{comment}
General data were presented that supported an effort increase of approximately 100:1. Don O¿Neill described data from IBM Rochester [10] in the pre-meeting feedback that found an increase in effort of about 13:1 for defect slippage from code to test and a further 9:1 increase for slippage from test to field (so, a ratio of about 117:1
from code to field).~\parencite{shull2002we}
\end{comment}
%}}}

~\textit{Software metrics} is a rather generic term traditionally meant to include the assessment of software by its internal and external attributes~\parencite{fenton1999software}. The characteristics of software are quantified through metrics in order to predict (1) effort/cost of development processes and (2) the quality of software~\parencite{fenton2000software}. The central idea of measuring software is that with access to the characteristics
(quality, complexity) of the software, developers and management can make educated guesses about development and resource planning~\parencite{fenton1999software}.
%{{{
\begin{comment}
¿Software metrics¿ is the rather misleading collective term used to describe the wide range of activities concerned with measurement in software engineering. These activities range from producing numbers that characterise properties of software code (these are the classic software ¿metrics¿) through to models that help predict software resource requirements and software quality. The subject also includes the quantitative aspects of quality control and assurance - and this covers
activities like recording and monitoring defects during development and testing.~\parencite{fenton1999software}.

Although there are literally thousands of metrics that have been proposed since the mid 1960's (all of which fit into the framework of Table 1) the rationale for almost all individual metrics has been motivated by one of two activities:
1. The desire to assess or predict effort/cost of development processes;
2. The desire to assess or predict quality of software
key in both cases has been the assumption that product 'size' measures should drive any predictive models.~\parencite{fenton2000software}
\end{comment}
%}}}

First attempts at recording software metrics were witnessed in the 1960s when .

The definition for software quality metrics is given in IEEE standard 1061~\parencite{ieeesoftwaremetrics1998} as: 
~\begin{quoting}
A function whose inputs are software data and whose output is a single numerical value that can be interpreted as the degree to which software possesses a given attribute that affects its quality.
~\end{quoting}

\noindent Examples of such attributes are lines of code (LOC) and cyclomatic complexity~\parencite{mccabe1976complexity}.

Although the theoretical effectiveness of software metrics and defect prediction TÄHÄN JUTTUA kritiikistä

This thesis adopts the general categorization of software measures as proposed by~\citet{fenton1998software}. In specific, measures are divided into size, structure and information flow measures.~\citet{briand1996property} argue that many of the metrics presented in research are misplaced in the sense that they do not obey mathematical axioms of a given category. As such, this thesis recognizes the interpretative nature of the metrics and does not attempt to prove the validity of each presented measure in mathematical terms.
%{{{
\begin{comment}
Unfortunately, new software measures are very often defined to capture elusive concepts such as complexity, cohesion, coupling, connectivity, etc. (Only size can be thought to be reasonably well understood.) Thus, it is impossible to assess the theoretical soundness of newly proposed measures, and the acceptance of a new measure is mostly a matter of belief.~\parencite{briand1996property}

Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems.~\parencite{briand1996property}
\end{comment}
%}}}

%{{{ TODO: sources
\begin{comment}
In the software engineering literature, the notion of measure validity is used in many ways with different mean- ings [16]. Fenton and Pfleeger [17] call a measure internally valid if it is a proper numerical characterisation of the attribute. The internal validity of a measure must be distinguished from its external validity. The latter perspective on measure validity refers to the usefulness of a measure. According to Briand et al. [18] a software measure is useful if it is related to a measure of some external attribute, e.g.~\parencite{poels2000distance}

A measure is (internally) valid if it is a homomorphism from the empirical relational system into a formal relational system, i.e. if it maps entities into values such that all empirical relations among the entities are preserved as formal relations among the measurement values. The funda- mental problem of software measurement is that for many software attributes, including software complexity, it is not known what the empirical relational system looks like [20].~\parencite{poels2000distance}.

As a consequence, these measure property sets are useful to in- validate software measures by showing that they do not measure the attribute such as it is empirically understood. However, the set of measures that would be considered valid for the attribute of interest is only a subset of the set of measures that satisfy all the necessary axioms.~\parencite{poels2000distance}

A useful way to understand empirical relations on a set of object-elements is to consider the measurement of complexity.  A designer generally has some intuitive ideas about the com- plexity of different object-elements, as to which element is more complex than another or which ones are equally complex.  For example, a designer intuitively understands that a class that has many methods is generally more complex, ceteris paribus, than one that has few methods. This intuitive idea is
defined as a viewpoint. The notion of a viewpoint was originally introduced to describe evaluation measures for information retrieval systems and is applied here to capture designer views [9 1. More recently, Fenton states that viewpoints characterize intuitive understanding and that viewpoints must be the logical starting point for the definition of metrics [14]. An empirical relation is identical to a viewpoint, and the two terms are distinguished here only for the sake of consistency with
the measurement theory literature.~\parencite{chidamber1994metrics}.
\end{comment}
%}}}

\section{Size metrics}

Perhaps the first approach to measuring the internal attributes of software is the ~\textit{size} metric. It is relatively easy to be quantified, since only source code is required for the analysis and the measurement is often made statically, meaning that the program code is not executed.~\citet{fenton1998software} divide the size metric into the following subcategories: 

\begin{itemize}
    \item~\textit{Length} (actual size of the source code)
    \item~\textit{Functionality} (the features the user receives)
    \item~\textit{Complexity} (how difficult the program code is to reason with) 
    \item~\textit{Reuse} (how much of the code is original and what is reused)
\end{itemize}

\noindent Essentially the simplest size metrics are relatively easy to be formed, but also provide only a one-dimensional view into the quality of the software. Thus some of the metrics are most often utilized as an aid for interpreting internal attributes, but not too much information should be expected of their results.~\parenciteseveral{fenton1998software}.
%{{{
\begin{comment}
Rejecting simple size measures on these grounds reflects a misunderstanding of the basic measurement principles.~\parencite{fenton1998software}
\end{comment}
%}}}

%{{{TODO: Sources
\begin{comment}
Several measures introduced in the literature can be classified as size measures, according to our properties Size.1 - Size.3. With reference to code measures, we have: LOC, #Statements, #Modules, #Procedures, Halstead's Length [H77], #Occurrences of Operators, #Occurrences of Operands, #Unique Operators, #Unique Operands. In each of the above cases, the representation of a program as a system is quite straightforward. Each counted entity is an element, and the relationship between elements is just the sequential relationship.~\parencite{briand1996property}
\end{comment}
%}}}

\subsection{Lines of code}

Counting the lines of code (LOC) is perhaps the most commonly used metric, but on the same time it poses challenges regarding the interpretation of the results~\parencite{fenton1998software}. Programmers (and projects) have distinct styles which can affect the results. For example, a function can be separated into multiple lines in order to make it more readable, where in other cases the same results are achieved by a shorter declaration. In turn, LOC varies even though the intent
of the function is the same.
%{{{
\begin{comment}
Some lines of code are different from others~\parencite{fenton1998software}.
\end{comment}
%}}}

Furthermore, not all source code lines are equal. Often comments and blank lines are used to increase the readability of the program. These types of lines might not require the same effort of development as the code itself and as a result they can distort the size metric. For these reasons, LOC is traditionally meant to contain only the ``effective'' source code lines without comments and blank lines~\parencite{grady1987software}. To iterate the LOC metric further,~\citet{fenton1998software} suggest calculating the ratio of the different types of source code lines: 
\begin{align}
\text{Total length (LOC) = Non-Comment LOC + Comment LOC}
\end{align}
\noindent Then the following serves as the ratio of commented lines in the source code:
\begin{align}
\dfrac{CLOC}{LOC}
\end{align}

\noindent Using a ratio different programs can be more easily compared, since the ratio of different types of LOC are evident. Executable statements (ES) is one form of quantifying lines of source codes. In ES, comment lines, data declarations and headers are ignored and instead the operation statements are counted. Multiple executable statements in one line are thus considered separate.
%{{{
\begin{comment}
(ES) This measure counts separate statements on the same physical line as distinct. it ignores comment lines, data delcarations and headings.~\parencite{fenton1998software}.
\end{comment}
%}}}

The implications are that the LOC-metric is highly situation-dependant. Different software projects can be compared based on their line count, but also modules in the same source code are often inspected. Puzzlingly, this seemingly simple metric can be non-trivial to interpret, since its use-cases are manifold. One source code line might be short, while another one is long and complex. As is, measuring the lines of code should be tailored for specific use.~\parenciteseveral{fenton1998software}.
%{{{
\begin{comment}
Clearly, the definition of code length is influenced by the way in which it is to be used. Some organizations use length to compare one proeject with another, to answer questions such as: What is our largest/smallest/average project? What is our productivity? What are the trends in project length over time?~\parencite{fenton1998software}.
\end{comment}
%}}}

TODO: Critique on LOC: Some misconceptions about lines of code~\parencite{rosenberg1997some}

\subsection{Halstead's program volume}

One of the earliest static analysis methods for source code is the Halstead-metric~\parencite{halstead1977elements}. With an effort to unify empirical research on software,~\citet{halstead1977elements} divides program code into tokens of~\textit{operators} and~\textit{operands} that can be used to assess several characteristics of the software regardless of, for example, the implementation language. Operators can be considered as ``actions'' in the source code (e.g. in imperative languages, the assignment operator =). Operands represent data (e.g. variables)~\parencite{ryder2004software}. Moreover, the variables of the Halstead-metric are:
%{{{
\begin{comment}
Halstead classifies symbols in a program into two classes, those that are used to specify actions are classed as operators while symbols that represent data are classed as operands.~\parencite{ryder2004software}.
\end{comment}
%}}}

\begin{flalign*}
\mu_1 = \text{number of unique operators}\\
\mu_2 = \text{number of unique operands}\\
\\
N_1 = \text{total occurrences of operators}\\
N_2 = \text{total occurrences of operands}\\
\end{flalign*}

\noindent These variables are then used to quantify several properties such as the~\textit{length},~\textit{vocabulary},~\textit{volume},~\textit{program level} and~\textit{difficulty} of the source code. (TODO: Calculations and examples of the formulas)

\section{Structural metrics}

Another interesting property of software is their structure, which indicates additional information of the development and maintenance effort of the source code. One approach to inspecting structural metrics is the perspective of~\textit{control-flow}, or the logical paths in the execution of the program. On the other hand,~\textit{Data-flow} deals with information flow between modules in the software.~\parenciteseveral{fenton1998software}. In this section, the more evident metrics of structural metrics are presented and their challenges are analyzed.
%{{{
\begin{comment}
The structure of the product plays a part, not only in requiring development effort but also in how the product is maintained.~\parencite{fenton1998software}.

It is the inter-module dependencies that interest us, and measures of these attributes are called inter-modular measures.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Flowgraphs}

As the research on software metrics traditionally centers on imperative languages,~\textit{flowgraphs}~\parencite{allen1970control} are an intuitive way of mapping the ``flow'' of program statements. Flowgraphs are directed graphs that indicate program statements as nodes, and flow of control as arcs (or edges) in the graph. Together these elements visualize all of the paths the program's execution can take~\parencite{fenton1998software}.
%{{{
\begin{comment}
A great deal of software metrics work has been devoted to measuring the control-flow structure of imperative language programs or algorithms. The control flow measures are usually modeled with directed graphs, where each node (or point) corresponds to a program statement, and each arc (or directed edge) indicates the flow of control from one statement to another.
\end{comment}
%}}}

For example, the following pseudo-code {\tt if A then X} can be expressed as a flowgraph (TODO: finish diagram):

\begin{figure}[h]\centering
    \begin{tikzpicture}[>=latex']
        \tikzset{block/.style= {draw, rectangle, align=center,minimum width=2cm,minimum height=1cm},
        rblock/.style={draw, shape=rectangle,rounded corners=1.5em,align=center,minimum width=2cm,minimum height=1cm},
        input/.style={ % requires library shapes.geometric
        draw,
        trapezium,
        trapezium left angle=60,
        trapezium right angle=120,
        minimum width=2cm,
        align=center,
        minimum height=1cm
    },
        }
        \node [rblock]  (start) {A};
        \node [rblock, below right =2cm of start] (acquire) {X};
        \node [rblock, below right =2cm and -5.5cm of start] (gchannel) {Y};

    %% paths
        \path[draw,->] (start) edge (acquire)
                    (start) edge (gchannel)
                    ;
    \end{tikzpicture}
  \caption[Example of flowgraph~\parencite{allen1970control}
  ]{Example of flowgraph~\parencite{allen1970control}}
    \label{fig:flowgraph-example}
\end{figure}
%{{{
\begin{comment}
\begin{figure}[h]\centering
  \begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=1cm,
    node/.style={%
      ellipse,
      minimum width=5em,
      minimum height=3em,
      draw
    }
  ]
    \node[node] (1)                                             {1};
    \node[node] (2) [below=of 1]                                {2};
    \node[node] (4) [node distance=1cm and 3mm,below left=of 2] {4};
    \node[node] (3) [left=of 4]                                 {3};
    \node[node] (5) [below=of 4]                                {5};
    \node[node] (6) [node distance=2cm,right=of 5]              {6};

    \path (1) edge                   node {} (2)
          (2) edge                   node {} (3)
          (2) edge                   node {} (4)
          (2) edge                   node {} (6)
          (3) edge                   node {} (5)
          (4) edge                   node {} (5)
          (5) edge [bend right=20pt] node {} (2);
  \end{tikzpicture}
  \caption[Example of flowgraph~\parencite{allen1970control}
  ]{Example of flowgraph~\parencite{allen1970control}}
    \label{fig:flowgraph-example}
\end{figure}
\end{comment}
%}}}

\noindent As can be seen from figure~\ref{fig:flowgraph-example}, directed graphs consist of a set of nodes that are connected with arcs. Furthermore, the arrowhead indicates the direction of the execution, flowing from one node to another.~\parenciteseveral{allen1970control}. Logical paths arriving at a node is called the~\textit{in-degree} of the node, while a node's~\textit{outdegree} is the number of arcs ``leaving'' the node~\parencite{fenton1998software}.
%{{{
\begin{comment}
Thus, directed graphs are depicted with a set of nodes, and each arc connects a pair of nodes. We write an arc as an ordered pair, <x,y>, where x and y are the nodes forming the endpoints of the arc, and the arrow indicates that the arc direction is from x to y. The arrowhead indicates that something flows from one node to another node.~\parencite{fenton1998software}.

The in-degree of a node is the number of arcs arriving at the node, and the outdegree is the number of arcs that leave the node. We can move from one node to another along the arcs, as long as we move in the direction of the arrows. A path is a sequence of consecutive (directed) edges, some of which may be traversed more than once during the sequence. A simple path is one in which there are no repeated edges.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Cyclomatic complexity}

~\textit{Cyclomatic complexity}~\parencite{mccabe1976complexity} utilizes flowgraphs to evaluate the complexity of a program's decision structure. A program is represented as a flowgraph $F$ with single entry and exit points of execution. The~\textit{cyclomatic number} is then calculated as:
\begin{align}
v(F) = e - n + 2
\end{align}

\noindent where the flowgraph $F$ has $e$ amount of arcs and $n$ amount of nodes. The cyclomatic number points out the number of independent execution paths through $F$ and is seen to indicate the complexity of the program's structure. More specifically, comprehending the source code in general increases in difficulty when the cyclomatic number is higher~\parencite{ryder2004software}.~\citet{fenton1998software} agree that the measurement is objective, but argue concurrently that the cyclomatic number is a poor indicator of ``general complexity'' of a program's structure. Mainly because this ``complexity'' of a flowgraph $F$ does not necessary hold in conjuction with general measurement theory and its ``intuitive relations about complexity''.~\citet{fenton1998software} do suggest, however, that cyclomatic number is a well-suited indicator for how difficult the program is to test and maintain.
%{{{
\begin{comment}
The number of independent paths is a good indicator of the complexity of a program because generally a program becomes harder to understand as the number of paths increases.~\parencite{ryder2004software}.

From a measurement theory perspective, it is extremely doubtful that any of these assumptions corresponds to intuitive relations about complexity. Thus, v cannot be used as a generla complexity measure.~\parencite{fenton1998software}
\end{comment}
%}}}

~\citet{mccabe1976complexity} suggests that a module with a complexity number of over 10 should be identified as problematic and refactoring should be considered. TODO: More text about CC
%{{{
\begin{comment}
Programmers have been required to calculate complexity as they create software modules. When the complexity exceeded 10 they had to either recognize and modularize subfunctions or redo the software. The intention was to keep the "size" of the modules manageable and allow for testing all the independent paths~\parencite{mccabe1976complexity}.

\end{comment}
%}}}

\subsection{Test coverage measures}

One approach to software measurement is how difficult the modules of the program are to test. In specific, if the structure of a program is misconstructed, also the testability should be low and difficult to be carried out. Moreover, motivation for software measurement and testing is that complex structures should often be concentrated on over ``simpler'' functionality. 
%{{{
\begin{comment}
In particular, the structure of a module is related to the difficulty we find in testing it.
\end{comment}
%}}}

TODO:~\textit{Mininum number of test cases}

~\textit{Test effectiveness ratio}, or test coverage, is a common metric to indicate to which extent the actual execution logic is covered by the testing strategy~\parencite{fenton1998software}. Test effectiveness ratio can be calculated as
%{{{
\begin{comment}
For a given program and a set of cases to test it, we would like to know the extent to which the test cases satisfy a particular testing strategy.~\parencite{fenton1998software}.
\end{comment}
%}}}

\begin{align}
TER_T = \dfrac{\text{number of T objects exercised at least once}}{\text{total number of T objects}}
\end{align}

\noindent Where $T$ objects are paths in the execution logic, such as loops and branches. Few important observations of $TER$ are that it often requires run-time execution, as which paths a test case covers is difficult to deduce statically~\parencite{ryder2004software}. Second, the resulting ratio can be deceiving if the $T$ objects are constrained loosely. Namely, a $TER$ of 100\% is easy to achieve by only ``simple paths'' as $T$ object values, but this hardly indicates whether some particular functionality is actually tested~\parencite{fenton1998software}.
%{{{
\begin{comment}
This may be difficult to achieve statically, so it is often necessary to perform some form of runtime execu- tion tracing to be able to perform this measurement.~\parencite{ryder2004software}.

However, the actual TER is typically no better than 40%. The typical TER for branch coverage is lower still, providing objective evidence that most software is not tested as rigorously as we like to think.~\parencite{fenton1998software}.
\end{comment}
%}}}

\section{Data-flow attributes}

The metrics so far have dealt with~\textit{intra-modular measures}, but another interesting application of software measurement is how different program modules relate to each other in terms of structure and modularity~\parencite{fenton1998software}.~\citet{fenton1998software} propose that a module is any program construct that can be ``at least theoretically separately compilable''. 

Data-flow is often depicted with~\textit{callgraphs}, which are directed graphs, but do not contain the start and end nodes found in flowgraphs~\parencite{fenton1998software}. Furthermore, a callgraph does not contain individual attributes of modules, but rather just describes which modules a specific module ``calls''.
%{{{
\begin{comment}
We use a more abstract model of the design, a directed graph known as the module call-graph. A call-graph is not a flowgraph, as it has no circled start or stop node.~\parencite{fenton1998software}

For example, instead of examining variables, we may need to know only whether or not one module calls (or depends on) another module.~\parencite{fenton1998software}
\end{comment}
%}}}

\begin{align}
\text{TODO: Example of callgraph}
\end{align}

\noindent Several properties for assessing software design quality from callgraphs have been formed~\parencite{yourdon1979structured}. The most obvious one is~\textit{size}, where the number of nodes and edges are counted from the callgraph. A large callgraph should then indicate increased complexity in the structure of the program~\parencite{ryder2004software}.~\textit{Depth}, on the other hand, is the longest path from the root node to a leaf node.~\textit{Width} measures the maximum number of nodes at any one level.~\textit{Arc-to-node ratio} indicates the amount of ``connections'' between nodes. In other words, the more function calls are in- or outbound to a node, the ratio increases simultaneously~\parencite{ryder2004software}. More specific attributes of callgraphs can be derived, and they are, in fact, often applicable to most graph-type models~\parencite{fenton1998software}.
%{{{
\begin{comment}
Yourdon and Constantine have analyzed what contributes to good design, and they suggest several ways to view design components and structure. They use the notion of morphology to refer to the ``shape'' of the overall system structure when expressed pictorially.~\parencite{fenton1998software}.

The greater the size of the callgraph, the more dependencies between different parts of the program there is likely to be, and so the greater the complexity.~\parencite{ryder2004software}.

This is a connectivity density measure that increases as more connections, or function calls, are made and is similar to cyclomatic complexity. A high arc-to-node ratio indicates complex interactions between functions, and thus may highlight areas of program code that may benefit from re-engineering. Section 2.7 examines the use of metrics in the re- engineering process.~\parencite{ryder2004software}.

These measures are not restricted to dependency graphs. They are generally applicable to most graph-type models, and we shall apply them in Chapter 12 to evaluate project management structures.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Graph impurity}

One measure for system complexity is~\textit{graph impurity}~\parencite{ince1988approach}, which can be derived from the callgraph of a program. The measure contains a simple hypothesis which holds that the more the program's callgraph deviates from being a tree-structure, the more complex the structure is~\parencite{ince1988approach}. 
%{{{
\begin{comment}
One measure of system complexity that has been sug- gested is graph impurity: the more a system deviates from being a pure tree structure towards being a graph structure, the worse the design is~\parencite{ince1988approach}.

\end{comment}
%}}}

\begin{figure}[h]\centering
  \begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=0.5cm,
    node/.style={%
      rectangle,
      minimum width=3em,
      minimum height=1.5em,
      draw
    }
  ]
    \node[node] (1)                                             {A};
    \node[node] (2) [below left=of 1]                           {B};
    \node[node] (3) [below right=of 1]                          {C};
    \node[node] (4) [below left=of 2]                           {D};
    \node[node] (5) [below right=of 2]                          {E};
    \node[node] (6) [below left=of 3]              {F};
    \node[node] (7) [below right=of 3]              {G};

    \path (1) edge                   node {} (2)
          (1) edge                   node {} (3)
          (2) edge                   node {} (4)
          (2) edge                   node {} (5)
          (3) edge                   node {} (6)
          (3) edge                   node {} (7);
  \end{tikzpicture}
  \caption[Example of graph impurity~\parencite{ince1988approach}
  ]{TODO: Example of graph impurity~\parencite{ince1988approach}}
    \label{fig:graph-impurity-example}
\end{figure}

\noindent~\citet{ince1988approach} argue, that graph impurity should not be achieved at the cost of generally accepted design patterns. For example, a module that is referenced often would be considered as contributing to graph impurity, but the structure of the program can actually be well-designed because of module reuse. As is, graph impurity should probably not be interpreted fully objectively, but rather ``where choice exists a design should be produced which minimises graph impurity''~\parencite{ince1988approach}.
%{{{
\begin{comment}
This does not mean that a designer should aim at producing a pure tree-structured design. The very nature of the subroutine, an object called from many places in a program which saves space, means that the designer will make use of common calls. What it does mean is that where choice exists a design should be produced which minimises graph impurity.~\parencite{ince1988approach}.
\end{comment}
%}}}

Furthermore,~\citet{fenton1998software} provide a formula for calculating the impurity:
\begin{align}
m(G) = \dfrac{2(e - n + 1)}{(n - 1)(n - 2)}
\end{align}

\noindent where $e$ is the edge (or arc) count and $n$ is the node count.~\citet{fenton1998software} conclude that ``system designs should strive for a value of $m$ near zero, but not at the expense of unnecessary duplication of modules''. This is in conjuction with the earlier presented observation that while graph impurity can indicate problematic software design, it might contradict with beneficial design choices.
%{{{
\begin{comment}
Results suggest a relationship between tree impurity and poor design. Thus m may be useful for quality assurance of designs. System designs should strive for a value of m near zero, but not at the expense of unnecessary duplication of modules.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Internal reuse}

Internal reuse is the measure to which extent a program reuses its own modules~\parencite{fenton1998software}.~\citet{yin1978establishment} have defined a measure for internal reuse (~\textit{system design measure}~\parencite{yin1978establishment}) which can be calculated from the callgraph of a program:
%{{{
\begin{comment}
We call internal reuse the extent to which modules are reused within the same product.~\parencite{fenton1998software}.
\end{comment}
%}}}

\begin{align}
r(G) = e - n + 1
\end{align}

\noindent where graph $G$ has $e$ edges and $n$ nodes.~\citet{fenton1998software} criticize the measure by stating that (1) it does not consider different calls from the same module and (2) it does not consider the size of the reused components. As such, the measure is perhaps best used as a general measure of a program's internal reuse and used in conjuction with other measures such as tree impurity.
%{{{
\begin{comment}
This measure is crude as a reuse measure; not only does it take no account of possible different calls from the same module, it also takes no account of the size of the reused components.~\parencite{fenton1998software}.
\end{comment}
%}}}

TODO: How is the measure interpreted?

\subsection{Coupling}

~\textit{Coupling} addresses the degree of interdependence between modules~\parencite{allen2001measuring}. Namely, coupling is most often inspected between two modules ($x$, $y$).~\textit{Global coupling} can be derived from the combination of coupling between modules~\parencite{fenton1998software}.
%{{{
\begin{comment}
Coupling of a subsystem characterizes its interde- pendence with other subsystems.~\parencite{allen2001measuring}

Usually, coupling is an attribute of pairs of modules, rather than of the design as a whole. The entire set of modules in the design exhibits global coupling, which can be derived from the coupling among the possible pairs.
\end{comment}
%}}}

~\citet{fenton1998software} define several relations for inspecting the coupling between two modules:
\begin{itemize}
    \item~\textit{No coupling relation} $R_0$ when $x$ and $y$ have no interdependency.
    \item~\textit{Data coupling relation} $R_1$ when $x$ and $y$ communicate by data parameters but no control element is incorporated. An example would be a mathematical function which performs the same computation regardless of the parameter values~\parencite{ryder2004software}.
    \item~\textit{Stamp coupling relation} $R_2$ when $x$ and $y$ accept the same types as parameters. The modules can be otherwise unrelated but interdependency can emerge from shared data types.
    \item~\textit{Control coupling relation} $R_3$ when $x$ passes parameters to $y$ which changes its behavior. For example, a rendering function $y$ might receive coordinate points from module $x$.
    \item~\textit{Common coupling relation} $R_4$ when $x$ and $y$ refer to the same global data. The modules are then coupled, because of the common interdependency on the same data. 
    \item~\textit{Content coupling relation} $R_5$ when $x$ refers and changes the contents (e.g. values) of $y$.
\end{itemize}
%{{{
\begin{comment}
n example might be passing parameters to a mathematical function, the func- tion performs the same actions regardless of the parameters passed. This is in contrast to control coupling in which different sets of actions may be performed depending on the parameters that are passed.~\parencite{ryder2004software}.
\end{comment}
%}}}

\noindent In essence, the higher the $R_i$ value for two given modules, the more~\textit{tightly coupled} they are. In contrast, a $R_i$ value of 1-2 between two module would be characterized as~\textit{loosely-coupled}. However, the coupling classification can be challenging to determine, at least programmatically. A manual inspection of the program's structure is thus often warranted~\parencite{ryder2004software}.
%{{{
\begin{comment}
We say that x and y are loosely coupled if i is 1 or 2, and they are tightly coupled if i is 4 or 5.~\parencite{fenton1998software}.

It is also worth noting that in practice it may be difficult to exactly classify coupling by Fenton¿s classifications. For instance, determining between R3 and R1 coupling may be difficult or impossible to do mechanically and may therefore require manual inspection in order to decide the exact classification.~\parencite{ryder2004software}.
\end{comment}
%}}}

\begin{figure}[h]\centering
  \begin{tikzpicture}[%
    ->,
    shorten >=2pt,
    >=stealth,
    node distance=0.5cm,
    node/.style={%
      rectangle,
      minimum width=3em,
      minimum height=1.5em,
      draw
    }
  ]
    \node[node] (1)                                             {A};
    \node[node] (2) [below left=of 1]                           {B};
    \node[node] (3) [below right=of 1]                          {C};
    \node[node] (4) [below left=of 2]                           {D};
    \node[node] (5) [below right=of 2]                          {E};
    \node[node] (6) [below left=of 3]              {F};
    \node[node] (7) [below right=of 3]              {G};

    \path (1) edge                   node {} (2)
          (1) edge                   node {} (3)
          (2) edge                   node {} (4)
          (2) edge                   node {} (5)
          (3) edge                   node {} (6)
          (3) edge                   node {} (7);
  \end{tikzpicture}
  \caption[Example of coupling model graph~\parencite{fenton1998software}
  ]{TODO: Example of coupling model graph~\parencite{fenton1998software}}
    \label{fig:coupling-model-graph-example}
\end{figure}

\noindent For visualizing the coupling of a system, a~\textit{coupling model graph} can be utilized~\parencite{fenton1998software}.  \noindent In the coupling model graph, nodes correspond to modules and the directed edges (arcs) are dependencies between the modules. A value $(i,j)$ defines the level of coupling ($i = R_i$) and the count of how many times a given coupling occurs ($j$) between two modules.~\parenciteseveral{fenton1998software}. Additionally, ~\citet{ryder2004software} argues that ``the visualisation of the module coupling may be of as much, or even more, use than any measures on coupling because it can allow developers to gain a high level overview of the interactions in the software system very quickly''.
%{{{
\begin{comment}
To model module coupling, we use a directed graph with more detail than a callgraph but less than the full module structure chart. The nodes correspond to the modules, and there may be more than one arc between two nodes. Each arc from a node x to node y represents coupling between modules x and y.~\parencite{fenton1998software}
\end{comment}
%}}}


\subsection{Cohesion}

The metric~\textit{cohesion} determines the relative proximity of similar program features~\parencite{briand1996property}. Another interpretation is that cohesion determines the level of dependencies inside a module, in contrast to inter-module inspection of coupling~\parencite{allen2001measuring}. The higher the cohesion of a module is, the better its features are encapsulated and the responsibilities defined. As a result, the reliability and maintainability of the system increases~\parencite{fenton1998software}. 
%{{{
\begin{comment}
The concept of cohesion has been used with reference to modules or modular systems. It assesses the tightness with which "related" program features are "grouped together" in systems or modules.~\parencite{briand1996property}.

 subsystem¿s cohe- sion, on the other hand, characterizes its internal in- terdependencies.~\parencite{allen2001measuring}.
\end{comment}
%}}}

TODO: More detailed presentation of cohesion

Curiously, the cohesion metric has no formal measure by which it can be quantified. Instead,~\citet{fenton1998software} state that the level of a module's cohesion can be deduced by simply asking questions about the module's purpose. For example, if the purpose of a module cannot be described in a single sentence, it might have too many responsibilities, and consequently its cohesion is low. Paradoxically, a good designer probably has asked the question already and developed a module with high cohesion.~\parenciteseveral{fenton1998software}.
%{{{
\begin{comment}
There is no obvious measurement procedure for determining the level of cohesion in a given module. However, we can get a rough idea by writing down a sentence to describe the module's purpose. A good designer should have already done so, but a module with low cohesion is unlikely to have been produced by a good designer! If it is impossible to describe its purpose in a single sentence, then the module is likely to have coincidental cohesion.~\parencite{fenton1998software}.
\end{comment}
%}}}

\subsection{Information flow}

As the flow of data between modules is an important part of measuring the attributes of software, also the overall level of information flow in a system is of interest.~\citet{henry1981software} describe three different types of~\textit{information flow}. (1)~\textit{Local indirect flow} occurs when a module passes information to another module or conversely, returns a result to a module invoking it. (2)~\textit{Local indirect flow} exists if ``the invoked module returns information
that is subsequently passed to a second invoked module''. Third,~\textit{global flow} exists if data is passed between modules through a global data structure.~\parenciteseveral{fenton1998software}.
\begin{comment}
Generally a modular program will consist of modules with data flowing through them in a manner that is termed information flow. Measuring properties of in- formation flow may be of interest to programmers because it can help to indicate where particularly complex interaction is occurring between functions, and thus where there may be complex algorithmic behaviour that is likely to make modifi- cations to the program difficult to perform.~\parencite{ryder2004software}.

We say a local direct flow exists if either 1. a module invokes a second module and passes information to it; or 2. the invoked module returns a result to the caller. Similarly, we say that a local indirect flow exists if the invoked module returns information that is subsequently passed to a second invoked module. A global flow exists if information flows from one module to another via a global data structure.~\parencite{fenton1998software}.
\end{comment}
%}}}

The information flow is relatively easy to be determined, as it can be done by simply analyzing the procedures and calls of the system's module~\parencite{henry1981software}. The~\textit{Henry-Kafura}~\parencite{henry1981software} measure formulates additional variables that help measuring the complexity of a module: The~\textit{fan-in} is the number of~\textit{local flows} into the module plus the number of data structures the module utilizes for information retrieval. On the other
hand,~\textit{fan-out} is the amount of local flows out from a module plus the number of data structures the module updates. ~\parenciteseveral{henry1981software}.
%{{{
\begin{comment}
An important property of the information flows defined above is that the knowledge necessary toconstructthecom- pleteflowsstructurecanbeobtainedfromasimpleprocedure- by-procedureanalysis.~\parencite{henry1981software}.

Thetermsfan-in,fan-out,complexity,andmodulearespecif- icallydefinedforinformationflowinordertopresentthe measurementsinthissection.Fan-inandfan-outaredescribed withrespecttoindividualprocedures.  Definition5:Thefan-inofprocedureAisthenumberof localflowsintoprocedureAplusthenumberofdatastruc- turesfromwhichprocedureAretrievesinformation.  Definition6:Thefan-outofprocedureAisthenumberof localflowsfromprocedureAplusthenumberofdatastruc-
tureswhichprocedureAupdates.~\parencite{henry1981software}.
\end{comment}
%}}}}

The~\textit{Henry-Kafura} measure is formulated as follows:
\begin{align}
    \text{Complexity of module $=$ length $*$ (fan-in $*$ fan-out)$^2$}
\end{align}

\noindent where~\textit{fan-in $*$ fan-out} indicates the number of possible combinations of an input source to an output destination. Furthermore, the number is squared because the complexity is believed to be more than linear. Length is used for a simple measure of the complexity of the module's code. The result defines the complexity of a module based on its external dependencies and internal code quality~\parenciteseveral{henry1981software}
%{{{
\begin{comment}
Thetermfan-in*fan-outrepresentsthetotalpossiblenumber ofcombinationsofaninputsourcetoanoutputdestination.~\parencite{henry1981software}

ofcombinationsofaninputsourcetoanoutputdestination.  Theweightingofthefan-inandfan-outcomponentisbasedon thebeliefthatthecomplexityismorethanlinearintermsof theconnectionswhichaprocedurehastoitsenvironment.  ThepoweroftwousedinthisweightingisthesameasBrook's ~\parencite{henry1981software}.
\end{comment}
%}}}}

%{{{
\section{Metrics for object-oriented languages}

The popularity of object-oriented programming (OOP) has produced numerous new or adjusted software measures specific to the paradigm~\parencite{chidamber1994metrics,lorenz1994object,bansiya2002hierarchical}. In specific, the main components of OOP are real-world simulated classes, from which~\textit{objects} are instantiated. This is distinct to the imperative and functional approaches, where functions and data are the main drivers of design~\parencite{chidamber1994metrics}. 
%{{{
\begin{comment}
The 00 approach centers around modeling the real world in terms of its objects, which is in contrast to older, more traditional approaches that emphasize a function-oriented view that separates data and procedures.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

An object possesses~\textit{methods} and~\textit{attributes} that characterize its functionality. For example, a {\tt Dog} class might have a method {\tt wagTail()}. Now, a {\tt Dog} object can be instantiated and its methods used:
\begin{lstlisting}
    var dog = new Dog();
    dog.wagTail();
\end{lstlisting}

The purpose of the OOP paradigm is to model real-world entities as closely as possible~\parencite{booch1986object}. This thesis chooses to present OOP metrics specific to the work of~\citet{chidamber1994metrics}, where several measures for OOP are defined based on previous research. Namely, a close inspection of an object's properties and its relation to other objects is mapped as the foundation for OOP software metrics. Their measures have been validated as good predictors defects in real-world software by numerous empirical studies~\parencite{basili1996validation,subramanyam2003empirical,gyimothy2005empirical,zhou2006empirical,olague2007empirical} and is the most widely used OOP metric suite~\parencite{catal2009systematic}.
%{{{
\begin{comment}
We conclude that the CK and QMOOD suites contain similar components and produce statistical models that are effective in detecting error-prone classes.~\parencite{olague2007empirical}

discussed. Several of Chidamber&Kemerer¿s OO metrics appear to be useful to predict class
fault-proneness during the early phases of the life-cycle. We also showed that they are, on our
data set, better predictors than ¿traditional¿ code metrics, which can only be collected at a later
phase of the software development processes.~\parencite{basili1996validation}

However, CK metrics suite is much more popular than other suites and they are mostly used if class-level metrics are applied.~\parencite{catal2009systematic}.
\end{comment}
%}}}

\subsection{Weighted methods per class}

~\citet{chidamber1994metrics} argue that the the number of methods and their complexity in a class is a predictor of the effort required to develop and maintain it. Furthermore, the more methods in a class, the greater the effect on subclasses. Thus the first metric proposed metric by~\citet{chidamber1994metrics} is the~\textit{weighted methods per class} (WMC) metric. WMC is defined as

\begin{align}
WMC = \sum_{i=1}^{n} c_i
\end{align}

\noindent where $n$ is the amount of methods in a given class and $c_i$ is the ``complexity'' of the method. The complexity metric is deliberately left undefined, as it can be interpreted in multiple ways and thus interchangeable in the metric(TODO: ???).

\subsection{Depth of inheritance}

As inheritance is often utilized in object-oriented design~\parencite{booch1986object}, also its quantity and quality should offer some information for software measurement.~\textit{Depth of inheritance} (DIT) simply inspects the ``position'' of a class in its inheritance hierarchy. For example, the deeper the class is in regards to inheritance, the more likely it is to obtain unneeded methods from parent classes and thus increase compleexity. On the other hand, inheritance's main
advantage is the reusability of methods, since general functionality can simply be acquired higher from the inheritance hierarchy. As is, the DIT metric is easy to be calculated, but does not directly point problems in the design. The inheritance tree can be ``top-heavy'', where the tree can be depicted as flat and wide, or ``bottom-heavy'', where the tree is thin but long. These properties do not, however, necessarily point out design flaws directly, as every program's design choices are unique.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    Metric 2: Depth of Inheritance Tree (DIT) 
    Definition: Depth of inheritance of the class is the DIT metric 
    for the class. In cases involving multiple inheritance, the DIT will 
    be the maximum length from the node to the root of the tree. 
    Theoretical Basis: DIT relates to Bunge¿s notion of the 
    scope of properties. DIT is a measure of how many ancestor 
    classes can potentially affect this class. 
    Viewpoints: 
    1) The deeper a class is in the hierarchy, the greater the 
    number of methods it is likely to inherit, making it more 
    complex to predict its behavior.16 
    ¿61nterestingly, this has been independently observed by other researchers 
    2) Deeper trees constitute greater design complexity, since 
    3) The deeper a particular class is in the hierarchy, the 
    more methods and classes are involved. 
    greater the potential reuse of inherited methods. 
    )~\parencite{chidamber1994metrics}
\end{comment}
%}}}

\subsection{Number of children}

The~\textit{number of children} metric is likewise easy to be measured. It is simply the amount of immediate subclasses (children) of a class. The number of children metric is based on the notions that the greater the amount of subclasses, also greater is the reusage of the parent class's methods. With increased subclass count, however, also the likelyhood of misuse of inheritance by improper abstractions is increased. Lastly, the number of subclasses predict the impact of the parent
class on the design. For example, a high subclass count may mandate more testing of that class.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
nated to a class in the class hierarchy. 
Theoretical Basis: NOC relates to the notion of scope of 
properties. It is a measure of how many subclasses are going 
to inherit the methods of the parent class. 
Viewpoints: 
1) Greater the number of children, greater the reuse, since 
inheritance is a form of reuse. 
2) Greater the number of children, the greater the likelihood 
of improper abstraction of the parent class. If a class has 
a large number of children, it may be a case of misuse 
of subclassing. 
3) The number of children gives an idea of the potential 
influence a class has on the design. If a class has a large 
number of children, it may require more testing of the 
methods in that class. ~\parencite{chidamber1994metrics}

\end{comment}
%}}}}

\subsection{Coupling between object classes}

The metric~\textit{coupling between object classes} (CBO) is measured by the amount of objects an object is coupled to.~\citet{chidamber1994metrics} define coupling as ``an object is coupled to another object if one of them acts on the other, i.e., methods of one use methods or instance variables of another''.  
%{{{
\begin{comment}
    Definition: CBO for a class is a count of the number of other 
    classes to which it is coupled. 
    Theoretical Basis: CBO relates to the notion that an object is 
    coupled to another object if one of them acts on the other, i.e., 
    methods of one use methods or instance variables of another. 
    As stated earlier, since objects of the same class have the same 
    properties, two classes are coupled when methods declared in 
    one class use methods or instance variables defined by the 
    other class.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

CBO is argued to predict the quality of the program's design. Tight coupling between objects hinders modularity and reuse of the components and subsequently increases the CBO count. In other words, coupling and the CBO count is favourable to be kept at minimum. Additionally, the CBO measure can be a indicator of the effort needed to test the program's code. A high CBO count is probable to warrant more testing.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    Viewpoints: 
    1) Excessive coupling between object classes is detrimental 
    to modular design and prevents reuse. The more inde- 
    pendent a class is, the easier it is to reuse it in another 
    application. 
    2) In order to improve modularity and promote encapsu- 
    lation, inter-object class couples should be kept to a 
    minimum. The larger the number of couples, the higher 
    the sensitivity to changes in other parts of the design, 
    and therefore maintenance is more difficult. 
    3) A measure of coupling is useful to determine how 
    complex the testing of various parts of a design are 
    likely to be. The higher the inter-object class coupling, 
    the more rigorous the testing needs to be.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

\subsection{Response for class}

The~\textit{Response for class} (RFC) metric deals with the amount of methods that are called when the object receives a message. If a high amount of method calls can potentially be executed when the object receives a message, also the testing and debugging of the object's class becomes more difficult. Additionally, the larger the RFC number is, the complexity of that class increases. With the help of the RFC metric, developers can draw opinions on the effort needed to e.g. test the
class.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    The response set of a class is a set of methods that can 
    potentially be executed in response to a message received 
    by an object of that class.26 The cardinality of this set is 
    a measure of the attributes of objects in the class. Since it 
    specifically includes methods called from outside the class, it 
    is also a measure of the potential communication between the 
    class and other classes. 
    Viewpoints: 
    1) If a large number of methods can be invoked in response 
    to a message, the testing and debugging of the class 
    becomes more complicated since it requires a greater 
    level of understanding required on the part of the tester. 
    2) The larger the number of methods that can be invoked 
    from a class, the greater the complexity of the class. 
    3) A worst case value for possible responses will assist in 
    appropriate allocation of testing time. 
    ~\parencite{chidamber1994metrics}.
\end{comment}
%}}}

\subsection{Lack of cohesion in methods}

The~\textit{lack of cohesion in methods} (LCOM) metric measures the level of cohesion in an object class. Namely, it calculates the amount of method pairs in a class that have no relation to each other. For example, if two methods do not share the use of an instance attribute of the object, their LCOM measure is zero. In essence, the LCOM metric ``provides a measure of the relative disparate nature of methods in the class''~\parencite{chidamber1994metrics}.
%{{{
\begin{comment}
    The LCOM is a count of the number of method pairs whose 
    similarity is 0 (i.e., U ( )  is a null set) minus the count of method 
    pairs whose similarity is not zero. The larger the number 
    of similar methods, the more cohesive the class, which is 
    consistent with traditional notions of cohesion that measure 
    the inter-relatedness between portions of a program. If none 
    of the methods of a class display any instance behavior, i.e., 
    do not use any instance variables, they have no similarity and 
    the LCOM value for the class will be zero. The LCOM value 
    provides a measure of the relative disparate nature of methods 
    in the class. A smaller number of disjoint pairs (elements of set 
    P) implies greater similarity of methods. LCOM is intimately 
    tied to the instance variables and methods of a class, and 
    therefore is a measure of the attributes of an object class.~\parencite{chidamber1994metrics}
\end{comment}
%}}}

Cohesiveness is roughly translated to the level of uniformity of a class. The greater the cohesiveness of a class's methods is, the greater the encapsulation is and subsequently the quality of design is heightened. A lack of cohesion increases complexity, which in turn increases the effort required to maintain and test the class. Low cohesion implies that the class has too many responsibilities and would benefit from division to smaller
classes.~\parenciteseveral{chidamber1994metrics}.
%{{{
\begin{comment}
    therefore is a measure of the attributes of an object class. 
    Viewpoints: 
    1) Cohesiveness of methods within a class is desirable, 
    since it promotes encapsulation. 
    2) Lack of cohesion implies classes should probably be 
    split into two or more subclasses. 
    3) Any measure of disparateness of methods helps identify 
    flaws in the design of classes. 
    4) Low cohesion increases complexity, thereby increasing 
    the likelihood of errors during the development process. 
    ~\parencite{chidamber1994metrics}.
\end{comment}
%}}}

\section{Change metrics}

One of the currently more active approaches to measuring and predicting the quality of software is the inclusion of version change history as a measurement dimension~\parencite{zimmermann2005mining,hassan2009predicting}. These so-called ``change metrics'' presume that problematic components can be identified by the focus they receive during development.~\citet{zimmermann2005mining} apply~\textit{evolutionary coupling} as an indicator of components that would benefit from maintenance. Evolutionary coupling is simply deduced by the amount of changes to any two modules in the same revision of the version history. For example, when two source code files change, their evolutionary coupling number is increased. After inspecting the whole revision history, the evolutionary coupling count between modules is argued to be a good indicator for maintenance effort~\parencite{zimmermann2005mining}. The metric can also be tuned more precise, by e.g. inspecting the evolutionary coupling of methods and functions.
%{{{
\begin{comment}
    Both files have been changed together 20 times, indicat-
    ing some evolutionary coupling. This is not a very strong
    coupling, though, since ComparePreferencePage.java
    has been changed 40 times overall¿that is, it has been
    changed 20 times without plugin.properties being
    changed at the same time.
    To obtain more details, we can increase the granularity
    from files to entities and determine the evolutionary
    coupling between the individual attributes and functions
    defined in ComparePreferencePage.java. This reveals
    new couplings¿for instance, a coupling between the
    fKeys[] attribute and the initDefaults() method as
    well as a coupling between the fKeys[] attribute and the
    plugin.properties file. Both couplings are strong: In
    10 out of 11 times that fKeys[] has been changed,
    plugin.properties has been changed, too.~\parencite{zimmermann2005mining}.
\end{comment}
%}}}

\section{Challenges of metrics}

Predicting fault incidence using software change history~\parencite{graves2000predicting}

None of the metric suites presented in research literature are without their criticism. For example,~\citet{hitz1996chidamber} show that some of the metrics of~\citet{chidamber1994metrics} can be easily misused by not establishing a solid measurement environment. In essence, measuring software's internal attributes is highly context-specific. Concentrating on irrelevant attributes is bound to cause skewed measurement results and thus active engagement in resolving the truly important aspects of the software is required~\parencite{hitz1996chidamber}. This seems to be the core problem of software metrics: Determining what to measure, what data is relevant and how to interpret the results. The problem is further escalated by the fact that many metrics serve only as ``proxies'' to relevant measures, making the recognition of what to measure even harder. 
%{{{
\begin{comment}
    Before any measurement activity, we must identify the attribute to 
    be measured. Such an attribute must bear a certain significance for 
    a person involved in the development process, such as a designer, 
    programmer, manager, user, etc. The attribute might not necessar- 
    ily be interesting per se, but might serve as an independent vari- 
    able for indirect measurement of another (interesting!) attribute or 
    in a given prediction model (a case we do not consider further 
    here), however, one should avoid collecting data about meaning- 
    less aspects of the software document under investigation (just 
    because they happen to be easily collectible). 
    In the next step, a "sufficient" empirical relation system must ~\parencite{hitz1996chidamber}
\end{comment}
%}}}

Even after decades, no uniform guidelines for measuring software exist~\parencite{kitchenham2010s}. Since metrics are scrutinized closely by the research community, also their theoretical foundation can be often invalidated. For example,~\citet{kitchenham2010s} notes that the metric suite of~\citet{chidamber1994metrics} is long been theoretically invalidated, and as a consequence empirical results of those metrics are also invalid. This results in a large body of studies and results that are potentially invalid in scientific sense. On the other hand, many researchers acknowledge the situational nature of metrics and seem to concentrate more on real-world scenarios~\parencite{kitchenham2010s}. As is, the best of both worlds (solid ontologies for creating metrics and guidelines for empirical studies) is still yet to emerge.
%{{{
\begin{comment}
    It seems that empirical validation papers are of major impor-
    tance to the software metrics research community. However, this
    study suggests that the limitations of empirical studies are not al-
    ways understood, in particular, the impact of the context in which
    data arises, the impact of constructed metrics (such as defect den-
    sity, change density), the implication of linear relationships, and
    the notion of measures needing to be identically, independently,
    distributed. Furthermore, we appear to have no idea when to stop
    empirical validation, even in the case of well-investigated metrics
    such as the Chidamber¿Kemerer metrics.
    I believe the software engineering research community must
    take up the challenge to aggregate its results into an empirically-
    based body of knowledge. The evidence from this mapping study~\parencite{kitchenham2010s}

    Currently many studies use multiple data sets. However, most
    researchers still construct post-hoc explanations for different out-
    comes rather than refining their hypotheses to predict differences.
    Overall, we need to reflect more on what we mean by ¿¿empirical
    validation¿ and identify appropriate methods for performing such
    studies. The complexity of the interaction between context and
    outcomes suggest support for Fenton and Neil¿s proposals to model~\parencite{kitchenham2010s}

\end{comment}
%}}}

\section{Summary}

A somewhat promising trend in the field of software measurement is the use of defect predicting analysis tools~\parencite{nagappan2005use,zimmermann2005mining,menzies2007data}. These techniques utilize machine learning and data mining~\parencite{witten2005data} to effectively train the tools used to recognize patterns in the source code that cause defects. In more depth, this approach concerns itself more on the algorithms which by the machine learning is conducted, rather than on the specific software metrics used. Even simple static code attributes, such as lines of code and cyclomatic complexity, have been found to be indicative of software quality~\parencite{menzies2007data}. 
%{{{
\begin{comment}
Our conclusion is that, contrary to prior pessimism [21],
[22], data mining static code attributes to learn defect
predictors is useful. Given our new results on naive Bayes
and log-filtering, these predictors are much better than
previously demonstrated. Also, prior contradictory results
on the merits of defect predictors can be explained in terms
of the brittleness of the space of ¿best¿ predictors. Further,
our baseline experiment clearly shows that it is a misdir-
ected discussion to debate, e.g., ¿lines of code versus
McCabe¿ for predicting defects. As we shall see, the choice of
learning method is far more important than which subset of the
available data is used for learning.~\parencite{menzies2007data}.

\end{comment}
%}}}


%{{{Additional sources
\begin{comment}
    This type of overriding can be particularly troublesome if the conflicting defi- nitions have the same type, which prevents the compiler from indicating possibly erroneous behaviour when modifying the program. For instance, consider the (contrived) area function in Example 10. If the a in the where clause is renamed, the function will compile with no errors, but will give incorrect results unless the a after the = is also changed.
    It therefore appears that measuring the number of pattern variables involved in overriding might predict potential points of error. However, the implementation of the metric used in this work does not yet contain a type system, and so for this work it is not possible to check if the variables involved in the overriding have the same type, which would be interesting to factor into the measurement, and so this implementation of the metric treats all overriding in the same manner, regardless of the types involved. However it would be possible to re-implement the metric using a framework such as Programatica [41] to make use of type information in~\parencite{ryder2004software}


    In the world of imperative and object-oriented languages, software measure- ment, also known as software metrics, has been used for many years to provide developers with additional information about their programs. Such information can give programmers important indications about where bugs are likely to be in- troduced, or about how easy parts of a program are to test, for instance. This can be extremely valuable in easing the testing process by focusing programmers¿ at- tention on parts
    of the program where their effort may provide the greatest overall benefit, which in turn can help ease the whole process of validating software.~\parencite{ryder2004software}.

    The example of the imperative and object-oriented communities suggests that software metrics could provide a useful complement to the existing debugging tools available to functional programmers today. Some of the measurement techniques from imperative and object-oriented languages may transfer quite cleanly to func- tional languages, for instance the pathcount metric which counts the number of execution paths through a piece of program code, but some of the more advanced features of
    functional programming languages may contribute to the complexity of a program in ways that are not considered by traditional imperative or object- oriented metrics. It may therefore be necessary to develop new metrics for certain aspects of functional programs.~\parencite{ryder2004software}

    Software measurement is a technique in which quantitative measures, often called metrics, are taken from the source code of a program. Typically metrics attempt to quantify how complex a piece of source code is to understand, modify or test. This notion of complexity should not be confused with the computational complexity of an algorithm, which is typically denoted using O( notation. Com- putational complexity is concerned with runtime behaviour, rather than how easy or hard it is for a
    programmer to maintain.~\parencite{ryder2004software}

    One of the claims of software measurement is that it can help identify the parts of a system which are most likely to benefit from inspection. This allows resources to be focused where they will help most and allows them to be used in their most effective way, e.g. peer reviews of small sections of code, rather than large sections, or indicating parts of a program that may benefit from refactoring code changes.~\parencite{ryder2004software}

    Abstract¿The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ¿McCabes versus Halstead versus lines of code counts¿ for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected.~\parencite{menzies2007data}. <-- Hyvaa pointtia siita kuinka metriikka itsessaan ei tarkoita mitaan, vaan kuinka sita kaytetaan!  

    Given recent research in artificial intelligence, it is now
    practical to use data miners to automatically learn
    predictors for software quality. When budget does not
    allow for complete testing of an entire system, software
    managers can use such predictors to focus the testing on
    parts of the system that seem defect-prone. These potential
    defect-prone trouble spots can then be examined in more
    detail by, say, model checking, intensive testing, etc.~\parencite{menzies2007data} <-- Hyvaa pointtia machine learningin kaytosta

    This brittleness result offers a new insight on prior work.
    Prior results about defect predictors were so contradictory
    since they were drawn from a large space of competing
    conclusions with similar but distinct properties. Different
    studies could conclude that, say, lines of code are a better/
    worse predictor for defects than the McCabes complexity
    attribute, just because of small variations to the data.
    Bayesian methods smooth over the brittleness problem by
    polling numerous Gaussian approximations to the nu-
    merics distributions. Hence, Bayesian methods do not get
    confused by minor details about candidate predictors.~\parencite{menzies2007data}

\end{comment}
%}}}

%}}}

%{{{Functional programming
\chapter{Functional programming}
%{{{Chapter questions
\begin{comment}
\end{comment}
%}}}

A recurring challenge of software engineering is to develop applications that are not too complex to comprehend and reason about. Actions towards this complexity is actively being taken by e.g. refactoring~\parencite{fowler1999refactoring}, and by analyzing the quality of the software with metrics~\parencite{boehm1976quantitative}. In turn,~\textit{modularization} is in the heart of software engineering, where the aim is to have loosely-coupled relationships between
components and the possibility of changing and upgrading these components with least effort possible~\parencite{hughes1989functional}. Effectively, software that has a well-thought structure is easier to develop, debug and maintain.
%{{{
\begin{comment}
As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write, easy to debug, and provides a collection of modules that can be re-used to reduce future programming costs. Conventional languages place conceptual limits on the way problems can be modularised.~\parencite{hughes1989functional}.

\end{comment}
%}}}

Several programming paradigms exist for developing software. The most popular paradigm is the~\textit{imperative} approach, where programs are written in step-by-step instructions and commands~\parencite{TODO: Better source than sebesta}. This approach is similar to the design of the von Neumann computer architecture and its basic implementation of sending messages back and forth between the CPU and the store (memory)~\parencite{backus1978can}. In turn, many programming languages have
adapted the same approach as in the von Neumann architecture to their execution flow: variables, control statements and assignment statements~\parencite{backus1978can}.
%{{{
\begin{comment}
In its simplest form a von Neumann computer has three parts, a central processing unit (or CPU) a store, and a connecting tube that can transmit a single word between the CPU and the store (and send an address to the store).~\parencite{backus1978can}.

All have been designed to make efficient use of von Neumann architecture computers. Although the impera- tive style of programming has been found acceptable by most programmers, its heavy reliance on the underlying architecture is thought by some to be an unnecessary restriction on the alternative approaches to software development.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

The critics of imperative languages argue that as programs grow large, the imperative approach inherently struggles with expressing non-trivial programs consisting of numerous modules in a succinct manner~\parencite{backus1978can,hughes1989functional}. For example, the inclusion of explicit variables forces the developers to understand all of them concurrently, and in the worst case, keep a mental model of the relationships of these variables~\parencite{TODO: Better source than sebesta}.
%{{{
\begin{comment}
One of the fundamental characteristics of programs written in impera- tive languages is that they have state, which changes throughout the execution process. This state is represented by the program¿s variables. The author and all readers of the program must understand the uses of its variables and how the program¿s state changes through execution. For a large program, this is a daunting task. This is one problem with programs written in an imperative language that is not present in
a program written in a pure functional language, for such programs have neither variables nor state.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

Functional programming is a different approach to software engineering compared to imperative languages. It utilizes mathematical functions for program flow, such as recursion and conditional expressions, in contrast to the characteristic of sequential and iterative repetition found in imperative languages~\parencite{TODO: Better source than sebesta}. Moreover, variables that model values in memory locations do not obviously exist in mathematics, and as such, are also omitted from pure functional
programming. 
%{{{
\begin{comment}
One of the fundamental characteristics of mathematical functions is that the evaluation order of their mapping expressions is controlled by recursion and conditional expressions, rather than the sequencing and iterative repetition that are common to the imperative programming languages.~\parencite{sebesta2002concepts}

However, a subprogram in an imperative language may depend on the current values of several nonlocal or global variables. This makes it difficult to determine statically what values the subprogram will produce and what side effects it will have on a particular execution.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}
TODO: Esimerkki factorialista?

This thesis uses the Haskell programming language~\parencite{marlow2010haskell} to demonstrate the concepts of functional programming. Haskell, named after the logician Haskell B. Curry, was developed to be a multi-purpose functional programming language. Used both in education, research and by practitioners, Haskell has enjoyed steady popularity as one of the modern functional languages. Albeit several other alternatives (and predecessors) exist, Haskell follows the functional paradigm in a ``pure'' manner with education as one of the language's design motives.~\parenciteseveral{marlow2010haskell}. However, this thesis does not serve as an introduction to Haskell itself, but rather to the main concepts of functional programming. The syntax of Haskell is not explained, although the expressiveness, one of the goals of the functional paradigm, should aid in understanding the examples provided in the thesis.
%{{{
\begin{comment}
The committee's primary goal was to deisgn a language that satisfied these constraints:
1. It should be suitable for teaching, research, and applications, including building large systems.
2. It should be completely described via the publication of a formal syntax and semantics.
3. It should be freely available. Anyone should be permitted to implement the language and distribute it to whomever they please.
4. It should be based on ideas that enjoy a wide consensus.
5. It should reduce unnecessary diversity in functional programming languages.
~\parencite{marlow2010haskell}
\end{comment}
%}}}

%{{{Additional sources
\begin{comment}
    Haskell is a high level language. A really high level language. We can spend our days programming entirely in abstractions, in monoids, functors and hylomorphisms, far removed from any particular hardware model of computation. The language specification goes to great lengths to avoid prescribing any particular evaluation model. These layers of abstraction let us treat Haskell as a notation for computation itself, letting the programmer concentrate on the essence of their problem without getting bogged down in low level implementation decisions. We get to program in pure thought.~\parencite{o2008real}

    The earliest programming languages were developed with one simple goal in mind: to provide a vehicle through which one could control the behavior of computers. Not sur- prisingly, the early languages reflected the structure of the underlying machines fairly well. First, it became obvious that what was easy for a machine to reason about was not necessarily easy for a human being to rea- son about.~\parencite{hudak1989conception} In Conception_evolution_and_application.....pdf

    The class of functional, or applicative, programming languages, in which computation is carried out entirely through the evaluation of expressions, is one such family of languages, and debates over its merits have been quite lively in recent years.~\parencite{hudak1989conception}

    Among the claims made by functional language advocates are that programs can be written quicker, are more concise, are higher level (resembling more closely traditional mathematical notation), are more amenable to formal reasoning and analysis, and can be executed more easily on parallel architectures. Of course, many of these features touch on rather subjective issues, which is one reason why the debates can be so lively.~\parencite{hudak1989conception}.

    TODO: Maininta myös Lispistä!! Esim. hudak1989conception :ssa hyvää juttua
    Despite its impurities, Lisp had a great influence on functional language development, and it is encouraging to note that modern Lisps (especially Scheme) have returned more to the purity of the lambda calculus rather than the ad hocery that plagued the Maclisp era. This return to the purity includes the first-class treatment of functions and the lexical scoping of identifiers.~\parencite{hudak1989conception}.

    A purely functional programming language does not use variables or assignment statements, thus freeing the programmer from concerns related to the memory cells, or state, of the program. Without variables, iterative con- structs are not possible, for they are controlled by variables.~\parencite{TODO: Better source than sebesta}.

    Haskell functions are in general pure functions: when given the same arguments, they return the same results. The reason for this paradigm is that pure functions are much easier to debug and to prove correct. Test cases can also be set up much more easily, since we can be sure that nothing other than the arguments will influence a function's result. We also require pure functions not to have side effects other than returning a value: a pure function must be self-contained, and cannot open a network connection, write a file or do anything other than producing its result. This allows the Haskell compiler to optimise the code very aggressively.
    However, there are very useful functions that cannot be pure: an input function, say getLine, will return different results every time it is called; indeed, that's the point of an input function, since an input function returning always the same result would be pointless. Output operations have side effects, such as creating files or printing strings on the terminal: this is also a violation of purity, because the function is no longer self-contained.~\parencite{wikibooks2014haskell}

    The basic difference between functional and imperative programming lan-
    guages lies in the hiding of the computational model (Petre & Winder, 1990). 
    The imperative model incorporates the Von Neuman machine characteristics 
    in the notions of assignment, state and effect. A characteristic of this language 
    class is the explicit flow of control, e.g. sequencing, selection and repetition. In 
    assignments, the value of memory places denoted with variables is changed 
    during program execution. This model of operation by change of state and by 
    alteration of variable values is also named 'computation by effect'. The func-
    tional model is characterised by 'computation by value'. Functions return val-
    ues and have no side-effects, and expressions represent values. There is no no-
    tion of updatable memory accessible by instruction. The program consists of a 
    script with a number of mathematical-like definitions and an expression that 
    must be evaluated. Functions can be passed as arguments to other functions 
    and can be the result of a function application (higher-order functions). These 
    programs possess the property of referential transparency, which means that 
    in a fixed context the replacement of a subexpression by its value is completely 
    independent of the surrounding expression. Therefore functional programming 
    is more closely related to mathematical activities (Bird & Wadler, 1988). ~\parencite{van1995software} <- Hyvaa johdantoa FP:hen!
\end{comment}
%}}}

\section{Concepts}

The different concepts of functional programming are numerous, but shared between languages. This section aims to provide an overview into the features of functional languages, as well as their strengths and weaknesses. 

\subsection{Functions}

The main motivation behind the functional programming paradigm is the use of mathematical functions in expressing programs concisely. Unlike in imperative languages such as C++, functions in functional languages resemble the mathematical notion of functions. That is, rather than specifying values over sequential operations, function parameters are meant to be mapped over values.~\parenciteseveral{TODO: Better source than sebesta}. A function that returns an integer doubled by itself (effectively calculating the square of a number) would be defined in Haskell as:

\begin{lstlisting}
square x = x * x 
\end{lstlisting}

\noindent The assignment operator, {\tt =} is meant to define a function (in this case, the computation of a number's square) and not to actually assign any values to a variable. Additionally, the parameters of the function definition are fixed (constant), meaning they can not change in the domain of the function. This is opposite in imperative languages, where variables received as parameters are often manipulated and their state is changed. In pure functional languages, such as Haskell, variables are omitted and state does not exist in an operational or denotational sense. This can be observed through~\textit{referential transparency}, which holds that a function should return the same result every time with the same given parameters, since no ``external'' variables, or~\textit{side-effects}, can affect the computation.~\parenciteseveral{TODO: Better source than sebesta}.
%{{{
\begin{comment}
In this definition, the domain and range sets are the real numbers. The symbol K is used to mean ¿is defined as.¿ The parameter x can represent any member of the domain set, but it is fixed to represent one specific element during evalu- ation of the function expression. This is one way the parameters of mathemati- cal functions differ from the variables in imperative languages.~\parencite{sebesta2002concepts}.

Without variables, the execution of a purely functional program has no state in the sense of operational and denotational semantics. The execution of a function always produces the same result when given the same parameters. This feature is called referential transparency.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

A particular inspiration for functional languages is the~\textit{lamdba calculus}~\parencite{church1932set}. Its main purpose is to define the behavior of functions in an intuitive manner, where computations are the central aspect. In particular, the functions in lamdba calculus have one key abstraction: They can be applied to themselves. In addition to functions being anonymous in lamdba calculus, recursion does not have to be explicitly stated because of the general applicability of the functions.~\parenciteseveral{hudak1989conception}. For an example of lamdba calculus, the following expression results in 8 as the value of the computation:

\begin{lstlisting}[mathescape]
$(\lambda (x)x * x * x)(2)$
\end{lstlisting}

\noindent Although the semantics of lamdba calculus are essentially small and simple~\parencite{hudak1989conception}, its application is more involved and thus is not described in this thesis. In conclusion, lambda calculus serves as the basis for functions in functional programming languages.
%{{{
\begin{comment}
Church's work was motivated by the desire to create a calculus (informally, a syntax for terms and set of rewrite rules for transforming terms) that captured one's intuition about the behavior of cuntions. This approach is counter to the consideration of functions, as for example, sets (more precisely, sets of argument/value pairs), since the intent was to capture the computational aspects of functions.~\parencite{hudak1989conception}.

Its [lambda calculus] type-free nature yielded a particularly small and simple calculus, and it had one very interesting property, capturing functions in their fullest generality: Functions could be applied to themselves.~\parencite{hudak1989conception}

This ability of self-application is what gives the lambda calculus its power. It allows us to gain the effect of recursion without explicitly writing a recursive definition.~\parencite{hudak1989conception}.
\end{comment}
%}}}

%{{{ Additional sources
\begin{comment}
The functional programming paradigm, which is based on mathematical functions, is the design basis of the most important nonimperative styles of languages. This style of programming is supported by functional programming languages.~\parencite{sebesta2002concepts}.

\end{comment}
%}}}

\subsection{Higher-order functions}

Higher-order functions are the landmark feature of functional languages. Derived from the previous observation of generally applicable functions in lamdba calculus, higher-order functions can take functions as parameters and also return a function~\parencite{TODO: Better source than sebesta}. Thus functions are treated like any other value. This has convenient practical implications, where functions can be composed of other functions. For example, composing a function that executes a given function twice is possible:
\begin{lstlisting}
twice f = f . f
\end{lstlisting}

\noindent Now {\tt twice sum 4}, where {\tt sum} is {\tt sum x = x * x}, would return 256. The function twice is now~\textit{curried} (i.e.\ composes multiple functions) and returns a new function. As twice is a function it can be used again for function composition, et cetera.
%{{{
\begin{comment}
A higher-order function, or functional form, is one that either takes one or more functions as parameters or yields a function as its result, or both.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

``Glueing'' together functions increases modularity of software written in a functional manner. Treating functions as first-class values also provides a way to abstract over functional behavior.~\parenciteseveral{hudak1989conception}.
%{{{
\begin{comment}
If functions are treated as first-class values in a language--allowing them to be stored in data structures, passed as arguments, and returned as results--they are referred to as higher-order functions.~\parencite{hudak1989conception}.

That glueing property comes not just from the ability to compose functions but also from the ability to abstract over functional behavior as described above.
\end{comment}
%}}}

\begin{itemize}
    \item Partial application, currying
\end{itemize}

TODO: Examples of popular higher-order functions? map, filter..

\subsection{Lazy evaluation}

Derived from lambda calculus, a~\textit{nonstrict} programming language such as Haskell avoids evaluating values until they are actually needed(TODO: Selvennä tätä, nonstrict <-> WHNF <-> lazy evaluation). In contrast,~\textit{strict} languages, such as Java and C++, eagerly compute parameter values to ensure proper evaluation order (TODO: ??). Deferring evaluation of parameter values offers the obvious advantage of saving computation time resulting in more efficient programs. On the other hand,~\textit{lazy evaluation} also grants the possibility of defining unbounded data structures such as infinite lists.~\parenciteseveral{hudak1989conception}. 
%{{{
\begin{comment}
A programming language is strict if it requires all actual parameters to be fully evaluated, which ensures that the value of a function does not depend on the order in which the parameters are evaluated. A language is nonstrict if it does not have the strict requirement.~\parencite{hudak1989conception}.

First, nonstrict languages are gener- ally more efficient, because some evaluation is avoided.13 Second, some inter- esting capabilities are possible with nonstrict languages that are not possible with strict languages. Among these are infinite lists. Nonstrict languages can use an evaluation form called lazy evaluation, which means that expressions are evaluated only if and when their values are needed.~\parencite{hudak1989conception}.

The idea is that a programmer should be able to describe a specific data structure without worrying about how it gets evaluated.~\parencite{hudak1989conception}.
\end{comment}
%}}}

\begin{lstlisting}
-- Examples of infinite lists
positives = [1,2..]
evens = [2,4..]
squares = [x * x | x <- [0..]] 

\end{lstlisting}

\noindent The values of the lists are not computed until they are needed. For example, {\tt take 5 squares} returns {\tt [0, 1, 2, 4, 9, 16, 25, 36, 49, 64, 81]}. Effectively, lazy evaluation provides three important mechanisms for evaluating function parameters~\parencite{TODO: Better source than sebesta}:

\begin{itemize}
    \item Only the needed parameters are evaluated
    \item Only the needed part of a single parameter is evaluated
    \item An evaluated parameter is reused if it appears again in function calls
\end{itemize}
%{{{
\begin{comment}
zy evaluation means that an actual parameter is evaluated only when its value is necessary to evaluate the function. So, if a function has two parameters, but on a particular execution of the function the first parameter is not used, the actual parameter passed for that execution will not be evaluated. Furthermore, if only a part of an actual parameter must be evaluated for an execution of the function, the rest is left unevaluated. Finally, actual parameters are evaluated only once, if at all, even if the same actual parameter appears more than once in a function call.~\parencite{sebesta2002concepts}.
\end{comment}
%}}}

\noindent As previously presented, functions have the convenient property of being freely composable from other functions. In conjuction with lazy evaluation, this function composition becomes especially useful when ``glueing'' several functions together. A popular usage in functional programs is to include ``generators'' (functions) that produce arbitrarily large data sets. However, combining these generators with functions that include predicates for selecting the correct values provides a modular and concise way to structure applications.~\parencite{hughes1989functional}. This separation of data structures and control is also one of the central notions of functional programming~\parencite{hudak1989conception}.
%{{{
\begin{comment}
Since this method of evaluation runs f as little as possible, it is called 'lazy evaluation'. It makes it practical to modularise a program as a generator which constructs a large number of possible answers, and a sleector which chooses the appropriate one.~\parencite{hughes1989functional}.
\end{comment}
%}}}

\subsection{Polymorphic types}

Although a somewhat technical concept,~\textit{data types} are an essential part of modern functional languages~\parencite{hudak2007history}. In this section, algebraic and abstract data types are discussed and one of the main mechanism for their use, pattern matching, is presented.
%{{{
\begin{comment}
Data types and pattern matching are fundamental to most modern functional languages (with the notable exception of Scheme).~\parencite{hudak2007history}.
\end{comment}
%}}}

~\textit{Algebraic data types} increase the flexibility in which a computation can depend on type alternatives. In other words, the data types of functional languages are often~\textit{polymorphic}, i.e.\ they can represent multiple types.~\parenciteseveral{hudak2007history}. A popular example regarding algebraic data types is to represent the types of a binary search tree:
%{{{
\begin{comment}
In general, an algebraic type specifies a sum of one or more alter- natives, where each alternative is a product of zero or more fields. It might have been useful to permit a sum of zero alternatives, which would be a completely empty type, but at the time the value of such a type was not appreciated.~\parencite{hudak2007history}
\end{comment}
%}}}

\begin{lstlisting}
data Tree a = Leaf a | Branch (Tree a) (Tree a)
\end{lstlisting}

Here {\tt Tree} is a~\textit{data type}. {\tt Leaf} and {\tt Branch} are~\textit{data constructors}. What follows is that~\textit{pattern matching} can be used to conveniently address either of these data constructors in a function~\parencite{hudak2007history}. For example, a function that returns the size (the number of elements) of the tree recursively can be expressed as follows:

\begin{lstlisting}
size (Leaf x)     = 1
size (Branch l r) = size l + size r + 1
\end{lstlisting}

Effectively, {\tt size} receives a data type of {\tt Tree} as the only parameter. In fact, the function signature is {\tt size :: Tree a -> Int}. The pattern matching mechanism is then utilized for evaluating either expression, depending on the actual data constructor. 

Due to the nature of lazy evaluation, the actual implementation of pattern matching has proved to be challenging in functional languages. In particular, the order of parameters rise subtle problems depending on their evaluation mechanism~\parencite{hudak1989conception}. In the end, matching patterns from top to bottom and evaluating parameters from left to right serves as a sufficient compromise and is used in most modern functional languages~\parencite{hudak2007history}.
%{{{
\begin{comment}
In SASL, KRC, Hope, SML, and Miranda, matching against equa- tions is in order from top to bottom, with the first matching equation being used. Moreover in SASL, KRC, and Miranda, matching is from left to right within each left-hand-side¿which is important in a lazy language, since as soon as a non-matching pattern is found, matching proceeds to the next equation, potentially avoiding non- termination or an error in a match further to the right. Eventually, these choices were made for
Haskell as well ~\parencite{hudak2007history}.
\end{comment}
%}}}

~\textit{Abstract data types} address the need for increased modularity and security. Specifically, the actual data type is not exposed outside its module, but rather, can be accessed through utility functions. Similar to~\textit{interfaces} in Java, abstract data types allow changing the representation type without affecting external modules that depend on this ``interface''.~\parenciteseveral{hudak1989conception}.
%{{{
\begin{comment}
Another idea in data abstraction originating in imperative languages is the notion of an abstract datatype (ADT) in which the details of the implementation of a datatype are hidden from the users of that type, thus enhancing modularity and security.~\parencite{hudak1989conception}.

The advantage of this, of course, is that one is free to change the representation type without fear of breaking some other code that uses the ADT.~\parencite{hudak1989conception}.
\end{comment}
%}}}

In conclusion, data types in functional languages bear similarities with polymorphism in object-oriented design (TODO: NOT!)

\subsection{Functors}

Functors add flexibility when encountering different data types, such as the previously presented {\tt Tree} construct or an ordinary list data structure. As looping over the values of a construct would be convenient, functors aim at providing an abstraction for this kind of behavior. Namely, functors enable ``mapping'' functions over data types and is one of the central routines of functional programming.~\parenciteseveral{lipovavca2011learn}. Functors can be thought as
containers for values, or more precisely, as computational contexts~\parencite{typeclassopedia2014haskellwiki}. For example, {\tt Tree} can be made an instance of {\tt Functor}:
%{{{
\begin{comment}
And now, we're going to take a look at the Functor typeclass, which is basically for things that can be mapped over.~\parencite{lipovavca2011learn}

Another intuition is that a Functor represents some sort of ¿computational context¿. This intuition is generally more useful, but is more difficult to explain, precisely because it is so general~\parencite{typeclassopedia2014haskellwiki}
\end{comment}
%}}}
\begin{lstlisting}
instance Functor Tree where
    fmap f (Leaf x) = Leaf (f x)
    fmap f (Branch a b) = Branch (fmap f a) (fmap f b)
\end{lstlisting}

\noindent Now a given function can be applied (~\textit{mapped}) over the values in a {\tt Tree}:

\begin{lstlisting}
fmap (*2) (Branch (Leaf 2) (Leaf 3)) 
-- Outputs: Branch (Leaf 4) (Leaf 6)
\end{lstlisting}

Thus the values inside the {\tt Tree} ``context'' are changed, without affecting the computational context itself. Perhaps a more general example of a functor instance is the list data type, since it provides a map function like our tree structure~\parencite{typeclassopedia2014haskellwiki}. Derived from category theory, functors provide certain ``homomorphism'' or abstractions over functional data structures. As is the case with mathematics, certain laws must be satisfied for functors, expressed in Haskell code as: 
%{{{
\begin{comment}
From the context point of view, the intention is that fmap applies a function to a value without altering its context~\parencite{typeclassopedia2014haskellwiki}

As noted before, the list constructor [ is a functor ¿; we can use the standard list function map to apply a function to each element of a list ¿.~\parencite{typeclassopedia2014haskellwiki}.
\end{comment}
%}}}

\begin{lstlisting}
fmap id = id
fmap (g . h) = (fmap g) . (fmap h))
\end{lstlisting}

\noindent The first law dictates simply that mapping an ``identity'' function (i.e. {\tt x = x}) over a value should always return the value itself, unchanged. The second law asserts that mapping over functions that are then composited should be the same act as mapping over a result of function composition. If a functor instance satisfies the first law, also the second law is automatically passed.~\parenciteseveral{typeclassopedia2014haskellwiki}. As trivial as these laws seem, they are a powerful aid in reasoning about code written in a functional language.
%{{{
\begin{comment}
As far as the Haskell language itself is concerned, the only requirement to be a Functor is an implementation of fmap with the proper type. Any sensible Functor instance, however, will also satisfy the functor laws, which are part of the definition of a mathematical functor. There are two:

fmap id = id
fmap (g . h) = (fmap g) . (fmap h))

Together, these laws ensure that fmap g does not change the structure of a container, only the elements. Equivalently, and more simply, they ensure that fmap g changes a value without altering its context ¿.

The first law says that mapping the identity function over every item in a container has no effect. The second says that mapping a composition of two functions over every item in a container is the same as first mapping one function, and then mapping the other.

The second law says that composing two functions and then mapping the resulting function over a functor should be the same as first mapping one function over the functor and then mapping the other one.~\parencite{typeclassopedia2014haskellwiki}.
\end{comment}
%}}}

\subsection{Applicative functors}

Applicative functors as an explicit abstraction are relatively new in functional programming~\parencite{mcbride2008functional}. As their name implies, applicative functors are an extension for functors. As functions essentially take only one parameter (and are thus~\textit{curried} functions for subsequent parameters), a data type can naturally contain functions. For example, the leaves of a {\tt Tree} data type could contain functions as values: 
\begin{lstlisting}
fmap (*) (Branch (Leaf 2) (Leaf 3)))
\end{lstlisting}

Now the leaves contain multiplicating functions of 2 and 3 respectively. However, a plain functor can not extract a function out of a functor value ({\tt Leaf} in this case). As a result this recurring pattern of ``picking'' the function value out of the functor was abstracted as applicative functors~\parencite{mcbride2008functional}. The {\tt Tree} data type can be made as an instance of {\tt Applicative}:
\begin{lstlisting}
instance Applicative Tree where
    pure = Leaf
    (Leaf f) <*> (Leaf x) = Leaf (f x)
    (Branch a b) <*> (Branch c d) = Branch (a <*> c) (a <*> d)
\end{lstlisting}

\noindent Now {\tt <*>} can be used for applying functor values:

\begin{lstlisting}
(Branch (Leaf (*2)) (Leaf (*3))) <*> (Branch (Leaf 2) (Leaf 3))
-- Outputs Branch (Leaf 4) (Leaf 6) 
\end{lstlisting}

In essence, {\tt <*>} is just function application with a context~\parencite{typeclassopedia2014haskellwiki}. The {\tt pure} function ``embeds pure computations into the pure fragment of an effectful world''~\parencite{mcbride2008functional}. In other words, a pure value is lifted into the computational context of {\tt Applicative}. Applicative functors must also satisfy some laws: 
%{{{
\begin{comment}
The idea is that pure embeds pure computations into the pure fragment of an effectful world¿the resulting computations may thus be shunted around freely, as long as the order of the genuinely effectful computations is preserved.~\parencite{mcbride2008functional}.
\end{comment}
%}}}

\begin{itemize}
    \item The identity law:
          {\tt pure id <*> v = v}
    \item Homomorphism
          {\tt pure f <*> pure x = pure (f x)}
    \item Interchange
    \item Composition
\end{itemize}


The applicative type class contains the operator {\tt<*>} used for applying functors over functors:

%{{{
\begin{comment}
So now we know: there are strictly more Applicative functors than Monads. Should we just throw the Monad class away and use Applicative instead? Of course not! The reason there are fewer monads is just that the Monad structure is more powerful.~\parencite{mcbride2008functional}

\end{comment}
%}}}

\subsection{Monoids}

\subsection{Monads}

Functional programming languages can roughly be divided into two categories; pure and impure approaches.~\textit{Pure} functional languages, such as Haskell, utilize lambda calculus in its simplest form, while~\textit{impure} languages, such as Scheme, introduce additional features, such as assignments, into computations. The main motivation behind ``pure'' languages is that they make data flow explicit and expressions only depend on their free variables. This enables a powerful aid in reasoning about the code's logic, since substitution is always valid(TODO: ??). On the other hand,
impure features in a language can offer more conveniency for the developer as the concept of purity is not strictly enforced.~\parenciteseveral{wadler1995monads}.
%{{{
\begin{comment}
The functional programming community divides into two camps. Pure languages,
such as Miranda2 and Haskell, are lambda calculus pure and simple. Impure lan- guages, such as Scheme and Standard ML, augment lambda calculus with a number of possible effects, such as assignment, exceptions, or continuations. Pure languages are easier to reason about and may benefit from lazy evaluation, while impure lan- guages offer efficiency benefits and sometimes make possible a more compact mode of expression.~\parencite{wadler1995monads}.

A program in a pure functional language is written as a set of equations. Ex- plicit data flow ensures that the value of an expression depends only on its free variables. Hence substitution of equals for equals is always valid, making such pro- grams especially easy to reason about. Explicit data flow also ensures that the order of computation is irrelevant, making such programs susceptible to lazy evaluation.~\parencite{wadler1995monads}
\end{comment}
%}}}

As an example, adding error handling to program logic can turn out to be tedious, since every recursive function call would need to be checked for errors and subsequently handled. In contrast, exceptions in an impure language would not introduce such structuring of code. More to the point, counting the number of operations performed on a function would need similar structuring of the code where the count is explicitly passed on for further operations. Again, in an impure language a global variable for the count would suffice.~\parenciteseveral{wadler1995monads}. Largely for this division of pure and impure functional languages,~\textit{monads} were introduced as an utility to combine the benefits of both approaches. Namely, monads are used ``to integrate impure effects into pure functional languages''~\parencite[25.]{wadler1995monads}. As is customary for the functional programming paradigm, also the concept of monads is derived from mathematics, and in this case, category theory.~\parenciteseveral{wadler1995monads}.
%{{{
\begin{comment}

Recent advances in theoretical computing science, notably in the areas of type theory and category theory, have suggested new approaches that may integrate the benefits of the pure and impure schools. These notes describe one, the use of monads to integrate impure effects into pure functional languages.~\parencite{wadler1995monads}

The concept of a monad, which arises from category theory, has been applied by Moggi to structure the denotational semantics of programming languages [13, 14]~\parencite{wadler1995monads}.
\end{comment}
%}}}

Monads can be seen as ``wrappers'' or contexts for values, similar to functors and applicative functors. For example, the standard Haskell data type {\tt Maybe} is an instance of the monad type class. 



Monads have three laws that they must obey.~\textit{Left identity}

~\textit{Right identity}

~\textit{Associativity}

~\textit{Monads} address the issue of ``purity'' in functional languages. For example, a user interacting with the application results in input and output, or in other words, communication between the application and external domains. However, as functions are pure, they can not As the meaning of purity was presented earlier in the chapter,  A

%{{{Additional sources
\begin{comment}
There are three laws that these definitions should satisfy in order to be a true monad in the sense defined by category theory. These laws guarantee that composition of functions with side effects is associative and has an identity (Wadler, 1992b).~\parencite{hudak2007history}

Although Wadler¿s development of Moggi¿s ideas was not directed towards the question of input/output, he and others at Glasgow soon realised that monads provided an ideal framework for I/O. The key idea is to treat a value of type as a ¿computation¿ that, when performed, might perform input and output before delivering a value of type.~\parencite{hudak2007history}

\end{comment}
%}}}

\section{Metrics for functional programming}

As previously presented in chapter~\ref{chap:software_metrics}, software metrics are an essential approach to quantifying attributes about the quality of software. Even though it is a profoundly different paradigm compared to object-oriented and imperative approaches, also functional programming can benefit from software metrics.

Prior research on metrics exclusively applicable to functional programming is scarce. Foundings of such research are primarily laid by~\citet{van1995software} and~\citet{ryder2004software}, whose research this section heavily leans on.~\citet{van1995software} initial work was spun by the need to effectively measure the readability of code programmed by students taking part in functional programming courses. Somewhat fortunately, many of the existing metrics, such as
lines of code and Halstead's program volume, are also applicable to functional programs with slight modifications~\parencite{van1995software}. Other such metrics are the measures of testing effort (pathcount, cyclomatic complexity), callgraph measures (size, depth, arc-to-node ratio) and coupling measures~\parencite{ryder2004software}.
%{{{
\begin{comment}
    A problem encountered was that these metrics have been defined mainly for imperative languages: there-
    fore, a first task was the definition of the metrics for functional programs.~\parencite{van1995software}

    ¿ Program size.The various measures of program size, such as LOC and
    Halstead¿s program volume can be transfered to functional programs with
    little or no modification. These measures are used in Section 4.5.2 of this
    chapter.
    ¿ Testing effort. Metrics that measure the effort involved in testing a program,
    such as pathcount and cyclomatic complexity, can be applied to functional
    programming languages, although as is described later in Section 4.5.1 im-
    plementing such metrics requires care to correctly consider all execution
    paths. The pathcount measure is analysed for Haskell in Section 4.5.1 of
    this chapter.
    ¿ Callgraph measures.Measurements taken from callgraphs, such as size,
    depth, width and arc-to-node ratio, can all be applied to functional pro-
    grams without modification.These measures are examined for use with
    Haskell in Section 4.4 of this chapter.
    ¿ Coupling measures. Although it is possible to apply OO coupling measures,
    such as object level and class level coupling and coupling strength, from
    functional languages such as Haskell, some modifications are needed to map
    different language elements into the metric. For instance, OO coupling met-
    rics normally measure coupling between classes, but Haskell classes are not
    classes in the OO sense, and instead one is most likely to measure coupling
    between modules or functions.Coupling between functions is examined
    Briefly in Sections 4.4.2 and 4.4.3 of this chapter.~\parencite{ryder2004software}.
\end{comment}
%}}}

~\citet{ryder2004software} presents many code-specific metrics that are specific to the functional paradigm. For example, measures for the size and complexity of patterns are provided, as well as some intermodule metrics, such as the distance of declarations and their use, are created. In addition,~\citet{ryder2004software} integrates existing measures such as attributes of callgraphs. 

The validation methodology used by~\citet{ryder2004software} is based on~\textit{subjective complexity}, which is referred as ``a notion of the complexity that might be perceived by a programmer attempting to understand or modify the given function''. The metrics are applied to source code version histories of two relatively small programs and statistical analysis is utilized for determining the correlation of the measurements.


Software Measurement for Functional Programming~\parencite{ryder2004software}

\subsection{Complexity of patterns}

One of the most used features in functional programming is pattern matching. In Haskell, pattern matching is used in various ways. In addition to normal function arguments, data types are often used as arguments to pattern matching. The {\tt \_} wildcard can be used to match (or ignore) any argument, and patterns can also be used in function bodies. Constructing metrics to deduce the complexity of patterns is then an obvious first interesting step towards measuring FP programs.(TODO:
Lahde? Esimerkki PatMatchista?)

~\citet{ryder2004software} found a relatively weak correlation between the~\textit{number of pattern variables} and subjective complexity in two case-study programs. He argues that an increased number of variables in pattern matching also increases the effort required to comprehend that specific code. The case-study programs utilize pattern matching heavily, with 70-85\% of the functions containing patterns.

The measure~\textit{number of overridden or overriding pattern variables} deals with the fact that pattern and expression variables can be easily overridden in functional programming. For example, a {\tt where} or {\tt let} are often used for denoting intermediary variables(TODO: Esimerkki?). The hypothesis is that an increased amount of overridden variables in patterns increase program complexity.~\citet{ryder2004software} finds a statistical correlation between the hypothesis
and the metric, but it can be argued that such a programming style is detrimental in its own. Thus the measure should be used as a sign of the need to refactor any patterns containing redundant naming~\parencite{ryder2004software}.
%{{{
\begin{comment}
    Patterns that introduce variables may override existing identifiers, and identifiers
    in patterns may be overridden by those in a where or let clause. Overriding
    identifiers in this manner can be confusing, for instance in Example 10 it is not
    immediately clear which a is being used at any point in the function.~\parencite{ryder2004software}.

    The correlation results for the Peg Solitaire program, shown in Table 9 in
    Appendix B, do not exhibit any statistically significant correlation with the num-
    ber of bug fixing changes, probably because of the low number of instances of
    overriding occurring in the program. However, the results from the Refactoring
    program show a statistically significant correlation of 0.3731. Thus the following
    observation can be made.
    Observation 4.1.2 Increasing the number of overridden pattern variables in-
    creases the subjective complexity of the function.
    One might claim that overriding pattern variables is an inherently undesirable
    programming paradigm and as such this metric may be a useful tool to highlight
    such occurrences, so that they may be corrected by the programmer.Such a
    tool might combine this metric with a visualisation technique such as the pixel
    representation to quickly highlight such code (See for instance Sections 6.1 and
    6.5 of Chapter 6), or form part of an automated refactoring tool such as HaRe
    [60].~\parencite{ryder2004software}.
\end{comment}
%}}}

The~\textit{number of constructors} in a pattern can be an indicator of program complexity. Two hypotheses are provided by~\citet{ryder2004software}, where the first deals with the notion that an increased count of constructors also increases complexity. The second hypothesis argues, counteractively, that if the constructors are named properly, it can reduce subjective complexity of a function. Once again,~\citet{ryder2004software} found a slight statistical correlation
between an elevated usage of constructors in patterns and subjective complexity.
%{{{
\begin{comment}
    Patterns are commonly used in Haskell programs for manipulating algebraic data
    types by matching against constructor names. A possible metric is to count how
    many constructors are used in a pattern. There are two interesting and opposing
    hypotheses about the effect of this attribute on the subjective complexity of a
    function.
    One hypothesis is that using more constructors in a pattern requires more
    objects to be understood in order to comprehend the pattern, therefore increasing
    the complexity. The alternative hypothesis is that if constructor names are chosen
    well they are descriptive, and therefore add an element of documentation to the
    pattern, possibly reducing the subjective complexity of the pattern.~\parencite{ryder2004software}.
\end{comment}
%}}}

Also often used entities in patterns are wildcards, which signal unneeded pattern arguments. However, given even slightly numerous function or constructor arguments, wildcards can be easy to misplace.
\begin{lstlisting}[caption={Example of wildcards in patterns},label={lst:wildcard-pattern}]
data Shape = Triangle Int Int Int -- Lengths of sides
                      Int Int Int -- RGB values

redValue :: Shape -> Int
redValue (Triangle _ _ _ r _ _) = r
\end{lstlisting}

Using the two case-study programs,~\citet{ryder2004software} found correlation between complexity and increased number of wildcards in pattern matching. However, he further argues that the complexity is not necessary due to wildcards themselves, but because of the more complex data structures involved.
%{{{
\begin{comment}
    Table 9 in Appendix B shows that once again the results for the Peg Soli-
    taire program show no statistically significant correlation, while the Refactoring
    program shows a correlation value of 0.3572. This seems to suggest that increas-
    ing the number of wildcards can increase the complexity, and so the following
    observation can be made.
    Observation 4.1.3 Large numbers of wildcards may indicate areas of increased
    subjective complexity.
    This seems to contradict the generally perceived wisdom that wildcards reduce
    complexity.However, an explanation that might account for this is that the
    functions that use patterns with wildcards may be manipulating complex data
    structures, and so changes may be due to the complexity of the data structures
    rather than the appearance of wildcards.~\parencite{ryder2004software}.
\end{comment}
%}}}

The~\textit{depth of nesting} in patterns can be measured. For example, {\tt [(a,b),(c,d)]} shows a list containing tuples. The maximum depth of the pattern is 2, since the nested elements amount to two (list and tuple). The sum of the depths, however, may be more accurate in depicting the level of nesting taking place.
\begin{lstlisting}
type Tuple = (Int, Int)
type TupleList = [Tuple]

pattern1 :: TupleList -> Int
pattern1 [(a,b)] = a + b

pattern2 :: TupleList -> Int
pattern2 [(a,b), (c,d),(e,f)] = a + b + c + d + e + f
\end{lstlisting}

For both patterns, {\tt pattern1} and {\tt pattern2}, the maximum depth is 2. However, sums of the depths are not the same between the patterns. Finally,~\citet{ryder2004software} found a somewhat strong correlation for increased depth of nesting and complexity.
%{{{
\begin{comment}
    When measuring the depth of nesting one must consider how to measure the
    depth of nesting of patterns that contain more than one nested pattern.For
    instance, [(a,b),(c,d)] contains two nested patterns. One method is to take
    the maximum depth of all the nested patterns. Another method is to take the
    sum of the depths of all the nested patterns.
    Taking the maximum of the depths measures only how deeply nested the pat-
    tern is, while taking the sum of the depths effectively measures how much nesting
    is taking place, which may be more accurate. However it may be that taking the
    sum of the depths will actually be measuring the size of the pattern, in which case
    one would see a strong correlation between those two measures.~\parencite{ryder2004software}
\end{comment}
%}}}

The last pattern metric presented by~\citet{ryder2004software} is~\textit{pattern size}, which can be measured in two ways. The number of components in a pattern can be calculated or the depth of the resulting~\textit{abstract syntax tree}(TODO: reference?) can be measured.~\citet{ryder2004software} utilizes the amount of nodes in the syntax tree as a measure, and found a correlation for increased size in patterns and subjective complexity.
%{{{
\begin{comment}
The size of a pattern could be measured in a number of ways. For instance, one
could count the number of components in the abstract syntax tree, or one might
take the depth of abstract syntax tree as a measure of its size. For the purpose
of exploring the effect of pattern size on the number of changes, the number of
nodes in the abstract syntax tree of the pattern was chosen as the size measure.
The metric results in Table 9 in Appendix B show that the Peg Solitaire
program had no significant correlation, while the Refactoring program had a cor-
relation of 0.5423, which seems to suggest the following observation.~\parencite{ryder2004software}
\end{comment}
%}}}

\subsection{Distance between declarations and usage}
~\begin{itemize}
    \item Distance by the number of scopes
    \item Distance by the number of declarations in scope
    \item Distance by the number of source lines
    \item Distance by the number of parse tree nodes
~\end{itemize}

\subsection{Attributes of recursive Functions}
~\begin{itemize}
    \item Binary indication of recursion 
    \item Number of recursive paths
    \item Number of trivial and non-trivial recursive paths
    \item Sum and product of recursive path lengths
~\end{itemize}

\subsection{Attributes of callgraphs}
~\begin{itemize}
    \item Strongly connected components
    \item Indegree of a function
    \item Outdegree of a function 
    \item Size of a functions callgraph
~\end{itemize}

\subsection{Miscellaneous attributes of functions}
~\begin{itemize}
    \item Pathcount of a function~\parencite{barnes2000evolution}
    \item Size of a function
    \item Operators and operands
    \item Outdegree of a function 
    \item Size of a functions callgraph
~\end{itemize}

\subsection{Degree of polymorphism}


\subsection{Summary}

Critique:
~\begin{itemize}
    \item Ryder's work uses only two small case-study programs
~\end{itemize}

\section{Summary}

%{{{ Additional sources
\begin{comment}
Some in the functional programming community have claimed that the use of functional programming results in an order-of-magnitude increase in productivity, largely due to functional programs being claimed to be only 10 percent as large as their imperative counterparts. While such numbers have been actually shown for certain problem areas, for other problem areas, func- tional programs are more like 25 percent as large as imperative solutions to the same problems (Wadler,
1998).~\parencite{sebesta2002concepts}

In fact, because of the necessity of dealing with variables, imperative programs have many trivially simple lines for initializing and making small changes to variables.~\parencite{sebesta2002concepts}

However, there are now compilers for most functional languages, so that execution speed disparities between functional languages and compiled imperative languages are no longer so great. One might be tempted to say that because functional programs are significantly smaller than equivalent imperative programs, they should execute much faster than the imperative programs. However, this often is not the case, because of a collection of language characteristics of the functional
lan- guages, such as lazy evaluation, that have a negative impact on execution efficiency.~\parencite{sebesta2002concepts}.

Another source of the difference in execution efficiency between functional and imperative programs is the fact that imperative languages were designed to run efficiently on von Neumann architecture computers, while the design of functional languages is based on mathematical functions. This gives the imperative languages a large advantage.~\parencite{sebesta2002concepts}.

Functional languages have a potential advantage in readability. In many imperative programs, the details of dealing with variables obscure the logic of the program.~\parencite{sebesta2002concepts}.

In an imperative language, the pro- grammer must make a static division of the program into its concurrent parts, which are then written as tasks, whose execution often must be synchronized. This can be a complicated process. Programs in functional languages are natu- rally divided into functions. In a pure functional language, these functions are independent in the sense that they do not create side effects and their operations do not depend on any nonlocal or global variables.
Therefore, it is much easier to determine which of them can be concurrently executed.~\parencite{sebesta2002concepts}.

\end{comment}
%}}}

%}}}
%{{{Empirical study
\chapter{Empirical study}
Preliminary guidelines for empirical research in software engineering~\parencite{kitchenham2002preliminary}
%{{{Additional Sources
\begin{comment}
    Before developing and using metrics, one must have a clear idea about the 
    purpose of the metric. A Goal-Question-Metric method (Basili & Rombach, 
    1988) gives a framework for this issue. The goal and some questions have been 
    expressed above. In the research in this thesis, one major issue has been the 
    objective assessment of the comprehensibility of programs. Understanding ex-
    isting code is one of the more time-consuming tasks in the maintenance of 
    software products. A recent survey of models for code understanding is given 
    by von Mayrhauser (1994). The models are classified as either top-down mod-
    els or bottom-up models. Top-down models emphasise the nature and structure 
    of domain knowledge and how it is represented in and mapped onto code and 
    documentation information. Bottom-up models build understanding from de-
    tail code using control flow and data flow.~\parencite{van1995software}
\end{comment}
%}}}

%}}}
%{{{Discussion
\chapter{Discussion}

%}}}
%{{{Summary and conclusion
\chapter{Summary and conclusion}

%}}}

\printbibliography

\end{document}
